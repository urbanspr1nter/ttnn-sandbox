{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonl\n",
    "\n",
    "with open(\"data/small-conversations/conversations.jsonl\", \"r\") as f:\n",
    "  all_data = list(jsonl.load(f))\n",
    "\n",
    "len(all_data), all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end_idx = int(0.9 * len(all_data))\n",
    "train_data = all_data[:train_end_idx]\n",
    "\n",
    "val_and_test_data = all_data[train_end_idx:]\n",
    "val_end_idx = int(0.9 * len(val_and_test_data))\n",
    "\n",
    "val_data = val_and_test_data[:val_end_idx]\n",
    "test_data = val_and_test_data[val_end_idx:]\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  list(filter(lambda x: x is not None, train_data))\n",
    "val_data = list(filter(lambda x: x is not None, val_data))\n",
    "test_data = list(filter(lambda x: x is not None, test_data))\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonl\n",
    "\n",
    "jsonl.dump(train_data, \"data/small-conversations/train_data.jsonl\")\n",
    "jsonl.dump(val_data, \"data/small-conversations/val_data.jsonl\")\n",
    "jsonl.dump(test_data, \"data/small-conversations/test_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "\n",
    "    self.encoded_texts = []\n",
    "    self.masks = []\n",
    "\n",
    "    for item in self.data:\n",
    "      tokens = []\n",
    "      mask = []\n",
    "\n",
    "      _instruction_base = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request\"\n",
    "      _instruction_base_tokens = tokenizer.encode(_instruction_base, allowed_special={\"<|endoftext|>\"})\n",
    "      tokens.extend(_instruction_base_tokens)\n",
    "      mask.extend([True] * len(_instruction_base_tokens))\n",
    "\n",
    "      for message in item:\n",
    "        if message[\"role\"] == \"user\":\n",
    "          _instruction_content_header = f\"\\n\\n### Instruction:\\n\"\n",
    "          _instruction_content_header_tokens = tokenizer.encode(_instruction_content_header, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_content_header_tokens)\n",
    "          mask.extend([True] * len(_instruction_content_header_tokens))\n",
    "\n",
    "          _instruction_content_content = message[\"content\"]\n",
    "          _instruction_content_content_tokens = tokenizer.encode(_instruction_content_content, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_content_content_tokens)\n",
    "          mask.extend([True] * len(_instruction_content_content_tokens))\n",
    "        else:\n",
    "          _instruction_response_header = f\"\\n\\n### Response:\\n\"\n",
    "          _instruction_response_header_tokens = tokenizer.encode(_instruction_response_header, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_response_header_tokens)\n",
    "          mask.extend([True] * len(_instruction_response_header_tokens))\n",
    "\n",
    "          # Dont mask the assistant response\n",
    "          _instruction_response_content = message[\"content\"] + \"\\n\\n\"\n",
    "          _instruction_response_content_tokens = tokenizer.encode(_instruction_response_content, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_response_content_tokens)\n",
    "          mask.extend([False] * len(_instruction_response_content_tokens))\n",
    "\n",
    "      self.encoded_texts.append(tokens)\n",
    "      self.masks.append(mask)\n",
    "\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "    return self.encoded_texts[index], self.masks[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "   batch,\n",
    "   pad_token_id=50256,\n",
    "   ignore_index=-100,\n",
    "   allowed_max_length=None,\n",
    "   device=\"cpu\" \n",
    "):\n",
    "  tokens_batch = [item[0] for item in batch]\n",
    "  masks_batch = [item[1] for item in batch]\n",
    "\n",
    "  # find the longest equence in the batch\n",
    "  batch_max_length = max(len(tokens) + 1 for tokens in tokens_batch)\n",
    "\n",
    "  inputs_lst, targets_lst = [], []\n",
    "\n",
    "  for tokens, mask in zip(tokens_batch, masks_batch):\n",
    "    new_tokens = tokens.copy()\n",
    "    new_tokens += [pad_token_id]\n",
    "    padded_tokens = (\n",
    "      new_tokens + ([pad_token_id] * (batch_max_length - len(new_tokens)))\n",
    "    )\n",
    "\n",
    "    new_mask = mask.copy()\n",
    "    new_mask += [True] # mask the added padded token\n",
    "    padded_mask = (\n",
    "      new_mask + ([True] * (batch_max_length - len(new_mask)))\n",
    "    )\n",
    "\n",
    "    inputs = torch.tensor(padded_tokens[:-1])\n",
    "    targets = torch.tensor(padded_tokens[1:])\n",
    "\n",
    "    pad_mask = targets == pad_token_id\n",
    "    indices = torch.nonzero(pad_mask).squeeze()\n",
    "\n",
    "    if indices.numel() > 1:\n",
    "      targets[indices[1:]] = ignore_index\n",
    "\n",
    "    # add the mask\n",
    "    for j in range(min(len(padded_mask), len(targets))):\n",
    "      if j + 1 < len(padded_mask) and padded_mask[j + 1]:\n",
    "        targets[j] = ignore_index\n",
    "\n",
    "    if allowed_max_length is not None:\n",
    "      inputs = inputs[:allowed_max_length]\n",
    "      targets = targets[:allowed_max_length]\n",
    "\n",
    "    inputs_lst.append(inputs)\n",
    "    targets_lst.append(targets)\n",
    "\n",
    "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "  targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "  return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "  custom_collate_fn,\n",
    "  device=device,\n",
    "  allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 16 \n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=True,\n",
    "  drop_last=True,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Train loader:\")\n",
    "i = 0\n",
    "for inputs, targets in train_loader:\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(inputs.shape, targets.shape)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0]) \n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import calc_loss_loader\n",
    "from scripts.model_loader import load_model_from_path\n",
    "\n",
    "\n",
    "model = load_model_from_path(\n",
    "  \"models/10b/gpt2-355M-bfloat16.pth\",\n",
    "  device=device \n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "model = torch.compile(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=10)\n",
    "  val_loss = calc_loss_loader(val_loader, model, device, num_batches=10)\n",
    "\n",
    "print(\"Training loss\", train_loss)\n",
    "print(\"Validation loss\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 - 5e-5\n",
    "Epoch 2 - 2e-5 <-- good!\n",
    "Epoch 3 - 1e-5 <-- diminishing returns. start to see reptitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.model_loader import load_model_from_path\n",
    "from scripts.fine_tune import format_input\n",
    "\n",
    "model = load_model_from_path(\n",
    "  \"models/gpt2-355M-model-it-ep6-long-v2.pth\",\n",
    "  device=device \n",
    ")\n",
    "\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "model.train()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "  model.parameters(),\n",
    "  lr=5e-5,\n",
    "  weight_decay=0.1,\n",
    "  fused=True\n",
    ")\n",
    "\n",
    "num_epochs=1\n",
    "\n",
    "double_new_line_id = tokenizer.encode(\"\\n\\n\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "train_losses, val_losses = train_model_simple(\n",
    "  model=model,\n",
    "  train_loader=train_loader,\n",
    "  val_loader=val_loader,\n",
    "  optimizer=optimizer,\n",
    "  num_epochs=num_epochs,\n",
    "  eval_freq=100,\n",
    "  eval_iter=50,\n",
    "  start_context=format_input(val_data[0]),\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    "  save_iters=200,\n",
    "  stop_sequence=[double_new_line_id]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpt2_common import save_model_and_optimizer\n",
    "model_directory = \"/home/rngo/code/ttnn-sandbox/notebooks/models\"\n",
    "save_model_and_optimizer(\n",
    "  model_path=f\"{str(model_directory)}/gpt2-355M-model-it-ep7-long-v2.pth\",\n",
    "  model=model,\n",
    "  optimizer_path=f\"{str(model_directory)}/optimizer-gpt2-355M-model-it-ep7-long-v2.pth\",\n",
    "  optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from scripts.generate import generate\n",
    "from scripts.util import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "checkpoints = [0, 500, 1000, 1500, 2000]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "for checkpoint in checkpoints:\n",
    "  print(\"-\" * 20)\n",
    "  if checkpoint // 4000 == 0:\n",
    "    ep = 1\n",
    "  else:\n",
    "    ep = 2\n",
    "\n",
    "  print(f\"Testing a message at checkpoint: {checkpoint}, Epoch: {ep}\")\n",
    "  model = load_model_from_path(f\"models/checkpoint-model-ep{ep}-{checkpoint}.pth\", device)\n",
    "  model.eval()\n",
    "\n",
    "  for i, test in enumerate(test_data[:3]):\n",
    "    print(f\"## Test message: {i}\")\n",
    "    test_message = format_input(test)\n",
    "\n",
    "    token_ids = generate(\n",
    "      model,\n",
    "      idx=text_to_token_ids(test_message, tokenizer).to(device),\n",
    "      max_new_tokens=256,\n",
    "      context_size=1024,\n",
    "      temperature=1.0,\n",
    "      top_k=20,\n",
    "      eos_id=50256,\n",
    "      device=device\n",
    "    )\n",
    "\n",
    "    text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(\"-\" * 20)\n",
    "    print(text)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Fine Tuning - More Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
