{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine Tuning\n",
    "\n",
    "We'll now take our base model and fine tune it to a chat bot like ChatGPT :).\n",
    "\n",
    "AFAIK, OpenAI never created a fine-tuned instruct model of GPT2, so we're in uncharted territory now. It also means that this can either turn out great, or just horrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "I have a dataset that I synthethically generated by the included `dataset_generation.py` script using `gpt-4o` with the OpenAI API. \n",
    "\n",
    "You can run this script yourself to generate as many conversations as you need. Do note, that this does cost money, but you can also generate the data using a local LLM such as Llama, or Qwen. \n",
    "\n",
    "I have included a small version of the dataset in this repo called `shared/data/conversations-sm.jsonl`. It contains about 10K multi-turn conversations up to 1000 tokens -- which is just hitting th maximum context length for GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonl\n",
    "\n",
    "dataset_file_path = \"data/small-conversations/conversations.jsonl\"\n",
    "\n",
    "with open(dataset_file_path, \"r\") as f:\n",
    "  all_data = list(jsonl.load(f))\n",
    "\n",
    "len(all_data), all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also shuffle the entire dataset in place to get a good mix of long and short conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll do a training split of 90%. The validation and test portions of the dataset will be the remaining 10%. \n",
    "\n",
    "Of the remaining 10%, 90% of that 10% will go in as validation data, and the rest, test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end_idx = int(0.9 * len(all_data))\n",
    "train_data = all_data[:train_end_idx]\n",
    "\n",
    "val_and_test_data = all_data[train_end_idx:]\n",
    "val_end_idx = int(0.9 * len(val_and_test_data))\n",
    "\n",
    "val_data = val_and_test_data[:val_end_idx]\n",
    "test_data = val_and_test_data[val_end_idx:]\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out any strange entries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  list(filter(lambda x: x is not None, train_data))\n",
    "val_data = list(filter(lambda x: x is not None, val_data))\n",
    "test_data = list(filter(lambda x: x is not None, test_data))\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And shuffle our training data once more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "\n",
    "    self.encoded_texts = []\n",
    "    self.masks = []\n",
    "\n",
    "    for item in self.data:\n",
    "      tokens = []\n",
    "      mask = []\n",
    "\n",
    "      _instruction_base = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request\"\n",
    "      _instruction_base_tokens = tokenizer.encode(_instruction_base, allowed_special={\"<|endoftext|>\"})\n",
    "      tokens.extend(_instruction_base_tokens)\n",
    "      mask.extend([True] * len(_instruction_base_tokens))\n",
    "\n",
    "      for message in item:\n",
    "        if message[\"role\"] == \"user\":\n",
    "          _instruction_content_header = f\"\\n\\n### Instruction:\\n\"\n",
    "          _instruction_content_header_tokens = tokenizer.encode(_instruction_content_header, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_content_header_tokens)\n",
    "          mask.extend([True] * len(_instruction_content_header_tokens))\n",
    "\n",
    "          _instruction_content_content = message[\"content\"]\n",
    "          _instruction_content_content_tokens = tokenizer.encode(_instruction_content_content, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_content_content_tokens)\n",
    "          mask.extend([True] * len(_instruction_content_content_tokens))\n",
    "        else:\n",
    "          _instruction_response_header = f\"\\n\\n### Response:\\n\"\n",
    "          _instruction_response_header_tokens = tokenizer.encode(_instruction_response_header, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_response_header_tokens)\n",
    "          mask.extend([True] * len(_instruction_response_header_tokens))\n",
    "\n",
    "          # Dont mask the assistant response\n",
    "          _instruction_response_content = message[\"content\"] + \"\\n\\n\"\n",
    "          _instruction_response_content_tokens = tokenizer.encode(_instruction_response_content, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_response_content_tokens)\n",
    "          mask.extend([False] * len(_instruction_response_content_tokens))\n",
    "\n",
    "      self.encoded_texts.append(tokens)\n",
    "      self.masks.append(mask)\n",
    "\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "    return self.encoded_texts[index], self.masks[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "   batch,\n",
    "   pad_token_id=50256,\n",
    "   ignore_index=-100,\n",
    "   allowed_max_length=None,\n",
    "   device=\"cpu\" \n",
    "):\n",
    "  tokens_batch = [item[0] for item in batch]\n",
    "  masks_batch = [item[1] for item in batch]\n",
    "\n",
    "  # find the longest equence in the batch\n",
    "  batch_max_length = max(len(tokens) + 1 for tokens in tokens_batch)\n",
    "\n",
    "  inputs_lst, targets_lst = [], []\n",
    "\n",
    "  for tokens, mask in zip(tokens_batch, masks_batch):\n",
    "    new_tokens = tokens.copy()\n",
    "    new_tokens += [pad_token_id]\n",
    "    padded_tokens = (\n",
    "      new_tokens + ([pad_token_id] * (batch_max_length - len(new_tokens)))\n",
    "    )\n",
    "\n",
    "    new_mask = mask.copy()\n",
    "    new_mask += [True] # mask the added padded token\n",
    "    padded_mask = (\n",
    "      new_mask + ([True] * (batch_max_length - len(new_mask)))\n",
    "    )\n",
    "\n",
    "    inputs = torch.tensor(padded_tokens[:-1])\n",
    "    targets = torch.tensor(padded_tokens[1:])\n",
    "\n",
    "    pad_mask = targets == pad_token_id\n",
    "    indices = torch.nonzero(pad_mask).squeeze()\n",
    "\n",
    "    if indices.numel() > 1:\n",
    "      targets[indices[1:]] = ignore_index\n",
    "\n",
    "    # add the mask\n",
    "    for j in range(min(len(padded_mask), len(targets))):\n",
    "      if j + 1 < len(padded_mask) and padded_mask[j + 1]:\n",
    "        targets[j] = ignore_index\n",
    "\n",
    "    if allowed_max_length is not None:\n",
    "      inputs = inputs[:allowed_max_length]\n",
    "      targets = targets[:allowed_max_length]\n",
    "\n",
    "    inputs_lst.append(inputs)\n",
    "    targets_lst.append(targets)\n",
    "\n",
    "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "  targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "  return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "  custom_collate_fn,\n",
    "  device=device,\n",
    "  allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 32 \n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=True,\n",
    "  drop_last=True,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Train loader:\")\n",
    "i = 0\n",
    "for inputs, targets in train_loader:\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(inputs.shape, targets.shape)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0]) \n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import calc_loss_loader\n",
    "from scripts.model_loader import load_model_from_path\n",
    "\n",
    "\n",
    "model = load_model_from_path(\n",
    "  \"models/10b/gpt2-355M-bfloat16.pth\",\n",
    "  device=device \n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "model = torch.compile(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=10)\n",
    "  val_loss = calc_loss_loader(val_loader, model, device, num_batches=10)\n",
    "\n",
    "print(\"Training loss\", train_loss)\n",
    "print(\"Validation loss\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this run it was like this:\n",
    "\n",
    "- Epoch 1 - 8e-5\n",
    "- Epoch 2 - 5e-5 \n",
    "- Epoch 3 - 2e-5 \n",
    "\n",
    "After Epoch 3 , i was starting to see diminishing returns. Training loss was 1.317 with a validation loss of 1.564. The gap was only widening. so I stopped there and consider this to be my complete model `gpt2-355M-it-bfloat16.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.model_loader import load_model_from_path\n",
    "from scripts.fine_tune import format_input\n",
    "\n",
    "model = load_model_from_path(\n",
    "  \"models/gpt2-355M-model-it-ep2-long-v3.pth\",\n",
    "  device=device \n",
    ")\n",
    "\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "model.train()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "  model.parameters(),\n",
    "  lr=2e-5,\n",
    "  weight_decay=0.1,\n",
    "  fused=True\n",
    ")\n",
    "\n",
    "num_epochs=1\n",
    "\n",
    "double_new_line_id = tokenizer.encode(\"\\n\\n\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "train_losses, val_losses = train_model_simple(\n",
    "  model=model,\n",
    "  train_loader=train_loader,\n",
    "  val_loader=val_loader,\n",
    "  optimizer=optimizer,\n",
    "  num_epochs=num_epochs,\n",
    "  eval_freq=100,\n",
    "  eval_iter=50,\n",
    "  start_context=format_input(val_data[0]),\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    "  save_iters=200,\n",
    "  stop_sequence=[double_new_line_id]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpt2_common import save_model_and_optimizer\n",
    "model_directory = \"/home/rngo/code/ttnn-sandbox/notebooks/models\"\n",
    "save_model_and_optimizer(\n",
    "  model_path=f\"{str(model_directory)}/gpt2-355M-model-it-ep3-long-v3.pth\",\n",
    "  model=model,\n",
    "  optimizer_path=f\"{str(model_directory)}/optimizer-gpt2-355M-model-it-ep3-long-v3.pth\",\n",
    "  optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from scripts.generate import generate\n",
    "from scripts.util import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "checkpoints = [0, 500, 1000, 1500, 2000]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "for checkpoint in checkpoints:\n",
    "  print(\"-\" * 20)\n",
    "  if checkpoint // 4000 == 0:\n",
    "    ep = 1\n",
    "  else:\n",
    "    ep = 2\n",
    "\n",
    "  print(f\"Testing a message at checkpoint: {checkpoint}, Epoch: {ep}\")\n",
    "  model = load_model_from_path(f\"models/checkpoint-model-ep{ep}-{checkpoint}.pth\", device)\n",
    "  model.eval()\n",
    "\n",
    "  for i, test in enumerate(test_data[:3]):\n",
    "    print(f\"## Test message: {i}\")\n",
    "    test_message = format_input(test)\n",
    "\n",
    "    token_ids = generate(\n",
    "      model,\n",
    "      idx=text_to_token_ids(test_message, tokenizer).to(device),\n",
    "      max_new_tokens=256,\n",
    "      context_size=1024,\n",
    "      temperature=1.0,\n",
    "      top_k=20,\n",
    "      eos_id=50256,\n",
    "      device=device\n",
    "    )\n",
    "\n",
    "    text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(\"-\" * 20)\n",
    "    print(text)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Fine Tuning - More Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
