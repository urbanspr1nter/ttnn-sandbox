{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79808,\n",
       " [{'role': 'user', 'content': \"Hi there, how's your day going?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Hello! My day is going great, thank you for asking. How about yours?'},\n",
       "  {'role': 'user',\n",
       "   'content': \"It's been a bit busy, but I'm managing. Any interesting facts to share?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3000 years old and still edible!'},\n",
       "  {'role': 'user',\n",
       "   'content': \"Wow, that's fascinating! I never would have guessed. What makes honey last so long?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Honey's longevity is due to its low moisture content and acidic nature, which make it inhospitable for bacteria and microorganisms to grow.\"},\n",
       "  {'role': 'user',\n",
       "   'content': \"That makes sense. Nature really is amazing, isn't it?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Absolutely! There's so much complexity and wonder in the natural world. Is there anything else you'd like to explore today?\"}])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonl\n",
    "\n",
    "with open(\"data/small-conversations/conversations.jsonl\", \"r\") as f:\n",
    "  all_data = list(jsonl.load(f))\n",
    "\n",
    "len(all_data), all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Hey there! Can you suggest a good movie for tonight?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Sure! Have you seen 'The Grand Budapest Hotel'? It's a delightful mix of comedy and adventure.\"},\n",
       " {'role': 'user', 'content': \"I haven't watched it yet. What's it about?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"It's about the adventures of a legendary concierge and his friendship with a young lobby boy at a famous European hotel. It's quite entertaining with a unique visual style.\"},\n",
       " {'role': 'user',\n",
       "  'content': \"Sounds interesting! I'll definitely give it a try. Thanks!\"},\n",
       " {'role': 'assistant', 'content': \"You're welcome! Enjoy the movie!\"}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(all_data)\n",
    "\n",
    "all_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71827, 7182, 799)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_end_idx = int(0.9 * len(all_data))\n",
    "train_data = all_data[:train_end_idx]\n",
    "\n",
    "val_and_test_data = all_data[train_end_idx:]\n",
    "val_end_idx = int(0.9 * len(val_and_test_data))\n",
    "\n",
    "val_data = val_and_test_data[:val_end_idx]\n",
    "test_data = val_and_test_data[val_end_idx:]\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71827, 7182, 799)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data =  list(filter(lambda x: x is not None, train_data))\n",
    "val_data = list(filter(lambda x: x is not None, val_data))\n",
    "test_data = list(filter(lambda x: x is not None, test_data))\n",
    "\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonl\n",
    "\n",
    "jsonl.dump(train_data, \"data/small-conversations/train_data.jsonl\")\n",
    "jsonl.dump(val_data, \"data/small-conversations/val_data.jsonl\")\n",
    "jsonl.dump(test_data, \"data/small-conversations/test_data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "\n",
    "    self.encoded_texts = []\n",
    "    self.masks = []\n",
    "\n",
    "    for item in self.data:\n",
    "      tokens = []\n",
    "      mask = []\n",
    "\n",
    "      _instruction_base = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request\"\n",
    "      _instruction_base_tokens = tokenizer.encode(_instruction_base, allowed_special={\"<|endoftext|>\"})\n",
    "      tokens.extend(_instruction_base_tokens)\n",
    "      mask.extend([True] * len(_instruction_base_tokens))\n",
    "\n",
    "      for message in item:\n",
    "        if message[\"role\"] == \"user\":\n",
    "          _instruction_content_header = f\"\\n\\n### Instruction:\\n\"\n",
    "          _instruction_content_header_tokens = tokenizer.encode(_instruction_content_header, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_content_header_tokens)\n",
    "          mask.extend([True] * len(_instruction_content_header_tokens))\n",
    "\n",
    "          _instruction_content_content = message[\"content\"]\n",
    "          _instruction_content_content_tokens = tokenizer.encode(_instruction_content_content, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_content_content_tokens)\n",
    "          mask.extend([True] * len(_instruction_content_content_tokens))\n",
    "        else:\n",
    "          _instruction_response_header = f\"\\n\\n### Response:\\n\"\n",
    "          _instruction_response_header_tokens = tokenizer.encode(_instruction_response_header, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_response_header_tokens)\n",
    "          mask.extend([True] * len(_instruction_response_header_tokens))\n",
    "\n",
    "          # Dont mask the assistant response\n",
    "          _instruction_response_content = message[\"content\"] + \"\\n\\n\"\n",
    "          _instruction_response_content_tokens = tokenizer.encode(_instruction_response_content, allowed_special={\"<|endoftext|>\"})\n",
    "          tokens.extend(_instruction_response_content_tokens)\n",
    "          mask.extend([False] * len(_instruction_response_content_tokens))\n",
    "\n",
    "      self.encoded_texts.append(tokens)\n",
    "      self.masks.append(mask)\n",
    "\n",
    "      \n",
    "  def __getitem__(self, index):\n",
    "    return self.encoded_texts[index], self.masks[index]\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "   batch,\n",
    "   pad_token_id=50256,\n",
    "   ignore_index=-100,\n",
    "   allowed_max_length=None,\n",
    "   device=\"cpu\" \n",
    "):\n",
    "  tokens_batch = [item[0] for item in batch]\n",
    "  masks_batch = [item[1] for item in batch]\n",
    "\n",
    "  # find the longest equence in the batch\n",
    "  batch_max_length = max(len(tokens) + 1 for tokens in tokens_batch)\n",
    "\n",
    "  inputs_lst, targets_lst = [], []\n",
    "\n",
    "  for tokens, mask in zip(tokens_batch, masks_batch):\n",
    "    new_tokens = tokens.copy()\n",
    "    new_tokens += [pad_token_id]\n",
    "    padded_tokens = (\n",
    "      new_tokens + ([pad_token_id] * (batch_max_length - len(new_tokens)))\n",
    "    )\n",
    "\n",
    "    new_mask = mask.copy()\n",
    "    new_mask += [True] # mask the added padded token\n",
    "    padded_mask = (\n",
    "      new_mask + ([True] * (batch_max_length - len(new_mask)))\n",
    "    )\n",
    "\n",
    "    inputs = torch.tensor(padded_tokens[:-1])\n",
    "    targets = torch.tensor(padded_tokens[1:])\n",
    "\n",
    "    pad_mask = targets == pad_token_id\n",
    "    indices = torch.nonzero(pad_mask).squeeze()\n",
    "\n",
    "    if indices.numel() > 1:\n",
    "      targets[indices[1:]] = ignore_index\n",
    "\n",
    "    # add the mask\n",
    "    for j in range(min(len(padded_mask), len(targets))):\n",
    "      if j + 1 < len(padded_mask) and padded_mask[j + 1]:\n",
    "        targets[j] = ignore_index\n",
    "\n",
    "    if allowed_max_length is not None:\n",
    "      inputs = inputs[:allowed_max_length]\n",
    "      targets = targets[:allowed_max_length]\n",
    "\n",
    "    inputs_lst.append(inputs)\n",
    "    targets_lst.append(targets)\n",
    "\n",
    "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "  targets_tensor = torch.stack(targets_lst).to(device)\n",
    "\n",
    "  return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\"\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "  custom_collate_fn,\n",
    "  device=device,\n",
    "  allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 32 \n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=True,\n",
    "  drop_last=True,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "  val_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=batch_size,\n",
    "  collate_fn=customized_collate_fn,\n",
    "  shuffle=False,\n",
    "  drop_last=False,\n",
    "  num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([32, 857]) torch.Size([32, 857])\n",
      "torch.Size([32, 775]) torch.Size([32, 775])\n",
      "torch.Size([32, 980]) torch.Size([32, 980])\n",
      "torch.Size([32, 875]) torch.Size([32, 875])\n",
      "torch.Size([32, 975]) torch.Size([32, 975])\n",
      "torch.Size([32, 932]) torch.Size([32, 932])\n",
      "torch.Size([32, 914]) torch.Size([32, 914])\n",
      "torch.Size([32, 860]) torch.Size([32, 860])\n",
      "torch.Size([32, 1014]) torch.Size([32, 1014])\n",
      "torch.Size([32, 995]) torch.Size([32, 995])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Train loader:\")\n",
    "i = 0\n",
    "for inputs, targets in train_loader:\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(inputs.shape, targets.shape)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  2882,   326, 20431, 32543,   262,  2581,   198,   198, 21017,\n",
      "        46486,    25,   198, 10814,    11,   466,   345,   760,  1521,   262,\n",
      "         6766,   318,  4171,    30,   198,   198, 21017, 18261,    25,   198,\n",
      "         5297,    11,   262,  6766,  3568,  4171,   780,   286,  7760, 42342,\n",
      "        45765,    13,   770, 10733,  8833,   618,   262,  3668,   338,  8137,\n",
      "          629, 34387, 19606,    11,   290,  4171,  1657,   318, 16830,   287,\n",
      "          477, 11678,   517,   621,   584,  7577,   780,   340, 17781,   287,\n",
      "        12238,    11,  4833,  9813,    13,   628,   198,   198, 21017, 46486,\n",
      "           25,   198,  2504,   338,  3499,     0,  1867,   546,  4252, 28709,\n",
      "          852,  2266,   290, 10912,    30,   198,   198, 21017, 18261,    25,\n",
      "          198,  7191, 26428,    11,   262,  4252,   318,  2793,   287,   262,\n",
      "         6766,    11,   290,   262,  1657,   468,   284,  1208,   832,   517,\n",
      "         8137,    13,   770,   629, 34387,   262, 12238,  4171, 45656,   503,\n",
      "          286,   674,  1627,   286,  6504,    11,  5086,   262,  2392,  2266,\n",
      "           11, 10912,    11,   290,  7872, 45656,   284,  3151,   674,  2951,\n",
      "           13,   628,   198,   198, 21017, 46486,    25,   198,  2504,  1838,\n",
      "         2565,    13,  6952,   345,   329, 11170,   340,     0,   198,   198,\n",
      "        21017, 18261,    25,   198,  1639,   821,  7062,     0,  1002,   345,\n",
      "          423,   597,   517,  2683,   546,   262,  6766,   393,  1997,  2073,\n",
      "           11,  1254,  1479,   284,  1265,     0,   628, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256], device='cuda:1')\n",
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  5297,\n",
      "           11,   262,  6766,  3568,  4171,   780,   286,  7760, 42342, 45765,\n",
      "           13,   770, 10733,  8833,   618,   262,  3668,   338,  8137,   629,\n",
      "        34387, 19606,    11,   290,  4171,  1657,   318, 16830,   287,   477,\n",
      "        11678,   517,   621,   584,  7577,   780,   340, 17781,   287, 12238,\n",
      "           11,  4833,  9813,    13,   628,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         7191, 26428,    11,   262,  4252,   318,  2793,   287,   262,  6766,\n",
      "           11,   290,   262,  1657,   468,   284,  1208,   832,   517,  8137,\n",
      "           13,   770,   629, 34387,   262, 12238,  4171, 45656,   503,   286,\n",
      "          674,  1627,   286,  6504,    11,  5086,   262,  2392,  2266,    11,\n",
      "        10912,    11,   290,  7872, 45656,   284,  3151,   674,  2951,    13,\n",
      "          628,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  1639,   821,  7062,     0,  1002,   345,   423,\n",
      "          597,   517,  2683,   546,   262,  6766,   393,  1997,  2073,    11,\n",
      "         1254,  1479,   284,  1265,     0,   628,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0]) \n",
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import calc_loss_loader\n",
    "from scripts.model_loader import load_model_from_path\n",
    "\n",
    "\n",
    "model = load_model_from_path(\n",
    "  \"models/10b/gpt2-355M-bfloat16.pth\",\n",
    "  device=device \n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "model = torch.compile(model)\n",
    "\n",
    "with torch.no_grad():\n",
    "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=10)\n",
    "  val_loss = calc_loss_loader(val_loader, model, device, num_batches=10)\n",
    "\n",
    "print(\"Training loss\", train_loss)\n",
    "print(\"Validation loss\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1 - 5e-5\n",
    "Epoch 2 - 2e-5 <-- good!\n",
    "Epoch 3 - 1e-5 <-- diminishing returns. start to see reptitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.model_loader import load_model_from_path\n",
    "from scripts.fine_tune import format_input\n",
    "\n",
    "model = load_model_from_path(\n",
    "  \"models/10b/gpt2-355M-bfloat16.pth\",\n",
    "  device=device \n",
    ")\n",
    "\n",
    "model = model.to(device).to(torch.bfloat16)\n",
    "model.train()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "  model.parameters(),\n",
    "  lr=5e-5,\n",
    "  weight_decay=0.1,\n",
    "  fused=True\n",
    ")\n",
    "\n",
    "num_epochs=1\n",
    "\n",
    "double_new_line_id = tokenizer.encode(\"\\n\\n\", allowed_special={\"<|endoftext|>\"})[0]\n",
    "\n",
    "train_losses, val_losses = train_model_simple(\n",
    "  model=model,\n",
    "  train_loader=train_loader,\n",
    "  val_loader=val_loader,\n",
    "  optimizer=optimizer,\n",
    "  num_epochs=num_epochs,\n",
    "  eval_freq=100,\n",
    "  eval_iter=50,\n",
    "  start_context=format_input(val_data[0]),\n",
    "  tokenizer=tokenizer,\n",
    "  device=device,\n",
    "  save_iters=200,\n",
    "  stop_sequence=[double_new_line_id]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpt2_common import save_model_and_optimizer\n",
    "model_directory = \"/home/rngo/code/ttnn-sandbox/notebooks/models\"\n",
    "save_model_and_optimizer(\n",
    "  model_path=f\"{str(model_directory)}/gpt2-355M-model-it-ep1-long-v3.pth\",\n",
    "  model=model,\n",
    "  optimizer_path=f\"{str(model_directory)}/optimizer-gpt2-355M-model-it-ep1-long-v3.pth\",\n",
    "  optimizer=optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from scripts.generate import generate\n",
    "from scripts.util import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "checkpoints = [0, 500, 1000, 1500, 2000]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "for checkpoint in checkpoints:\n",
    "  print(\"-\" * 20)\n",
    "  if checkpoint // 4000 == 0:\n",
    "    ep = 1\n",
    "  else:\n",
    "    ep = 2\n",
    "\n",
    "  print(f\"Testing a message at checkpoint: {checkpoint}, Epoch: {ep}\")\n",
    "  model = load_model_from_path(f\"models/checkpoint-model-ep{ep}-{checkpoint}.pth\", device)\n",
    "  model.eval()\n",
    "\n",
    "  for i, test in enumerate(test_data[:3]):\n",
    "    print(f\"## Test message: {i}\")\n",
    "    test_message = format_input(test)\n",
    "\n",
    "    token_ids = generate(\n",
    "      model,\n",
    "      idx=text_to_token_ids(test_message, tokenizer).to(device),\n",
    "      max_new_tokens=256,\n",
    "      context_size=1024,\n",
    "      temperature=1.0,\n",
    "      top_k=20,\n",
    "      eos_id=50256,\n",
    "      device=device\n",
    "    )\n",
    "\n",
    "    text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(\"-\" * 20)\n",
    "    print(text)\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued Fine Tuning - More Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
