{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Attention Mechanisms\n",
    "\n",
    "This notebook builds off the Simple Weighted Attention Mechanism notebooks. We implemented the causal attention mechanism which is just a self-attention mechanism (like the simple weighted one) where we only consider tokens that appear previously when attempting to predict the next token in the sequence.\n",
    "\n",
    "Causal attention\n",
    "> Restricts model to only consider previous and current inputs in a sequence when processing any given token when computing attention scores. \n",
    ">\n",
    "> -- _Sebastian Raschka - Build a Large Language Model from Scratch_\n",
    "\n",
    "The code is adapted again, from Build a Large Language Model from Scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:39:41.910 | DEBUG    | ttnn.library_tweaks:prepare_dir_as_metal_home:54 - Existing installation of 0.57.0rc60+any detected\n",
      "2025-05-03 15:39:41.933 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare the familiar input, `context` representing the string `Your journey starts with one step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For optimizations relating to TTNN, we can declare the dimensions for `CoreGrid` for all optimized `linear` and `matmul` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_grid_x = 8\n",
    "core_grid_y = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now bring back our `ttnn` optimized `SelfAttention` class and use that to demonstrate causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out , bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self._device = device\n",
    "    \n",
    "    # Extract weight matrices from PyTorch layers and convert to TTNN once\n",
    "    self.W_query_ttnn = ttnn.from_torch(\n",
    "      self.W_query.weight, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=self._device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    \n",
    "    self.W_key_ttnn = ttnn.from_torch(\n",
    "      self.W_key.weight, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=self._device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    \n",
    "    self.W_value_ttnn = ttnn.from_torch(\n",
    "      self.W_value.weight, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=self._device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "\n",
    "    self._scaler = 1 / (d_out ** 0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_ttnn = ttnn.from_torch(\n",
    "      x, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=device,\n",
    "    )\n",
    "    \n",
    "    queries_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_query_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "    values_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_value_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "    keys_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_key_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn, \n",
    "      ttnn.permute(keys_ttnn, (1, 0)),\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "\n",
    "    attn_weights_ttnn = ttnn.softmax(\n",
    "      attn_scores_ttnn * self._scaler,\n",
    "      dim=-1\n",
    "    )\n",
    "\n",
    "    context_vec_ttnn = ttnn.matmul(\n",
    "      attn_weights_ttnn,\n",
    "      values_ttnn,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "\n",
    "    context_vec = ttnn.to_torch(context_vec_ttnn)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dimensions which will be used to instantiate the SelfAttention_v2 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = context.shape\n",
    "d_in = context.shape[1]\n",
    "d_out = context.shape[1]\n",
    "\n",
    "d_in, d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Attention TTNN Demonstration\n",
    "\n",
    "Causal attention will result in different attention weights. In order to demonstrate this effectively, we'll create a `SelfAttention_v2` instance, but not perform the forward pass directly. Instead, we'll use the weights from query, key and values to perform a linear transformation (manually) on the given input batch. \n",
    "\n",
    "Once we have the linear transformations, we can find the attention scores against the queries and transposed result of keys through matrix multiplication.\n",
    "\n",
    "We can get the attention weights by performing a softmax operation on the attention scores, scaled by the square root of the dimension of the keys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Compute Attention Weights\n",
    "\n",
    "Note, at this point we are still not using causal attention, but just needing the necessary components to manually compute the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New chip! We now have 1 chips\n",
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n",
      "\u001b[32m2025-05-03 15:39:46.524\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-05-03 15:39:46.534\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-03 15:39:46.537\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-05-03 15:39:46.537\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-03 15:39:46.538\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-05-03 15:39:46.538\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-05-03 15:39:46.561\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_transpose_wh_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_transpose_wh_interleaved_start_id, transpose_wh\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[0.1621, 0.1621, 0.1621, 0.1680, 0.1914, 0.1562],\n",
       "              [0.1621, 0.1602, 0.1611, 0.1719, 0.1846, 0.1621],\n",
       "              [0.1631, 0.1611, 0.1621, 0.1719, 0.1846, 0.1621],\n",
       "              [0.1641, 0.1631, 0.1631, 0.1699, 0.1738, 0.1660],\n",
       "              [0.1641, 0.1641, 0.1650, 0.1680, 0.1787, 0.1611],\n",
       "              [0.1641, 0.1611, 0.1621, 0.1709, 0.1748, 0.1660]],\n",
       "             dtype=torch.bfloat16),\n",
       " torch.Size([6, 6]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out, device)\n",
    "inputs_ttnn = ttnn.from_torch(\n",
    "  context,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "queries_ttnn = ttnn.linear(\n",
    "  inputs_ttnn,\n",
    "  sa_v2.W_query_ttnn,\n",
    "  transpose_b=True,\n",
    "  core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    ")\n",
    "keys_ttnn = ttnn.linear(\n",
    "  inputs_ttnn,\n",
    "  sa_v2.W_key_ttnn,\n",
    "  transpose_b=True,\n",
    "  core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    ")\n",
    "attn_scores_ttnn = ttnn.matmul(\n",
    "  queries_ttnn, \n",
    "  ttnn.permute(keys_ttnn, (1, 0)),\n",
    "  core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    ")\n",
    "\n",
    "attn_weights_ttnn = ttnn.softmax(\n",
    "  attn_scores_ttnn * (1 / (d_out ** 0.5)),\n",
    "  dim=-1\n",
    ")\n",
    "\n",
    "attn_scores = ttnn.to_torch(attn_scores_ttnn, device=device)\n",
    "attn_weights = ttnn.to_torch(attn_weights_ttnn, device= device)\n",
    "ttnn.close_device(device)\n",
    "\n",
    "attn_weights, attn_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Causal Attention with Torch\n",
    "\n",
    "Now, let's break down this process using `torch` to understand the process better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets create a mask to be applied on the attention scores we have computed using `ttnn` in the previous step. It is going to have the shape of `(context_length, context_length)`. In this case, (6, 6).\n",
    "\n",
    "We can use `torch.tril` to create a lower triangular matrix with ones on the diagonal and zeros elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "mask_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply the mask to the attention weights (as found in the earlier steps).\n",
    "\n",
    "The result after applying the lower triangular mask should result in, well, a lower triangular matrix where the upper triangle is filled with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTensor([[0.1621, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.1621, 0.1602, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.1631, 0.1611, 0.1621, 0.0000, 0.0000, 0.0000],\n",
       "             [0.1641, 0.1631, 0.1631, 0.1699, 0.0000, 0.0000],\n",
       "             [0.1641, 0.1641, 0.1650, 0.1680, 0.1787, 0.0000],\n",
       "             [0.1641, 0.1611, 0.1621, 0.1709, 0.1748, 0.1660]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "masked_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the masked attention weights are no longer normalized. We need to normalize them again so that the rows add up to 1 as close as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.5030, 0.4970, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.3353, 0.3313, 0.3333, 0.0000, 0.0000, 0.0000],\n",
       "             [0.2485, 0.2470, 0.2470, 0.2574, 0.0000, 0.0000],\n",
       "             [0.1953, 0.1953, 0.1965, 0.2000, 0.2128, 0.0000],\n",
       "             [0.1642, 0.1613, 0.1623, 0.1711, 0.1750, 0.1662]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "masked_simple_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alteratively we can maske the original attention scores to be -inf where the mask is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTensor([[-0.2295,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "             [-0.3105, -0.3281,    -inf,    -inf,    -inf,    -inf],\n",
       "             [-0.3086, -0.3242, -0.3164,    -inf,    -inf,    -inf],\n",
       "             [-0.1729, -0.1855, -0.1826, -0.0996,    -inf,    -inf],\n",
       "             [-0.1338, -0.1309, -0.1235, -0.0820,  0.0620,    -inf],\n",
       "             [-0.2314, -0.2520, -0.2500, -0.1328, -0.0840, -0.1963]],\n",
       "            dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying softmax, we get more or less, the same answer as if we had done a norm on the triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.5039, 0.4980, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.3359, 0.3320, 0.3340, 0.0000, 0.0000, 0.0000],\n",
       "             [0.2480, 0.2461, 0.2471, 0.2598, 0.0000, 0.0000],\n",
       "             [0.1943, 0.1943, 0.1953, 0.2002, 0.2168, 0.0000],\n",
       "             [0.1631, 0.1611, 0.1611, 0.1719, 0.1768, 0.1660]],\n",
       "            dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / d_out ** 0.5, dim=1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we can just do the similar steps but using `ttnn` when we write our `CausalAttention` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout with Torch\n",
    "\n",
    "Dropout is used to reduce the chances of overfitting data. We just remove some of the values in the masked attention weights to keep things spicy.\n",
    "\n",
    "We will apply the dropout after computing the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `torch`, it is easy to apply dropout. We can just create a `nn.Dropout` module and assign the probability of the dropout. \n",
    "\n",
    "The example below shows where the dropout is applied. You'll see that now some elements in the matrix now become 0, while some become 2 (because we applied dropout at 0.5 probability, the 1s are rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying dropout to the attention weights does result in something more interesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.0000, 0.0000, 0.6680, 0.0000, 0.0000, 0.0000],\n",
       "             [0.0000, 0.4922, 0.0000, 0.5195, 0.0000, 0.0000],\n",
       "             [0.0000, 0.3887, 0.3906, 0.4004, 0.4336, 0.0000],\n",
       "             [0.3262, 0.3223, 0.0000, 0.0000, 0.3535, 0.3320]],\n",
       "            dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's conceptually all there is to it. Now, let's see how we can do dropout with `ttnn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout with TTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `ttnn`, we use `ttnn.experimental.dropout` to apply dropout. The API allows us to create a result tensor that has dropout applied to an input given some probability with appropriate scaling in one shot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: reader_dropout_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: writer_dropout_interleaved_start_id, reader_dropout_interleaved_start_id, dropout_kernel\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchTensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [1.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "             [0.6719, 0.6641, 0.6680, 0.0000, 0.0000, 0.0000],\n",
       "             [0.4961, 0.0000, 0.0000, 0.5195, 0.0000, 0.0000],\n",
       "             [0.0000, 0.3887, 0.3906, 0.4004, 0.4336, 0.0000],\n",
       "             [0.0000, 0.3223, 0.3223, 0.0000, 0.3535, 0.0000]],\n",
       "            dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ttnn import TILE_LAYOUT\n",
    "\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "attn_weights_ttnn = ttnn.to_device(\n",
    "  ttnn.from_torch(attn_weights, dtype=ttnn.bfloat16, layout=TILE_LAYOUT),\n",
    "  device\n",
    ")\n",
    "\n",
    "prob = 0.5\n",
    "attn_weights_dropout_ttnn = ttnn.experimental.dropout(\n",
    "  attn_weights_ttnn,\n",
    "  seed=123,\n",
    "  probability=prob,\n",
    "  scale=1.0 / (1.0 - prob)\n",
    ")\n",
    "\n",
    "dropped = ttnn.to_torch(attn_weights_dropout_ttnn)\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Causal Attention with Torch\n",
    "\n",
    "Putting it all together in `torch`, here is the `CausalAttention` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_out = d_out\n",
    "    \n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    self.mask = torch.triu(\n",
    "      torch.ones(context_length, context_length),\n",
    "      diagonal=1\n",
    "    )\n",
    "\n",
    "  def forward(self, x): \n",
    "    b, num_tokens, d_in = x.shape\n",
    "    keys = self.W_query(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    # transpose the last 2 dimensions while leaving the batch dimension alone.\n",
    "    attn_scores = queries @ keys.transpose(1, 2)\n",
    "    attn_scores.masked_fill(\n",
    "      self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
    "    )\n",
    "    attn_weights = torch.softmax(\n",
    "      attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "    )\n",
    "\n",
    "    attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "    context_vec = attn_weights @ values\n",
    "    return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2329,  0.3394, -0.1157],\n",
       "         [ 0.2621,  0.4198, -0.1169],\n",
       "         [ 0.2447,  0.3468, -0.0615],\n",
       "         [ 0.4368,  0.6380, -0.1251],\n",
       "         [ 0.2478,  0.3970, -0.1106],\n",
       "         [ 0.1850,  0.3498, -0.1246]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.reshape(context, [1, context.shape[0], context.shape[1]])\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.5)\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "context_vecs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Causal Attention with TTNN\n",
    "\n",
    "Same approach as the `ttnn` optimized version of `SelfAttention_v2`, we will create `ttnn` tensors of all the torch tensors created in the constructor. \n",
    "\n",
    "Then on forward pass, we apply linear transformations. Additional thing here is that now we get to use `ttnn.experimental.dropout` to apply dropout using `ttnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINUS_INFINITY = -1e9\n",
    "\n",
    "class CausalAttention_ttnn(nn.Module):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, device, qkv_bias=False):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_out = d_out\n",
    "    \n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "    self.device = device\n",
    "    self.W_query_ttnn = ttnn.from_torch(\n",
    "      self.W_query.weight,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "\n",
    "    self.W_key_ttnn = ttnn.from_torch(\n",
    "      self.W_key.weight,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    \n",
    "    self.W_value_ttnn = ttnn.from_torch(\n",
    "      self.W_value.weight,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "\n",
    "    self.mask_ttnn = ttnn.from_torch(\n",
    "      self.mask,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "\n",
    "    self.dropout_prob = dropout\n",
    "\n",
    "  def forward(self, x):\n",
    "    b, num_tokens, d_in = x.shape\n",
    "\n",
    "    x_ttnn = ttnn.from_torch(\n",
    "      x,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self.device\n",
    "    )\n",
    "    keys_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_key_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "    values_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_value_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "    queries_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_query_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "\n",
    "    keys_ttnn_transpose = ttnn.permute(keys_ttnn, (0, 2, 1))\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn,\n",
    "      keys_ttnn_transpose,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "\n",
    "    if self.dropout_prob > 0.0:\n",
    "      inf_ttnn = ttnn.full_like(\n",
    "        attn_scores_ttnn,\n",
    "        MINUS_INFINITY, \n",
    "        layout=ttnn.TILE_LAYOUT\n",
    "      )\n",
    "\n",
    "      attn_scores_ttnn = ttnn.where(self.mask_ttnn[:num_tokens, :num_tokens], inf_ttnn, attn_scores_ttnn)\n",
    "\n",
    "    attn_weights_ttnn = ttnn.softmax(attn_scores_ttnn * (1 / self.d_out ** 0.5), dim=-1)\n",
    "    attn_weights_ttnn = ttnn.experimental.dropout(\n",
    "      attn_weights_ttnn,\n",
    "      seed=123,\n",
    "      probability=self.dropout_prob,\n",
    "      scale=1.0 / (1.0 - self.dropout_prob)\n",
    "    )\n",
    "\n",
    "    context_vec_ttnn = ttnn.matmul(\n",
    "      attn_weights_ttnn,\n",
    "      values_ttnn,\n",
    "      core_grid=ttnn.CoreGrid(y=core_grid_y, x=core_grid_x)\n",
    "    )\n",
    "\n",
    "    context_vec = ttnn.to_torch(context_vec_ttnn, device=self.device)\n",
    "\n",
    "    return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TorchTensor([[[ 0.6523,  1.0547, -0.5898],\n",
       "              [ 0.2852,  0.4609, -0.2578],\n",
       "              [ 0.6641,  1.0547, -0.3750],\n",
       "              [ 0.2500,  0.3809, -0.1504],\n",
       "              [ 0.3457,  0.6094, -0.1982],\n",
       "              [ 0.2285,  0.4258, -0.1572]]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# make a batch\n",
    "batch = torch.reshape(context, [1, context.shape[0], context.shape[1]])\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention_ttnn(d_in, d_out, context_length, 0.5, device)\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.perf_timer import PerfTimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1024, 2048]), 7093.925714492798)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "t = PerfTimer()\n",
    "\n",
    "t.start()\n",
    "torch_tensors = torch.stack([torch.randn(1024, 2048) for _ in range(0, 1000)])\n",
    "t.stop()\n",
    "\n",
    "torch_tensors.shape, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[     0.7871,      1.1834,      1.3771,  ...,     -0.4499,\n",
       "                1.0692,      1.0057],\n",
       "          [    -1.8585,     -2.1064,      0.5071,  ...,     -0.7326,\n",
       "                0.9790,      0.1301],\n",
       "          [     1.0886,      0.4733,     -2.0841,  ...,     -2.2115,\n",
       "                0.1684,      0.6580],\n",
       "          ...,\n",
       "          [    -0.5703,     -0.4417,      1.5339,  ...,     -0.0207,\n",
       "                0.5780,      0.4572],\n",
       "          [    -0.0000,     -0.0000,      0.0000,  ...,      0.0000,\n",
       "                0.0000,      0.0000],\n",
       "          [     0.2907,      0.5297,     -0.3098,  ...,      3.2427,\n",
       "               -0.9388,      1.0972]]], grad_fn=<UnsafeViewBackward0>),\n",
       " torch.Size([1, 1024, 2048]),\n",
       " 58048.393964767456)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "t.start()\n",
    "\n",
    "ca = CausalAttention(2048, 2048, 2048, 0.5)\n",
    "for tensor in torch_tensors:\n",
    "  batch = torch.reshape(tensor, [1, tensor.shape[0], tensor.shape[1]])\n",
    "  result = ca(batch)\n",
    "t.stop()\n",
    "\n",
    "result, result.shape, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[[ 0.7383,  1.1094,  1.2891,  ..., -0.4453,  1.0469,  0.9844],\n",
       "               [ 0.2812,  0.4219,  0.4922,  ..., -0.1699,  0.3984,  0.3750],\n",
       "               [ 0.2197,  0.0210, -0.3965,  ..., -1.2734,  0.6133,  0.6250],\n",
       "               ...,\n",
       "               [-0.0216,  0.0087, -0.0022,  ..., -0.0060,  0.0085,  0.0141],\n",
       "               [-0.0459, -0.0352, -0.0222,  ...,  0.0297,  0.0199, -0.0039],\n",
       "               [-0.0071, -0.0320, -0.0133,  ...,  0.0432,  0.0168,  0.0493]]],\n",
       "             dtype=torch.bfloat16),\n",
       " torch.Size([1, 1024, 2048]),\n",
       " 20172.93071746826)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "t.start()\n",
    "\n",
    "ca = CausalAttention_ttnn(2048, 2048, 2048, 0.5, device)\n",
    "for tensor in torch_tensors:\n",
    "  batch = torch.reshape(tensor, [1, tensor.shape[0], tensor.shape[1]])\n",
    "  result = ca(batch)\n",
    "t.stop()\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "result, result.shape, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Always | WARNING  | Attempting to synchronize Device 0 which is not initialized. Ignoring...\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
