{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa7863",
   "metadata": {},
   "source": [
    "# Pretraining 2: GPT-2 355M "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80f18a",
   "metadata": {},
   "source": [
    "we are going to go big here \n",
    "\n",
    "https://huggingface.co/datasets/PatrickHaller/fineweb-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb4229",
   "metadata": {},
   "source": [
    "This dataset is over 8 GB. So hopefully your internet connection is fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f data/fineweb-3b/README.md ]; then\n",
    "    echo \"Data set not yet downloaded. Downloading now...\"\n",
    "    git clone https://huggingface.co/datasets/PatrickHaller/fineweb-3B data/fineweb-3b\n",
    "else\n",
    "    echo \"Data set is downloaded.\"\n",
    "fi \n",
    "\n",
    "mkdir -p data/fineweb-3b/text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafe08b",
   "metadata": {},
   "source": [
    "THe data set is in parqet format. so we will need to write a conversion script that will convert parquet to CSV text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38523281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "base_path = \"data/fineweb-3b/data\"\n",
    "\n",
    "all_files = os.listdir(base_path)\n",
    "\n",
    "output_path = \"data/fineweb-3b/text\"\n",
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05667e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "    \n",
    "    print(data)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e54bb",
   "metadata": {},
   "source": [
    "Writing all files to output. The order at which these are processed isn't really important. But expect it to be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "\n",
    "    with open(f\"{output_path}/data-{i}.txt\", \"w\") as f:\n",
    "        # Skip the first line\n",
    "        if data.startswith(\"text\\n\"):\n",
    "            data = data[5:]\n",
    "        \n",
    "        f.write(data)\n",
    "\n",
    "    print(f\"Processed: {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b229dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_path}/data-2.txt\") as f:\n",
    "  print(f.readline()) # this is the header\n",
    "  print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e29a5",
   "metadata": {},
   "source": [
    "Now that we have all this data, we will need to create batches for the training set and validation data set. But this can be quite large. Just for this data set we do have enough memory to hold everything with 64 GB in this system. So we will be lazy and just load everything into memory and will do a split.\n",
    "\n",
    "I think Jupyter will run out of memory, so we will have to do this differently. I wrote a utility in C to quickly concatenate the raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../text-builder/text-builder data/fineweb-3b/text data/fineweb-3b/raw_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../text-splitter/text-splitter data/fineweb-3b/raw_data.txt data/fineweb-3b/train_data.txt data/fineweb-3b/val_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52288194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
