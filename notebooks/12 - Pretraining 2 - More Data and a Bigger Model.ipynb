{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa7863",
   "metadata": {},
   "source": [
    "# Pretraining 2: GPT-2 355M "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab917c85",
   "metadata": {},
   "source": [
    "**WARNING**\n",
    "\n",
    "The data set we will be using is very large. In spirit in assuring I show you everything step by step, you will likely run out of memory running each step of the notebook if you do not have at least 64 GB of RAM (my system). \n",
    "\n",
    "I configured my system to have 128GB of swap and traded off performance to get the project done. If you are concerned about your SSD health, then you should probably run this notebook on the cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80f18a",
   "metadata": {},
   "source": [
    "we are going to go big here \n",
    "\n",
    "https://huggingface.co/datasets/PatrickHaller/fineweb-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb4229",
   "metadata": {},
   "source": [
    "This dataset is over 8 GB. So hopefully your internet connection is fast enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -f data/fineweb-3b/README.md ]; then\n",
    "    echo \"Data set not yet downloaded. Downloading now...\"\n",
    "    git clone https://huggingface.co/datasets/PatrickHaller/fineweb-3B data/fineweb-3b\n",
    "else\n",
    "    echo \"Data set is downloaded.\"\n",
    "fi \n",
    "\n",
    "mkdir -p data/fineweb-3b/text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafe08b",
   "metadata": {},
   "source": [
    "THe data set is in parqet format. so we will need to write a conversion script that will convert parquet to CSV text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38523281",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "base_path = \"data/fineweb-3b/data\"\n",
    "\n",
    "all_files = os.listdir(base_path)\n",
    "\n",
    "output_path = \"data/fineweb-3b/text\"\n",
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05667e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "    \n",
    "    print(data)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e54bb",
   "metadata": {},
   "source": [
    "Writing all files to output. The order at which these are processed isn't really important. But expect it to be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "\n",
    "    with open(f\"{output_path}/data-{i}.txt\", \"w\") as f:\n",
    "        # Skip the first line\n",
    "        if data.startswith(\"text\\n\"):\n",
    "            data = data[5:]\n",
    "        \n",
    "        f.write(data)\n",
    "\n",
    "    print(f\"Processed: {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b229dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_path}/data-2.txt\") as f:\n",
    "  print(f.readline()) # this is the header\n",
    "  print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e29a5",
   "metadata": {},
   "source": [
    "Now that we have all this data, we will need to create batches for the training set and validation data set. But this can be quite large. Just for this data set we do have enough memory to hold everything with 64 GB in this system. So we will be lazy and just load everything into memory and will do a split.\n",
    "\n",
    "I think Jupyter will run out of memory, so we will have to do this differently. I wrote a utility in C to quickly concatenate the raw text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../text-builder/text-builder data/fineweb-3b/text data/fineweb-3b/raw_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../text-splitter/text-splitter data/fineweb-3b/raw_data.txt data/fineweb-3b/train_data.txt data/fineweb-3b/val_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52288194",
   "metadata": {},
   "source": [
    "Create tokenized data set\n",
    "\n",
    "This one is going to take a while since the data set is so big. I also recommend you increase the size of your swap file as the amount of data is going to for sure, be over 64 GB -- which is the amount of RAM i have on my system.\n",
    "\n",
    "I increased my swap file to 100 GB. You can do the same with the `increase_swap.sh` helper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.tokenize_data import tokenize\n",
    "\n",
    "# Tokenize the train data\n",
    "tokenize(\n",
    "  \"data/fineweb-3b/train_data.txt\",\n",
    "  \"data/fineweb-3b/train_tokens.txt\"\n",
    ")\n",
    "\n",
    "# tokenize the validation data\n",
    "tokenize(\n",
    "  \"data/fineweb-3b/val_data.txt\",\n",
    "  \"data/fineweb-3b/val_tokens.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fcc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.load_token_data import load_token_data, save_tokens\n",
    "\n",
    "# feed them to create lists\n",
    "train_tokens_list = load_token_data(\"data/finweb-3b/train_tokens.txt\")\n",
    "save_tokens(train_tokens_list, \"data/fineweb-3b/train_tokens.lst\")\n",
    "\n",
    "val_tokens_list = load_token_data(\"data/finweb-3b/val_tokens.txt\")\n",
    "save_tokens(val_tokens_list, \"data/fineweb-3b/val_tokens.lst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619bb07",
   "metadata": {},
   "source": [
    "## GPT-2 355 Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d3b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_355M = {\n",
    "  \"vocab_size\": 50257,   # Vocabulary size\n",
    "  \"context_length\": 1024, # Context length\n",
    "  \"emb_dim\": 1024,        # Embedding dimension (larger than 124M)\n",
    "  \"n_heads\": 16,         # Number of attention heads (larger than 124M)\n",
    "  \"n_layers\": 24,        # Number of layers (larger than 124M)\n",
    "  \"drop_rate\": 0.0,      # Dropout rate\n",
    "  \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba396278",
   "metadata": {},
   "source": [
    "## Loading the Input and Validation Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0116af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preload_dataloaders import load_train_dataloader, load_val_dataloader\n",
    "\n",
    "train_loader = load_train_dataloader(\"data/fineweb-3b/train_loader.dl\")\n",
    "print(\"Loaded train_loader.\")\n",
    "\n",
    "val_loader = load_val_dataloader(\"data/fineweb-3b/val_loader.dl\")\n",
    "print(\"Loaded val_loader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4bbe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_355M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import calc_loss_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loss = calc_loss_loader(train_loader, model)\n",
    "val_loss = calc_loss_loader(val_loader, model)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39ca66",
   "metadata": {},
   "source": [
    "Now it is time to train our 355M model. Here we go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaaf2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.perf_timer import PerfTimer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_355M)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "# We have lots of data, so we can just train for a single epoch.\n",
    "num_epochs = 1\n",
    "\n",
    "timer = PerfTimer()\n",
    "\n",
    "timer.start()\n",
    "train_losses, val_losses = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=50, # eval less frequently\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "timer.stop()\n",
    "\n",
    "print(f\"Took this long to train: {timer.elapsed_ms()} ms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5437a",
   "metadata": {},
   "source": [
    "## Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/gpt2-355M-model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546fc5d",
   "metadata": {},
   "source": [
    "## Reload the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_355M)\n",
    "model.load_state_dict(\n",
    "  torch.load(\"models/gpt2-355M-model.pth\", weights_only=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c51157",
   "metadata": {},
   "source": [
    "## Testing by inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f572493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.perf_timer import PerfTimer\n",
    "from scripts.generate import generate_text_simple\n",
    "\n",
    "perf_timer = PerfTimer()\n",
    "\n",
    "perf_timer.start()\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_355M[\"context_length\"]\n",
    ")\n",
    "perf_timer.stop()\n",
    "\n",
    "print(\"Generated tokens in\", perf_timer.elapsed_ms(), \"ms\")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6942fa",
   "metadata": {},
   "source": [
    "## TTNN\n",
    "\n",
    "now let's load up model weights and perform the inference. this time we do all the same benchmarks as with notebook 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760049a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
