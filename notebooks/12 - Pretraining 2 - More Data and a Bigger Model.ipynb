{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa7863",
   "metadata": {},
   "source": [
    "# Pretraining 2: GPT-2 355M - Dataset Preparation \n",
    "\n",
    "This notebook is independent to the work that we have been doing so far. The goal of this notebook is to just walk through basic dataset preparation for training a bigger GPT-2 model. In this case, we're going to try and train a GPT-2 355M model from scratch. \n",
    "\n",
    "There are 3 different types of datasets we can prepare for our model. They are 100M, 1B, 3B and 10B tokens. Use what is best for your system. GPT-2 was trained closer to 10B tokens, so that would get us the best results, but requires a lot of system RAM to hold the dataset.\n",
    "\n",
    "If you are just interested in learning, 100M is good enough to see the difference and produce something somewhat coherent for a 355M parameter model. This requires much less system memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab917c85",
   "metadata": {},
   "source": [
    "### ⚠️⚠️ WARNING - FOR THOSE WHO WANT TO GO BEYOND 100M TOKENS ️⚠️⚠️\n",
    "\n",
    "If you choose to use a dataset that is very large (ex. 1B+ tokens), this can use _gigabytes_ of memory.\n",
    "\n",
    "When the data is tokenized and turned into tensors, you'll need a lot of headroom on disk and memory to accomodate.\n",
    "\n",
    "In spirit in making sure that I show you everything step by step, you **will likely** run **out of memory** 💣 running each step of the notebook if you **do not have at least** **64 GB of RAM** (such as my system). \n",
    "\n",
    "To get around this memory limitation, I have personally configured my system to have **128 GB** of swap space. Increasing the swap space to utilize it is a trade-off for performance. But it is necessary to do to get this project done if you don't have enough memory. If you are concerned about your SSD health, then you should probably run this notebook on the cloud. I do personally think it is not a big deal though. \n",
    "\n",
    "Reminder, there is a 100M token dataset as an option if you're concerned about all this. I think that would work for most systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928bd3d",
   "metadata": {},
   "source": [
    "## Acquiring the Dataset\n",
    "\n",
    "Let's acquire the dataset. We'll be using `fineweb` family of datasets from HuggingFace. You can grab the dataset manually through a `git clone` from these location:\n",
    "\n",
    "* 100M - https://huggingface.co/datasets/Butanium/fineweb-100m-sample-test-set\n",
    "* 1B - https://huggingface.co/datasets/PatrickHaller/fineweb-1B\n",
    "* 3B - https://huggingface.co/datasets/PatrickHaller/fineweb-3B\n",
    "* 10B - https://huggingface.co/datasets/PatrickHaller/fineweb-10B\n",
    "\n",
    "\n",
    "If you're lazy, you can use this shell script I've written here to trigger the download. The files will be placed in `data/fineweb-xx` within project directory. (`xx` is the number of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d75c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set not yet downloaded. Downloading now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'data/fineweb-100m'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Possible values: fineweb-100m, fineweb-1B, fineweb-3B, fineweb-10B\n",
    "DATASET_NAME=\"fineweb-100m\" # change this to your dataset\n",
    "\n",
    "if [ \"$DATASET_NAME\" == \"fineweb-100m\" ]; then\n",
    "    DATASET_URL=\"https://huggingface.co/datasets/Butanium/fineweb-100m-sample-test-set\"\n",
    "elif [ \"$DATASET_NAME\" == \"fineweb-1b\" ]; then\n",
    "    DATASET_URL=\"https://huggingface.co/datasets/PatrickHaller/fineweb-1B\"\n",
    "elif [ \"$DATASET_NAME\" == \"fineweb-3b\" ]; then\n",
    "    DATASET_URL=\"https://huggingface.co/datasets/PatrickHaller/fineweb-3B\"\n",
    "elif [ \"$DATASET_NAME\" == \"fineweb-10b\" ]; then\n",
    "    DATASET_URL=\"https://huggingface.co/datasets/PatrickHaller/fineweb-10B\"\n",
    "else\n",
    "    DATASET_URL=\"https://huggingface.co/datasets/Butanium/fineweb-100m-sample-test-set\"\n",
    "fi\n",
    "\n",
    "\n",
    "if [ ! -f \"data/$DATASET_NAME/README.md\" ]; then\n",
    "    echo \"Data set not yet downloaded. Downloading now...\"\n",
    "    git clone \"$DATASET_URL\" \"data/$DATASET_NAME\"\n",
    "else\n",
    "    echo \"Data set is downloaded.\"\n",
    "fi \n",
    "\n",
    "mkdir -p \"data/$DATASET_NAME/text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafe08b",
   "metadata": {},
   "source": [
    "The data set is in `parquet` format. so we will need to write a conversion script that will convert `parquet` file to CSV text. \n",
    "\n",
    "To do that, `pandas` and `pyarrow` must be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38523281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pyarrow in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.11/site-packages (20.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14001b08",
   "metadata": {},
   "source": [
    "Let's check out what files are in your chosen `fineweb-xx` data. We'll Perform the conversion soon.\n",
    "\n",
    "First, specify the dataset you will want to be using for pre-training.\n",
    "\n",
    "Possible options are:\n",
    "```\n",
    "fineweb-100m\n",
    "fineweb-1B\n",
    "fineweb-3B\n",
    "fineweb-10B\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3a2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this here\n",
    "dataset_name = 'fineweb-100m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1f69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-00000-of-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "base_path = f\"data/{dataset_name}/data\"\n",
    "\n",
    "all_files = os.listdir(base_path)\n",
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ecf16",
   "metadata": {},
   "source": [
    "What does a single file look like? Let's take a look at the first 100 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05667e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "\"Want to beat your Scrabble friends no matter where you are?WordSolver Android App!\n",
      "1 Definitio\n"
     ]
    }
   ],
   "source": [
    "output_path = f\"data/{dataset_name}/text\"\n",
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "    \n",
    "    print(data[:100])\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e54bb",
   "metadata": {},
   "source": [
    "Writing all files to output. The order at which these are processed isn't really important. But expect it to be large. When converting the file to text, we can also scrub away the first line which is just simply `text\\n`. I don't think it is a big deal to leave it in, but it's so easy to just handle it now, so we might as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e5d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: train-00000-of-00001.parquet\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "\n",
    "    with open(f\"{output_path}/data-{i}.txt\", \"w\") as f:\n",
    "        # Skip the first line\n",
    "        if data.startswith(\"text\\n\"):\n",
    "            data = data[5:]\n",
    "        \n",
    "        f.write(data)\n",
    "\n",
    "    print(f\"Processed: {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e5114",
   "metadata": {},
   "source": [
    "Test to see if the text has been converted and written correctly. Let's just output the first 2 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b229dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Want to beat your Scrabble friends no matter where you are?WordSolver Android App!\n",
      "\n",
      "1 Definition of Rained\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{output_path}/data-0.txt\") as f:\n",
    "  print(f.readline())\n",
    "  print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d7152",
   "metadata": {},
   "source": [
    "## Create the Training and Validation Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e29a5",
   "metadata": {},
   "source": [
    "Now that we have all this data, we will need to create batches for the training set and validation dataset.\n",
    "\n",
    "We can decide to write some fancy streaming/generator thing to accommodate, but I'm not really here for that right now. \n",
    "\n",
    "I am lazy, and for this time, loading everything into memory and doing a split can work. \n",
    "\n",
    "I wrote 2 C programs to do:\n",
    "* `text-builder` - Concatenate all `txt` files to a single `raw_data.txt` file.\n",
    "* `text-splitter` - To split the `raw_data.txt` and create separate `train_data.txt` and `val_data.txt` datasets.\n",
    "\n",
    "The performance is really good. It is way faster than what I can do in Python. It was also worth the time to just get some more C skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77bc296d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1\n",
      "Total size of files: 306659581 bytes\n",
      "Total file size read into memory: 306659582, Number of additional characters for new line: 1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATASET_NAME=\"fineweb-100m\" # Replace this here!!\n",
    "../text-builder/text-builder \"data/$DATASET_NAME/text\" \"data/$DATASET_NAME/raw_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412667c",
   "metadata": {},
   "source": [
    "Split the data into 2 text files. `train_data.txt` and `val_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9060c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters total: 306659582, Split index: 260660656\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATASET_NAME=\"fineweb-100m\" # Replace this here!!\n",
    "../text-splitter/text-splitter \"data/$DATASET_NAME/raw_data.txt\" \"data/$DATASET_NAME/train_data.txt\" \"data/$DATASET_NAME/val_data.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52288194",
   "metadata": {},
   "source": [
    "Create tokenized data set. Basically just call `tiktoken`'s `encode` on the text. The tokens will go into an array, and we can `pickle` it for later use. In fact, I recommend doing so as by now you're also probably running out of memory/swap space to maintain all this data. \n",
    "\n",
    "We will write code to unpickle, and create `torch` tensors Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8fa654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data/fineweb-100m/train_data.txt\n",
      "Lines Read: 10000\n",
      "Lines Read: 20000\n",
      "Lines Read: 30000\n",
      "Lines Read: 40000\n",
      "Lines Read: 50000\n",
      "Lines Read: 60000\n",
      "Lines Read: 70000\n",
      "Lines Read: 80000\n",
      "Lines Read: 90000\n",
      "Lines Read: 100000\n",
      "Lines Read: 110000\n",
      "Lines Read: 120000\n",
      "Lines Read: 130000\n",
      "Lines Read: 140000\n",
      "Lines Read: 150000\n",
      "Lines Read: 160000\n",
      "Lines Read: 170000\n",
      "Lines Read: 180000\n",
      "Lines Read: 190000\n",
      "Lines Read: 200000\n",
      "Lines Read: 210000\n",
      "Lines Read: 220000\n",
      "Lines Read: 230000\n",
      "Lines Read: 240000\n",
      "Lines Read: 250000\n",
      "Lines Read: 260000\n",
      "Lines Read: 270000\n",
      "Lines Read: 280000\n",
      "Lines Read: 290000\n",
      "Lines Read: 300000\n",
      "Lines Read: 310000\n",
      "Lines Read: 320000\n",
      "Lines Read: 330000\n",
      "Lines Read: 340000\n",
      "Lines Read: 350000\n",
      "Lines Read: 360000\n",
      "Lines Read: 370000\n",
      "Lines Read: 380000\n",
      "Lines Read: 390000\n",
      "Lines Read: 400000\n",
      "Lines Read: 410000\n",
      "Lines Read: 420000\n",
      "Lines Read: 430000\n",
      "Lines Read: 440000\n",
      "Lines Read: 450000\n",
      "Lines Read: 460000\n",
      "Lines Read: 470000\n",
      "Lines Read: 480000\n",
      "Lines Read: 490000\n",
      "Lines Read: 500000\n",
      "Lines Read: 510000\n",
      "Lines Read: 520000\n",
      "Lines Read: 530000\n",
      "Lines Read: 540000\n",
      "Lines Read: 550000\n",
      "Lines Read: 560000\n",
      "Lines Read: 570000\n",
      "Lines Read: 580000\n",
      "Lines Read: 590000\n",
      "Lines Read: 600000\n",
      "Lines Read: 610000\n",
      "Lines Read: 620000\n",
      "Lines Read: 630000\n",
      "Lines Read: 640000\n",
      "Lines Read: 650000\n",
      "Lines Read: 660000\n",
      "Lines Read: 670000\n",
      "Lines Read: 680000\n",
      "Lines Read: 690000\n",
      "Lines Read: 700000\n",
      "Lines Read: 710000\n",
      "Lines Read: 720000\n",
      "Lines Read: 730000\n",
      "Lines Read: 740000\n",
      "Lines Read: 750000\n",
      "Lines Read: 760000\n",
      "Lines Read: 770000\n",
      "Lines Read: 780000\n",
      "Lines Read: 790000\n",
      "Lines Read: 800000\n",
      "Lines Read: 810000\n",
      "Lines Read: 820000\n",
      "Lines Read: 830000\n",
      "Lines Read: 840000\n",
      "Lines Read: 850000\n",
      "Lines Read: 860000\n",
      "Lines Read: 870000\n",
      "Lines Read: 880000\n",
      "Lines Read: 890000\n",
      "Lines Read: 900000\n",
      "Lines Read: 910000\n",
      "Lines Read: 920000\n",
      "Lines Read: 930000\n",
      "Lines Read: 940000\n",
      "Lines Read: 950000\n",
      "Lines Read: 960000\n",
      "Lines Read: 970000\n",
      "Lines Read: 980000\n",
      "Lines Read: 990000\n",
      "Lines Read: 1000000\n",
      "Lines Read: 1010000\n",
      "Lines Read: 1020000\n",
      "Lines Read: 1030000\n",
      "Lines Read: 1040000\n",
      "Lines Read: 1050000\n",
      "Lines Read: 1060000\n",
      "Lines Read: 1070000\n",
      "Lines Read: 1080000\n",
      "Lines Read: 1090000\n",
      "Lines Read: 1100000\n",
      "Lines Read: 1110000\n",
      "Lines Read: 1120000\n",
      "Lines Read: 1130000\n",
      "Lines Read: 1140000\n",
      "Lines Read: 1150000\n",
      "Lines Read: 1160000\n",
      "Lines Read: 1170000\n",
      "Lines Read: 1180000\n",
      "Lines Read: 1190000\n",
      "Lines Read: 1200000\n",
      "Lines Read: 1210000\n",
      "Lines Read: 1220000\n",
      "Lines Read: 1230000\n",
      "Lines Read: 1240000\n",
      "Lines Read: 1250000\n",
      "Lines Read: 1260000\n",
      "Lines Read: 1270000\n",
      "Lines Read: 1280000\n",
      "Lines Read: 1290000\n",
      "Lines Read: 1300000\n",
      "Lines Read: 1310000\n",
      "Lines Read: 1320000\n",
      "Lines Read: 1327832\n",
      "Tokenized data/fineweb-100m/train_data.txt. Number of tokens: 58545967. Total lines: 1327832. Time: 15902.745723724365ms\n",
      "Tokenizing data/fineweb-100m/val_data.txt\n",
      "Lines Read: 10000\n",
      "Lines Read: 20000\n",
      "Lines Read: 30000\n",
      "Lines Read: 40000\n",
      "Lines Read: 50000\n",
      "Lines Read: 60000\n",
      "Lines Read: 70000\n",
      "Lines Read: 80000\n",
      "Lines Read: 90000\n",
      "Lines Read: 100000\n",
      "Lines Read: 110000\n",
      "Lines Read: 120000\n",
      "Lines Read: 130000\n",
      "Lines Read: 140000\n",
      "Lines Read: 150000\n",
      "Lines Read: 160000\n",
      "Lines Read: 170000\n",
      "Lines Read: 180000\n",
      "Lines Read: 190000\n",
      "Lines Read: 200000\n",
      "Lines Read: 210000\n",
      "Lines Read: 220000\n",
      "Lines Read: 230000\n",
      "Lines Read: 237857\n",
      "Tokenized data/fineweb-100m/val_data.txt. Number of tokens: 10364666. Total lines: 237857. Time: 2745.1863288879395ms\n"
     ]
    }
   ],
   "source": [
    "from scripts.tokenize_data import tokenize\n",
    "\n",
    "# Tokenize the train data\n",
    "train_tokens = tokenize(\n",
    "  f\"data/{dataset_name}/train_data.txt\",\n",
    "  f\"data/{dataset_name}/train_tokens.lst\"\n",
    ")\n",
    "\n",
    "# Tokenize the validation data\n",
    "val_tokens = tokenize(\n",
    "  f\"data/{dataset_name}/val_data.txt\",\n",
    "  f\"data/{dataset_name}/val_tokens.lst\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619bb07",
   "metadata": {},
   "source": [
    "## GPT-2 355M Config\n",
    "\n",
    "Let's define the GPT-2 configuration. The context length, number of heads, and layers will be increased to increase the overall number of trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d3b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_355M = {\n",
    "  \"vocab_size\": 50257,   # Vocabulary size\n",
    "  \"context_length\": 1024, # Context length\n",
    "  \"emb_dim\": 1024,        # Embedding dimension (larger than 124M)\n",
    "  \"n_heads\": 16,         # Number of attention heads (larger than 124M)\n",
    "  \"n_layers\": 24,        # Number of layers (larger than 124M)\n",
    "  \"drop_rate\": 0.0,      # Dropout rate\n",
    "  \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643b369",
   "metadata": {},
   "source": [
    "## Training and Validation Dataloaders\n",
    "\n",
    "Using the config, let's now use the tokens list to create the dataloaders and pickle them too. We want to pickle so that we can reload it all later and won't have to go through the same pain in buildng this dataset as we had just now.\n",
    "\n",
    "I'd also like to point out that internally `create_train_dataloader` and `create_val_dataloader` operate on a batch size of `4`. If you find that you need to use less memory usage during training, you will need to recreate these data loaders again with a smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01837d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to your desired batch size.\n",
    "# Note: bigger batch size means more VRAM necessary.\n",
    "# Try: 4, 8, 16, 32\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ebc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk: 0. Token: 0 of 58545967\n",
      "Created train_loader.\n"
     ]
    }
   ],
   "source": [
    "from scripts.preload_dataloaders import create_dataloader_to_pickle\n",
    "\n",
    "create_dataloader_to_pickle(\n",
    "  GPT_CONFIG_355M,\n",
    "  f\"data/{dataset_name}/train_tokens.lst\",\n",
    "  f\"data/{dataset_name}/train_loader.dl\",\n",
    "  batch_size=batch_size\n",
    ")\n",
    "print(\"Created train_loader.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f58af5",
   "metadata": {},
   "source": [
    "Create the validation data loader. It's must smaller, and won't take as long. It is good practice to make the validation set unshuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ba4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk: 0. Token: 0 of 10364666\n",
      "Created val_loader.\n"
     ]
    }
   ],
   "source": [
    "create_dataloader_to_pickle(\n",
    "  GPT_CONFIG_355M,\n",
    "  f\"data/{dataset_name}/val_tokens.lst\",\n",
    "  f\"data/{dataset_name}/val_loader.dl\",\n",
    "  batch_size=batch_size,\n",
    "  shuffle=False\n",
    ")\n",
    "print(\"Created val_loader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30166839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
