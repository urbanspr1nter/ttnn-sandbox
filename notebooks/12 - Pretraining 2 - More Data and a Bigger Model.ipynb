{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "effa7863",
   "metadata": {},
   "source": [
    "# Pretraining 2: GPT-2 355M - Dataset Preparation \n",
    "\n",
    "This notebook is independent to the work that we have been doing so far. The goal of this notebook is to just walk through basic dataset preparation for training a bigger GPT-2 model. In this case, we're going to try and train a GPT-2 355M model from scratch. \n",
    "\n",
    "This will involve using a much, much larger dataset with about 3 billion tokens. If you're not interested in prepping the dataset, you can download the prepped dataset already as prepared data loaders here: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab917c85",
   "metadata": {},
   "source": [
    "### ⚠️⚠️ WARNING ️⚠️⚠️\n",
    "\n",
    "The dataset we will be using is very large with about 3 billion tokens and totalling up to about 13 GB of raw uncompressed text.\n",
    "\n",
    "When the data is tokenized and turned into tensors, you'll need about 50 GB of space to store these on disk.\n",
    "\n",
    "In spirit in making sure that I show you everything step by step, you **will likely** run **out of memory** 💣 running each step of the notebook if you **do not have at least** **64 GB of RAM** (such as my system). \n",
    "\n",
    "To get around this memory limitation, I have personally configured my system to have **128 GB** of swap space. Increasing the swap space to utilize it is a trade-off for performance. But it is necessary to do to get this project done if you don't have enough memory. If you are concerned about your SSD health, then you should probably run this notebook on the cloud. I do personally think it is not a big deal though. \n",
    "\n",
    "Overall, you should expect to use about 50 GB for the data set, and 128 GB for swap space. This means that the expectation to get this task done will be _at least_ having 180 GB of SSD space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928bd3d",
   "metadata": {},
   "source": [
    "## Acquiring the Dataset\n",
    "\n",
    "Let's acquire the dataset. We'll be using `fineweb-3B` from HuggingFace. You can grab the dataset manually through a `git clone` from this location:\n",
    "\n",
    "https://huggingface.co/datasets/PatrickHaller/fineweb-3B\n",
    "\n",
    "If you're lazy, you can use this shell script I've written here to trigger the download. The files will be played in `data/fineweb-3b` within project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d75c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set is downloaded.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [ ! -f data/fineweb-3b/README.md ]; then\n",
    "    echo \"Data set not yet downloaded. Downloading now...\"\n",
    "    git clone https://huggingface.co/datasets/PatrickHaller/fineweb-3B data/fineweb-3b\n",
    "else\n",
    "    echo \"Data set is downloaded.\"\n",
    "fi \n",
    "\n",
    "mkdir -p data/fineweb-3b/text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafe08b",
   "metadata": {},
   "source": [
    "The data set is in `parquet` format. so we will need to write a conversion script that will convert `parquet` file to CSV text. This is also going to also need a lot of storage on the computer in addition to the original 8 GB dataset. Expect the converted files to total around 13 GB. \n",
    "\n",
    "To do that, `pandas` and `pyarrow` must be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38523281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /home/rngo/code/ttnn-sandbox/.venv/lib/python3.12/site-packages (20.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14001b08",
   "metadata": {},
   "source": [
    "Let's check out what files are in the `fineweb-3b` data. We'll Perform the conversion soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1f69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-00000-of-00028.parquet\n",
      "train-00016-of-00028.parquet\n",
      "train-00027-of-00028.parquet\n",
      "train-00021-of-00028.parquet\n",
      "train-00008-of-00028.parquet\n",
      "train-00009-of-00028.parquet\n",
      "train-00018-of-00028.parquet\n",
      "train-00001-of-00028.parquet\n",
      "train-00026-of-00028.parquet\n",
      "train-00019-of-00028.parquet\n",
      "train-00002-of-00028.parquet\n",
      "train-00023-of-00028.parquet\n",
      "train-00006-of-00028.parquet\n",
      "train-00004-of-00028.parquet\n",
      "train-00012-of-00028.parquet\n",
      "train-00003-of-00028.parquet\n",
      "train-00015-of-00028.parquet\n",
      "train-00025-of-00028.parquet\n",
      "train-00024-of-00028.parquet\n",
      "train-00005-of-00028.parquet\n",
      "train-00017-of-00028.parquet\n",
      "train-00013-of-00028.parquet\n",
      "train-00010-of-00028.parquet\n",
      "train-00022-of-00028.parquet\n",
      "train-00020-of-00028.parquet\n",
      "train-00007-of-00028.parquet\n",
      "train-00011-of-00028.parquet\n",
      "train-00014-of-00028.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "base_path = \"data/fineweb-3b/data\"\n",
    "\n",
    "all_files = os.listdir(base_path)\n",
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ecf16",
   "metadata": {},
   "source": [
    "What does a single file look like? Let's take a look at the first 100 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05667e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "\"However it is not simply those who are traditions away option plans to help you wedding just w\n"
     ]
    }
   ],
   "source": [
    "output_path = \"data/fineweb-3b/text\"\n",
    "\n",
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "    \n",
    "    print(data[:100])\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e54bb",
   "metadata": {},
   "source": [
    "Writing all files to output. The order at which these are processed isn't really important. But expect it to be large. When converting the file to text, we can also scrub away the first line which is just simply `text\\n`. I don't think it is a big deal to leave it in, but it's so easy to just handle it now, so we might as well..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e5d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, filename in enumerate(all_files):\n",
    "    fullpath = f\"{base_path}/{filename}\"\n",
    "\n",
    "    df = pd.read_parquet(fullpath)\n",
    "\n",
    "    data = df[\"text\"].to_csv(index=False)\n",
    "\n",
    "    with open(f\"{output_path}/data-{i}.txt\", \"w\") as f:\n",
    "        # Skip the first line\n",
    "        if data.startswith(\"text\\n\"):\n",
    "            data = data[5:]\n",
    "        \n",
    "        f.write(data)\n",
    "\n",
    "    print(f\"Processed: {filename}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e5114",
   "metadata": {},
   "source": [
    "Test to see if the text has been converted and written correctly. Let's just output the first 2 lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b229dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{output_path}/data-2.txt\") as f:\n",
    "  print(f.readline())\n",
    "  print(f.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d7152",
   "metadata": {},
   "source": [
    "## Create the Training and Validation Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e29a5",
   "metadata": {},
   "source": [
    "Now that we have all this data, we will need to create batches for the training set and validation dataset. But this can be quite large. Just for this dataset alone, we do have enough memory to hold everything with only 64 GB of RAM in this system.\n",
    "\n",
    "We can decide to write some fancy streaming/generator thing to accommodate, but I'm not really here for that right now. \n",
    "\n",
    "I am lazy, and for this time, loading everything into memory and doing a split can work. \n",
    "\n",
    "I wrote 2 C programs to do:\n",
    "* `text-builder` - Concatenate all `txt` files to a single `raw_data.txt` file.\n",
    "* `text-splitter` - To split the `raw_data.txt` and create separate `train_data.txt` and `val_data.txt` datasets.\n",
    "\n",
    "The performance is really good. It is way faster than what I can do in Python. It was also worth the time to just get some more C skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc296d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../text-builder/text-builder data/fineweb-3b/text data/fineweb-3b/raw_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1412667c",
   "metadata": {},
   "source": [
    "Split the data into 2 text files. `train_data.txt` and `val_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "../text-splitter/text-splitter data/fineweb-3b/raw_data.txt data/fineweb-3b/train_data.txt data/fineweb-3b/val_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52288194",
   "metadata": {},
   "source": [
    "Create tokenized data set. Basically just call `tiktoken`'s `encode` on the text. The tokens will go into an array, and we can `pickle` it for later use. In fact, I recommend doing so as by now you're also probably running out of memory/swap space to maintain all this data. \n",
    "\n",
    "We will write code to unpickle, and create `torch` tensors Dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.tokenize_data import tokenize\n",
    "\n",
    "# Tokenize the train data\n",
    "train_tokens = tokenize(\n",
    "  \"data/fineweb-3b/train_data.txt\",\n",
    "  \"data/fineweb-3b/train_tokens.lst\"\n",
    ")\n",
    "\n",
    "# Tokenize the validation data\n",
    "val_tokens = tokenize(\n",
    "  \"data/fineweb-3b/val_data.txt\",\n",
    "  \"data/fineweb-3b/val_tokens.lst\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619bb07",
   "metadata": {},
   "source": [
    "## GPT-2 355M Config\n",
    "\n",
    "Let's define the GPT-2 configuration. The context length, number of heads, and layers will be increased to increase the overall number of trainable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3b1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_355M = {\n",
    "  \"vocab_size\": 50257,   # Vocabulary size\n",
    "  \"context_length\": 1024, # Context length\n",
    "  \"emb_dim\": 1024,        # Embedding dimension (larger than 124M)\n",
    "  \"n_heads\": 16,         # Number of attention heads (larger than 124M)\n",
    "  \"n_layers\": 24,        # Number of layers (larger than 124M)\n",
    "  \"drop_rate\": 0.0,      # Dropout rate\n",
    "  \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643b369",
   "metadata": {},
   "source": [
    "## Training and Validation Dataloaders\n",
    "\n",
    "Using the config, let's now use the tokens list to create the dataloaders and pickle them too. We want to pickle so that we can reload it all later and won't have to go through the same pain in buildng this dataset as we had just now.\n",
    "\n",
    "I'd also like to point out that internally `create_train_dataloader` and `create_val_dataloader` operate on a batch size of `4`. If you find that you need to use less memory usage during training, you will need to recreate these data loaders again with a smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ebc103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk: 0. Token: 0 of 2527323724\n",
      "Processing chunk: 100000. Token: 102400000 of 2527323724\n",
      "Processing chunk: 200000. Token: 204800000 of 2527323724\n",
      "Processing chunk: 300000. Token: 307200000 of 2527323724\n",
      "Processing chunk: 400000. Token: 409600000 of 2527323724\n",
      "Processing chunk: 500000. Token: 512000000 of 2527323724\n",
      "Processing chunk: 600000. Token: 614400000 of 2527323724\n",
      "Processing chunk: 700000. Token: 716800000 of 2527323724\n",
      "Processing chunk: 800000. Token: 819200000 of 2527323724\n",
      "Processing chunk: 900000. Token: 921600000 of 2527323724\n",
      "Processing chunk: 1000000. Token: 1024000000 of 2527323724\n",
      "Processing chunk: 1100000. Token: 1126400000 of 2527323724\n",
      "Processing chunk: 1200000. Token: 1228800000 of 2527323724\n",
      "Processing chunk: 1300000. Token: 1331200000 of 2527323724\n",
      "Processing chunk: 1400000. Token: 1433600000 of 2527323724\n",
      "Processing chunk: 1500000. Token: 1536000000 of 2527323724\n",
      "Processing chunk: 1600000. Token: 1638400000 of 2527323724\n",
      "Processing chunk: 1700000. Token: 1740800000 of 2527323724\n",
      "Processing chunk: 1800000. Token: 1843200000 of 2527323724\n",
      "Processing chunk: 1900000. Token: 1945600000 of 2527323724\n",
      "Processing chunk: 2000000. Token: 2048000000 of 2527323724\n",
      "Processing chunk: 2100000. Token: 2150400000 of 2527323724\n",
      "Processing chunk: 2200000. Token: 2252800000 of 2527323724\n",
      "Processing chunk: 2300000. Token: 2355200000 of 2527323724\n",
      "Processing chunk: 2400000. Token: 2457600000 of 2527323724\n",
      "Created train_loader.\n"
     ]
    }
   ],
   "source": [
    "from scripts.preload_dataloaders import create_dataloader_to_pickle\n",
    "\n",
    "create_dataloader_to_pickle(\n",
    "  GPT_CONFIG_355M,\n",
    "  \"data/fineweb-3b/train_tokens.lst\",\n",
    "  \"data/fineweb-3b/train_loader.dl\",\n",
    "  batch_size=4\n",
    ")\n",
    "print(\"Created train_loader.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f58af5",
   "metadata": {},
   "source": [
    "Create the validation data loader. It's must smaller, and won't take as long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09ba4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk: 0. Token: 0 of 446181160\n",
      "Processing chunk: 100000. Token: 102400000 of 446181160\n",
      "Processing chunk: 200000. Token: 204800000 of 446181160\n",
      "Processing chunk: 300000. Token: 307200000 of 446181160\n",
      "Processing chunk: 400000. Token: 409600000 of 446181160\n",
      "Created val_loader.\n"
     ]
    }
   ],
   "source": [
    "create_dataloader_to_pickle(\n",
    "  GPT_CONFIG_355M,\n",
    "  \"data/fineweb-3b/val_tokens.lst\",\n",
    "  \"data/fineweb-3b/val_loader.dl\",\n",
    "  batch_size=4\n",
    ")\n",
    "print(\"Created val_loader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30166839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
