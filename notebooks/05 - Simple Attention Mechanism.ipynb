{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Very Simple Attention Mechanism\n",
    "\n",
    "This tutorial will demonstrate how we can use `ttnn` to create a simple, and naive attention mechanism to calculate context vectors from input embeddings. \n",
    "\n",
    "This code, like the Embeddings notebook has been adapted from  [Sebastian Raschka](https://github.com/rasbt)'s [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) repository. Please check it out. He is a huge inspiration for the LLM-related work in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Attention Mechanisms\n",
    "\n",
    "In a previous tutorial, we had calculated some input embeddings with trainable weights. It is now time to put those embeddings to use by creating a simple attention mechanism.\n",
    "\n",
    "So here's our first fact:\n",
    "> Attention mechanisms start from an input vector sourced from the **input embeddings**.\n",
    "\n",
    "As a reminder, the input embeddings tensors had a dimensions like: (B, C, D) where B is the batch size, while C is the dimension representing the context length, and D is the embedding dimension. This means that for every element in batch B, we have a tensor that is of C context length. Within an element within the C dimension, we have D number of elements representing embedding information. \n",
    "\n",
    "Yes, a lot of digest, but we can break it down further in this notebook.\n",
    "\n",
    "To create a _very simple_ attention mechansim, we'll need some terminology.\n",
    "\n",
    "1. A _query_ is the vector given some token in context within input embeddings tensor. Essentially it is the corresponding embeddings vector across the D dimension for a given token. We can call this `q_i`, where `i` is the ith element in the input embeddings tensor.\n",
    "2. An **attention score** is the result of the dot product between the given query's embedding vector against another token in the same context. We attention scores for all of the tokens in relative to the query. At the end the number of elements in the attention score vector is value of dimension C.\n",
    "3. The **attention weight** is just the normalization of attention scores. Within the entire context row, all elements should sum to 1.0.\n",
    "4. The **context vector** is the result of the matrix multiplication between the attention weights and the input embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Setup\n",
    "\n",
    "We'll first start with a `torch`-only implementation. Once we the basic approach figured out, we can do the same exercise, but with `ttnn`. \n",
    "\n",
    "First thing's first, let's import `torch`, set a manual seed and define a `batch`. This is the element at some index of dimension `B`. What results is a tensor of (C, D) shape. In this case, (6, 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting an Item for Query\n",
    "\n",
    "Given our `context`, we would like to compute for the second token in there. This corresponds to the word `journey`. \n",
    "\n",
    "The selected token in which we want to compute the attention for is called the `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3 = context[1] # select the second token in the context\n",
    "query_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the attention scores first by taking the dot product of the query against all other tokens and their values across the embedding dimension. \n",
    "\n",
    "The resulting length of the attn_scores will match the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.empty(len(context))\n",
    "for i, t_i in enumerate(context):\n",
    "  attn_scores[i] = torch.dot(query_3, t_i)\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we normalize the attention scores by using `softmax`. This becomes the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can take ths and softmax across the last dimension\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see if the weights sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did it sum to 1?\n",
    "attn_weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector\n",
    "\n",
    "We'll now compute the context vector in relation to the query. We just multiply each attention weight with the corresponding token value within the existing context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets take teh context vector\n",
    "context_vec = torch.zeros(len(query_3))\n",
    "for i, c_i in enumerate(context):\n",
    "  context_vec += attn_weights[i] * c_i\n",
    "\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, that's how we compute the context vector for a single token. Can we generalize this across the entire context for other elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector for All Tokens in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just need to take the matrix multiplication of the tensor in context and its transposed form. This is equivalent to performing the dot product of a query and the rest of the tokens in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generalize the attention weights\n",
    "attn_scores = first_batch @ first_batch.T\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is easy, just again, apply `softmax` across the last dimension of the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we'll find the context vectors by performing a matrix multiplication of the attention weights and all tokens in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs = attn_weights @ context\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector in tt-nn\n",
    "\n",
    "It's time to do this same exercise but now using the `ttnn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
