{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Very Simple Attention Mechanism\n",
    "\n",
    "This tutorial will demonstrate how we can use `ttnn` to create a simple, and naive attention mechanism to calculate context vectors from input embeddings. \n",
    "\n",
    "This code, like the Embeddings notebook has been adapted from  [Sebastian Raschka](https://github.com/rasbt)'s [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) repository. Please check it out. He is a huge inspiration for the LLM-related work in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Attention Mechanisms\n",
    "\n",
    "In a previous tutorial, we had calculated some input embeddings with trainable weights. It is now time to put those embeddings to use by creating a simple attention mechanism.\n",
    "\n",
    "So here's our first fact:\n",
    "> Attention mechanisms start from an input vector sourced from the **input embeddings**.\n",
    "\n",
    "As a reminder, the input embeddings tensors had a dimensions like: (B, C, D) where B is the batch size, while C is the dimension representing the context length, and D is the embedding dimension. This means that for every element in batch B, we have a tensor that is of C context length. Within an element within the C dimension, we have D number of elements representing embedding information. \n",
    "\n",
    "Yes, a lot of digest, but we can break it down further in this notebook.\n",
    "\n",
    "To create a _very simple_ attention mechansim, we'll need some terminology.\n",
    "\n",
    "1. A _query_ is the vector given some token in context within input embeddings tensor. Essentially it is the corresponding embeddings vector across the D dimension for a given token. We can call this `q_i`, where `i` is the ith element in the input embeddings tensor.\n",
    "2. An **attention score** is the result of the dot product between the given query's embedding vector against another token in the same context. We attention scores for all of the tokens in relative to the query. At the end the number of elements in the attention score vector is value of dimension C.\n",
    "3. The **attention weight** is just the normalization of attention scores. Within the entire context row, all elements should sum to 1.0.\n",
    "4. The **context vector** is the result of the matrix multiplication between the attention weights and the input embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and Setup\n",
    "\n",
    "We'll first start with a `torch`-only implementation. Once we the basic approach figured out, we can do the same exercise, but with `ttnn`. \n",
    "\n",
    "First thing's first, let's import `torch`, set a manual seed and define a `batch`. This is the element at some index of dimension `B`. What results is a tensor of (C, D) shape. In this case, (6, 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "context.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting an Item for Query\n",
    "\n",
    "Given our `context`, we would like to compute for the second token in there. This corresponds to the word `journey`. \n",
    "\n",
    "The selected token in which we want to compute the attention for is called the `query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_3 = context[1] # select the second token in the context\n",
    "query_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the attention scores first by taking the dot product of the query against all other tokens and their values across the embedding dimension. \n",
    "\n",
    "The resulting length of the attn_scores will match the context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = torch.empty(len(context))\n",
    "for i, t_i in enumerate(context):\n",
    "  attn_scores[i] = torch.dot(query_3, t_i)\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we normalize the attention scores by using `softmax`. This becomes the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can take ths and softmax across the last dimension\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see if the weights sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# did it sum to 1?\n",
    "attn_weights.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector\n",
    "\n",
    "We'll now compute the context vector in relation to the query. We just multiply each attention weight with the corresponding token value within the existing context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets take teh context vector\n",
    "context_vec = torch.zeros(len(query_3))\n",
    "for i, c_i in enumerate(context):\n",
    "  context_vec += attn_weights[i] * c_i\n",
    "\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, that's how we compute the context vector for a single token. Can we generalize this across the entire context for other elements?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector for All Tokens in Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will just need to take the matrix multiplication of the tensor in context and its transposed form. This is equivalent to performing the dot product of a query and the rest of the tokens in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
       "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
       "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
       "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
       "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
       "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generalize the attention weights\n",
    "attn_scores = context @ context.T\n",
    "\n",
    "attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is easy, just again, apply `softmax` across the last dimension of the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we'll find the context vectors by performing a matrix multiplication of the attention weights and all tokens in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs = attn_weights @ context\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Vector in tt-nn\n",
    "\n",
    "It's time to do this same exercise but now using the `ttnn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TTNN_CONFIG_OVERRIDES\"] = \"{\\\"enable_fast_runtime_mode\\\": false}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we import all our dependencies, and create a context for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:50:25.284 | DEBUG    | ttnn:<module>:80 - Loading ttnn configuration overrides from environment variable TTNN_CONFIG_OVERRIDES\n",
      "2025-04-20 16:50:25.285 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=false,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from ttnn.tracer import trace, visualize\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "context.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to create `ttnn` tensors of both context and the transposed version of it. Note that the hardware in the Tenstorrent device will read in the data as `bfloat16`. Some precision changes will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:50:26.765 | DEBUG    | ttnn.tracer:visualize:443 - Dumping graph of the model to None\n",
      "2025-04-20 16:50:26.768 | DEBUG    | ttnn.tracer:visualize:443 - Dumping graph of the model to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Always | INFO     | Begin op: tt::tt_metal::detail::convert_python_tensor_to_tt_tensor\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<pybind11::handle const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Layout>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Tile> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | End op: tt::tt_metal::detail::convert_python_tensor_to_tt_tensor\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | Begin op: ttnn::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::MemoryConfig> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | Begin op: Tensor::pad\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<float>\n",
      "                 Always | INFO     | End op: Tensor::pad\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | Begin op: Tensor::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | End op: Tensor::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | Begin op: ttnn::experimental::view\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | Begin op: Tensor::reshape\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | End op: Tensor::reshape\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | End op: ttnn::experimental::view\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | End op: ttnn::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | Begin op: tt::tt_metal::detail::convert_python_tensor_to_tt_tensor\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<pybind11::handle const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Layout>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Tile> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | End op: tt::tt_metal::detail::convert_python_tensor_to_tt_tensor\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | Begin op: ttnn::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::MemoryConfig> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | Begin op: Tensor::pad\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::BorrowedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<float>\n",
      "                 Always | INFO     | End op: Tensor::pad\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | Begin op: Tensor::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | End op: Tensor::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | Begin op: ttnn::experimental::view\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | Begin op: Tensor::reshape\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | End op: Tensor::reshape\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | End op: ttnn::experimental::view\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | End op: ttnn::to_layout\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"422pt\" height=\"222pt\"\n",
       " viewBox=\"0.00 0.00 422.00 222.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 218)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-218 418,-218 418,4 -4,4\"/>\n",
       "<!-- torch_input_0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>torch_input_0</title>\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"180,-214 112,-214 112,-147 180,-147 180,-214\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-197.5 113,-212.5 179,-212.5 179,-197.5 113,-197.5\"/>\n",
       "<text text-anchor=\"start\" x=\"116\" y=\"-202.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.Tensor</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-147.5 113,-196.5 179,-196.5 179,-147.5 113,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"124.5\" y=\"-186.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"133\" y=\"-175.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(6, 3)</text>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-164.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>ttnn.from_torch_5</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"/operation_buffer_report/1\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"198,-100 0,-100 0,0 198,0 198,-100\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1,-1 1,-99 93,-99 93,-1 1,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"3\" y=\"-47.5\" font-family=\"Linux libertine\" font-size=\"10.00\">1: ttnn.from_torch</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"94,-50 94,-99 197,-99 197,-50 94,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"128.5\" y=\"-89\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-78\" font-family=\"Linux libertine\" font-size=\"10.00\">(6, 3)</text>\n",
       "<text text-anchor=\"start\" x=\"114.5\" y=\"-67\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"94,-1 94,-49 197,-49 197,-1 94,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"124\" y=\"-39\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"113.5\" y=\"-28\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-17\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"115.5\" y=\"-6\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- torch_input_0&#45;&gt;ttnn.from_torch_5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>torch_input_0:#0&#45;&gt;ttnn.from_torch_5:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M146,-147C146,-130.03 146,-123.4 146,-110.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"149.5,-110 146,-100 142.5,-110 149.5,-110\"/>\n",
       "<text text-anchor=\"middle\" x=\"161.5\" y=\"-121\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- torch_input_6 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>torch_input_6</title>\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"396,-214 328,-214 328,-147 396,-147 396,-214\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"329,-197.5 329,-212.5 395,-212.5 395,-197.5 329,-197.5\"/>\n",
       "<text text-anchor=\"start\" x=\"332\" y=\"-202.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.Tensor</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"329,-147.5 329,-196.5 395,-196.5 395,-147.5 329,-147.5\"/>\n",
       "<text text-anchor=\"start\" x=\"340.5\" y=\"-186.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"349\" y=\"-175.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(3, 6)</text>\n",
       "<text text-anchor=\"start\" x=\"331\" y=\"-164.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_11 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ttnn.from_torch_11</title>\n",
       "<g id=\"a_node4\"><a xlink:href=\"/operation_buffer_report/2\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"414,-100 216,-100 216,0 414,0 414,-100\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"217,-1 217,-99 309,-99 309,-1 217,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"219\" y=\"-47.5\" font-family=\"Linux libertine\" font-size=\"10.00\">2: ttnn.from_torch</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"310,-50 310,-99 413,-99 413,-50 310,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"344.5\" y=\"-89\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"348.5\" y=\"-78\" font-family=\"Linux libertine\" font-size=\"10.00\">(3, 6)</text>\n",
       "<text text-anchor=\"start\" x=\"330.5\" y=\"-67\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"310,-1 310,-49 413,-49 413,-1 310,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"340\" y=\"-39\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"329.5\" y=\"-28\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([3, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-17\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"331.5\" y=\"-6\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- torch_input_6&#45;&gt;ttnn.from_torch_11 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>torch_input_6:#0&#45;&gt;ttnn.from_torch_11:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M362,-147C362,-130.03 362,-123.4 362,-110.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"365.5,-110 362,-100 358.5,-110 365.5,-110\"/>\n",
       "<text text-anchor=\"middle\" x=\"377.5\" y=\"-121\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7907c9bc1e70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with trace():\n",
    "  context_ttnn = ttnn.from_torch(context, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT)\n",
    "  context_transposed_ttnn = ttnn.from_torch(context.T, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT)\n",
    "\n",
    "visualize(context_ttnn)\n",
    "visualize(context_transposed_ttnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have everything set up to compute the context vectors. The steps are:\n",
    "1. Transfer the context and transposed context to the device\n",
    "2. Compute the attention scores using `ttnn.matmul`. Assume both the context and context transposed are already in `TILE_LAYOUT`.\n",
    "3. Perform softmax on the resulting matrix of (2)\n",
    "4. Compute the context vectors using `ttnn.matmul` against the attention weights and context in hardware.\n",
    "5. Transfer the context vectors back to the host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n",
      "\u001b[32m2025-04-20 16:50:28.438\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-04-20 16:50:28.449\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-04-20 16:50:28.451\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-04-20 16:50:28.451\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-04-20 16:50:28.452\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-04-20 16:50:28.452\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-04-20 16:50:28.478\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                 Always | INFO     | Begin op: Tensor::to_device\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | End op: Tensor::to_device\n",
      "                 Always | INFO     | Begin op: Tensor::to_device\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | End op: Tensor::to_device\n",
      "                 Always | INFO     | Begin op: ttnn::matmul\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::MemoryConfig const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<ttnn::operations::matmul::MatmulMultiCoreProgramConfig, ttnn::operations::matmul::MatmulMultiCoreNonOptimizedReuseProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseMultiCastProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseMultiCast1DProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<ttnn::GrayskullComputeKernelConfig, ttnn::WormholeComputeKernelConfig> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<ttnn::types::CoreGrid const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Tile const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<std::__1::monostate, tt::tt_metal::experimental::GlobalCircularBuffer, ttnn::global_circular_buffer::MultiDeviceGlobalCircularBuffer> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::stl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag>> const>\n",
      "                 Always | INFO     | Begin op: ttnn::prim::old_infra_device_operation\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::stl::StrongType<unsigned char, ttnn::QueueIdTag>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::DeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<std::__1::optional<tt::tt_metal::Tensor const>, std::__1::allocator<std::__1::optional<tt::tt_metal::Tensor const>>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<std::__1::optional<tt::tt_metal::Tensor>, std::__1::allocator<std::__1::optional<tt::tt_metal::Tensor>>> const>\n",
      "                 Always | INFO     | Begin op: Matmul\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::DeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::OldInfraDeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>>::tensor_args_t const>\n",
      "                 Always | INFO     | Begin op: tt::tt_metal::create_device_tensor\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::DataType>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | End op: tt::tt_metal::create_device_tensor\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                 Always | INFO     | End op: Matmul\n",
      "                 Always | INFO     | End op: ttnn::prim::old_infra_device_operation\n",
      "                 Always | INFO     | End op: ttnn::matmul\n",
      "                 Always | INFO     | Begin op: ttnn::softmax\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<signed char const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::MemoryConfig> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<ttnn::GrayskullComputeKernelConfig, ttnn::WormholeComputeKernelConfig> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | Begin op: ttnn::reshape\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::stl::StrongType<unsigned char, ttnn::QueueIdTag> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | Begin op: ttnn::experimental::view\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | Begin op: Tensor::reshape\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | End op: Tensor::reshape\n",
      "                 Always | INFO     | End op: ttnn::experimental::view\n",
      "                 Always | INFO     | End op: ttnn::reshape\n",
      "                 Always | INFO     | Begin op: ttnn::prim::old_infra_device_operation\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::stl::StrongType<unsigned char, ttnn::QueueIdTag>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::DeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<std::__1::optional<tt::tt_metal::Tensor const>, std::__1::allocator<std::__1::optional<tt::tt_metal::Tensor const>>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<std::__1::optional<tt::tt_metal::Tensor>, std::__1::allocator<std::__1::optional<tt::tt_metal::Tensor>>> const>\n",
      "                 Always | INFO     | Begin op: Softmax\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::DeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::OldInfraDeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>>::tensor_args_t const>\n",
      "                 Always | INFO     | Begin op: tt::tt_metal::create_device_tensor\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::DataType>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | End op: tt::tt_metal::create_device_tensor\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n",
      "                 Always | INFO     | End op: Softmax\n",
      "                 Always | INFO     | End op: ttnn::prim::old_infra_device_operation\n",
      "                 Always | INFO     | Begin op: ttnn::reshape\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::stl::StrongType<unsigned char, ttnn::QueueIdTag> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | Begin op: ttnn::experimental::view\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape>\n",
      "                 Always | INFO     | Begin op: Tensor::reshape\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | End op: Tensor::reshape\n",
      "                 Always | INFO     | End op: ttnn::experimental::view\n",
      "                 Always | INFO     | End op: ttnn::reshape\n",
      "                 Always | INFO     | End op: ttnn::softmax\n",
      "                 Always | INFO     | Begin op: ttnn::matmul\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::MemoryConfig const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::DataType const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<ttnn::operations::matmul::MatmulMultiCoreProgramConfig, ttnn::operations::matmul::MatmulMultiCoreNonOptimizedReuseProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseMultiCastProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseMultiCast1DProgramConfig, ttnn::operations::matmul::MatmulMultiCoreReuseMultiCastDRAMShardedProgramConfig> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<ttnn::GrayskullComputeKernelConfig, ttnn::WormholeComputeKernelConfig> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<ttnn::types::CoreGrid const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::tt_metal::Tile const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<std::__1::variant<std::__1::monostate, tt::tt_metal::experimental::GlobalCircularBuffer, ttnn::global_circular_buffer::MultiDeviceGlobalCircularBuffer> const> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::optional<tt::stl::StrongType<unsigned char, tt::tt_metal::SubDeviceIdTag>> const>\n",
      "                 Always | INFO     | Begin op: ttnn::prim::old_infra_device_operation\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::stl::StrongType<unsigned char, ttnn::QueueIdTag>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::DeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>>>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<std::__1::optional<tt::tt_metal::Tensor const>, std::__1::allocator<std::__1::optional<tt::tt_metal::Tensor const>>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<std::__1::vector<std::__1::optional<tt::tt_metal::Tensor>, std::__1::allocator<std::__1::optional<tt::tt_metal::Tensor>>> const>\n",
      "                 Always | INFO     | Begin op: Matmul\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::DeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>> const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::operation::OldInfraDeviceOperation<std::__1::vector<tt::tt_metal::Tensor, std::__1::allocator<tt::tt_metal::Tensor>>>::tensor_args_t const>\n",
      "                 Always | INFO     | Begin op: tt::tt_metal::create_device_tensor\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Shape const>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::DataType>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::Layout>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::IDevice*>\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<tt::tt_metal::MemoryConfig const>\n",
      "                 Always | INFO     | End op: tt::tt_metal::create_device_tensor\n",
      "                 Always | INFO     | End op: Matmul\n",
      "                 Always | INFO     | End op: ttnn::prim::old_infra_device_operation\n",
      "                 Always | INFO     | End op: ttnn::matmul\n",
      "                 Always | INFO     | Begin op: Tensor::cpu\n",
      "                 Always | INFO     | input any type name ignored: std::__1::reference_wrapper<bool>\n",
      "                 Always | INFO     | End op: Tensor::cpu\n",
      "                 Always | INFO     | Tensor doesn't have buffer, but storage is tt::tt_metal::OwnedStorage\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New chip! We now have 1 chips\n",
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[ 0.44141,  0.58984,  0.57812],\n",
       "             [ 0.43359,  0.64844,  0.56641],\n",
       "             ...,\n",
       "             [ 0.46094,  0.58594,  0.52344],\n",
       "             [ 0.41602,  0.64844,  0.55859]], shape=Shape([6, 3]), dtype=DataType::BFLOAT16, layout=Layout::TILE)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "with trace():\n",
    "  context_ttnn = ttnn.to_device(context_ttnn, device)\n",
    "  context_transposed_ttnn = ttnn.to_device(context_transposed_ttnn, device)\n",
    "\n",
    "  attn_scores_ttnn = ttnn.matmul(context_ttnn, context_transposed_ttnn)\n",
    "  attn_weights_ttnn = ttnn.softmax(attn_scores_ttnn, dim=-1)\n",
    "\n",
    "  context_vecs_ttnn = ttnn.matmul(attn_weights_ttnn, context_ttnn)\n",
    "\n",
    "  context_vecs_ttnn_host = ttnn.from_device(context_vecs_ttnn)\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "context_vecs_ttnn_host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below visualizes the entire workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 16:50:32.114 | DEBUG    | ttnn.tracer:visualize:443 - Dumping graph of the model to None\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"549pt\" height=\"1040pt\"\n",
       " viewBox=\"0.00 0.00 549.00 1040.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1036)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-1036 545,-1036 545,4 -4,4\"/>\n",
       "<!-- torch_input_0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>torch_input_0</title>\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"199,-1032 131,-1032 131,-965 199,-965 199,-1032\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"132,-1015.5 132,-1030.5 198,-1030.5 198,-1015.5 132,-1015.5\"/>\n",
       "<text text-anchor=\"start\" x=\"135\" y=\"-1020.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.Tensor</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"132,-965.5 132,-1014.5 198,-1014.5 198,-965.5 132,-965.5\"/>\n",
       "<text text-anchor=\"start\" x=\"143.5\" y=\"-1004.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-993.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(6, 3)</text>\n",
       "<text text-anchor=\"start\" x=\"134\" y=\"-982.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>ttnn.from_torch_5</title>\n",
       "<g id=\"a_node2\"><a xlink:href=\"/operation_buffer_report/1\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"217,-918 19,-918 19,-818 217,-818 217,-918\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"20,-819 20,-917 112,-917 112,-819 20,-819\"/>\n",
       "<text text-anchor=\"start\" x=\"22\" y=\"-865.5\" font-family=\"Linux libertine\" font-size=\"10.00\">1: ttnn.from_torch</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-868 113,-917 216,-917 216,-868 113,-868\"/>\n",
       "<text text-anchor=\"start\" x=\"147.5\" y=\"-907\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"151.5\" y=\"-896\" font-family=\"Linux libertine\" font-size=\"10.00\">(6, 3)</text>\n",
       "<text text-anchor=\"start\" x=\"133.5\" y=\"-885\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-819 113,-867 216,-867 216,-819 113,-819\"/>\n",
       "<text text-anchor=\"start\" x=\"143\" y=\"-857\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-846\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-835\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"134.5\" y=\"-824\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- torch_input_0&#45;&gt;ttnn.from_torch_5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>torch_input_0:#0&#45;&gt;ttnn.from_torch_5:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165,-965C165,-948.03 165,-941.4 165,-928.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.5,-928 165,-918 161.5,-928 168.5,-928\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.5\" y=\"-939\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- ttnn.to_device_14 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>ttnn.to_device_14</title>\n",
       "<g id=\"a_node5\"><a xlink:href=\"/operation_buffer_report/3\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"242,-771 0,-771 0,-661 242,-661 242,-771\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1,-662 1,-770 87,-770 87,-662 1,-662\"/>\n",
       "<text text-anchor=\"start\" x=\"3\" y=\"-713.5\" font-family=\"Linux libertine\" font-size=\"10.00\">3: ttnn.to_device</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"88,-722 88,-770 241,-770 241,-722 88,-722\"/>\n",
       "<text text-anchor=\"start\" x=\"147.5\" y=\"-760\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-749\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-738\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"134.5\" y=\"-727\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"88,-662 88,-721 241,-721 241,-662 88,-662\"/>\n",
       "<text text-anchor=\"start\" x=\"143\" y=\"-711\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-700\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"115\" y=\"-689\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"134.5\" y=\"-678\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"90\" y=\"-667\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_5&#45;&gt;ttnn.to_device_14 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>ttnn.from_torch_5:#0&#45;&gt;ttnn.to_device_14:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165,-818C165,-801.03 165,-794.4 165,-781.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.5,-781 165,-771 161.5,-781 168.5,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"180.5\" y=\"-792\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- torch_input_6 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>torch_input_6</title>\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"498,-1032 430,-1032 430,-965 498,-965 498,-1032\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"431,-1015.5 431,-1030.5 497,-1030.5 497,-1015.5 431,-1015.5\"/>\n",
       "<text text-anchor=\"start\" x=\"434\" y=\"-1020.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.Tensor</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"431,-965.5 431,-1014.5 497,-1014.5 497,-965.5 431,-965.5\"/>\n",
       "<text text-anchor=\"start\" x=\"442.5\" y=\"-1004.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"451\" y=\"-993.5\" font-family=\"Linux libertine\" font-size=\"10.00\">(3, 6)</text>\n",
       "<text text-anchor=\"start\" x=\"433\" y=\"-982.5\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_11 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>ttnn.from_torch_11</title>\n",
       "<g id=\"a_node4\"><a xlink:href=\"/operation_buffer_report/2\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"516,-918 318,-918 318,-818 516,-818 516,-918\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"319,-819 319,-917 411,-917 411,-819 319,-819\"/>\n",
       "<text text-anchor=\"start\" x=\"321\" y=\"-865.5\" font-family=\"Linux libertine\" font-size=\"10.00\">2: ttnn.from_torch</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"412,-868 412,-917 515,-917 515,-868 412,-868\"/>\n",
       "<text text-anchor=\"start\" x=\"446.5\" y=\"-907\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"450.5\" y=\"-896\" font-family=\"Linux libertine\" font-size=\"10.00\">(3, 6)</text>\n",
       "<text text-anchor=\"start\" x=\"432.5\" y=\"-885\" font-family=\"Linux libertine\" font-size=\"10.00\">torch.float32</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"412,-819 412,-867 515,-867 515,-819 412,-819\"/>\n",
       "<text text-anchor=\"start\" x=\"442\" y=\"-857\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-846\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([3, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"414\" y=\"-835\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"433.5\" y=\"-824\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- torch_input_6&#45;&gt;ttnn.from_torch_11 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>torch_input_6:#0&#45;&gt;ttnn.from_torch_11:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464,-965C464,-948.03 464,-941.4 464,-928.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.5,-928 464,-918 460.5,-928 467.5,-928\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.5\" y=\"-939\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- ttnn.to_device_17 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>ttnn.to_device_17</title>\n",
       "<g id=\"a_node6\"><a xlink:href=\"/operation_buffer_report/4\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"541,-771 299,-771 299,-661 541,-661 541,-771\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"300,-662 300,-770 386,-770 386,-662 300,-662\"/>\n",
       "<text text-anchor=\"start\" x=\"302\" y=\"-713.5\" font-family=\"Linux libertine\" font-size=\"10.00\">4: ttnn.to_device</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"387,-722 387,-770 540,-770 540,-722 387,-722\"/>\n",
       "<text text-anchor=\"start\" x=\"446.5\" y=\"-760\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-749\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([3, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"414\" y=\"-738\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"433.5\" y=\"-727\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"387,-662 387,-721 540,-721 540,-662 387,-662\"/>\n",
       "<text text-anchor=\"start\" x=\"442\" y=\"-711\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-700\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([3, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"414\" y=\"-689\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"433.5\" y=\"-678\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"389\" y=\"-667\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- ttnn.from_torch_11&#45;&gt;ttnn.to_device_17 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>ttnn.from_torch_11:#0&#45;&gt;ttnn.to_device_17:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464,-818C464,-801.03 464,-794.4 464,-781.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.5,-781 464,-771 460.5,-781 467.5,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.5\" y=\"-792\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- ttnn.matmul_20 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>ttnn.matmul_20</title>\n",
       "<g id=\"a_node7\"><a xlink:href=\"/operation_buffer_report/5\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"540.5,-614 153.5,-614 153.5,-493 540.5,-493 540.5,-614\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"155,-493.5 155,-612.5 232,-612.5 232,-493.5 155,-493.5\"/>\n",
       "<text text-anchor=\"start\" x=\"157\" y=\"-550.5\" font-family=\"Linux libertine\" font-size=\"10.00\">5: ttnn.matmul</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"233,-553.5 233,-612.5 386,-612.5 386,-553.5 233,-553.5\"/>\n",
       "<text text-anchor=\"start\" x=\"292.5\" y=\"-602.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"277.5\" y=\"-591.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"260\" y=\"-580.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"279.5\" y=\"-569.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"235\" y=\"-558.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"387,-553.5 387,-612.5 540,-612.5 540,-553.5 387,-553.5\"/>\n",
       "<text text-anchor=\"start\" x=\"446.5\" y=\"-602.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 1</text>\n",
       "<text text-anchor=\"start\" x=\"431.5\" y=\"-591.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([3, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"414\" y=\"-580.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"433.5\" y=\"-569.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"389\" y=\"-558.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"233,-493.5 233,-552.5 540,-552.5 540,-493.5 233,-493.5\"/>\n",
       "<text text-anchor=\"start\" x=\"365\" y=\"-542.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"354.5\" y=\"-531.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"337\" y=\"-520.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"356.5\" y=\"-509.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-498.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- ttnn.to_device_14&#45;&gt;ttnn.matmul_20 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>ttnn.to_device_14:#0&#45;&gt;ttnn.matmul_20:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M242,-691C261.92,-691 298.28,-649.09 307.05,-623.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"310.5,-623.99 309,-613.5 303.63,-622.63 310.5,-623.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"319.5\" y=\"-635\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- ttnn.matmul_26 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>ttnn.matmul_26</title>\n",
       "<g id=\"a_node9\"><a xlink:href=\"/operation_buffer_report/7\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"390.5,-278 3.5,-278 3.5,-157 390.5,-157 390.5,-278\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"5,-157.5 5,-276.5 82,-276.5 82,-157.5 5,-157.5\"/>\n",
       "<text text-anchor=\"start\" x=\"7\" y=\"-214.5\" font-family=\"Linux libertine\" font-size=\"10.00\">7: ttnn.matmul</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83,-217.5 83,-276.5 236,-276.5 236,-217.5 83,-217.5\"/>\n",
       "<text text-anchor=\"start\" x=\"142.5\" y=\"-266.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"127.5\" y=\"-255.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"110\" y=\"-244.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"129.5\" y=\"-233.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"85\" y=\"-222.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"237,-217.5 237,-276.5 390,-276.5 390,-217.5 237,-217.5\"/>\n",
       "<text text-anchor=\"start\" x=\"296.5\" y=\"-266.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 1</text>\n",
       "<text text-anchor=\"start\" x=\"281.5\" y=\"-255.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"264\" y=\"-244.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"283.5\" y=\"-233.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"239\" y=\"-222.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83,-157.5 83,-216.5 390,-216.5 390,-157.5 83,-157.5\"/>\n",
       "<text text-anchor=\"start\" x=\"215\" y=\"-206.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"204.5\" y=\"-195.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"187\" y=\"-184.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"206.5\" y=\"-173.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"162\" y=\"-162.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- ttnn.to_device_14&#45;&gt;ttnn.matmul_26 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>ttnn.to_device_14:#0&#45;&gt;ttnn.matmul_26:$1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165,-661C165,-638.12 148.77,-636.38 144,-614 114.48,-475.58 62.65,-397.78 161,-296 182.1,-274.16 286.08,-303.43 309.37,-286.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"312.52,-287.99 314,-277.5 306.3,-284.78 312.52,-287.99\"/>\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-467\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 1</text>\n",
       "</g>\n",
       "<!-- ttnn.to_device_17&#45;&gt;ttnn.matmul_20 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>ttnn.to_device_17:#0&#45;&gt;ttnn.matmul_20:$1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M464,-661C464,-643.68 464,-637.02 464,-623.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"467.5,-623.5 464,-613.5 460.5,-623.5 467.5,-623.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"479.5\" y=\"-635\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 1</text>\n",
       "</g>\n",
       "<!-- ttnn.softmax_23 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>ttnn.softmax_23</title>\n",
       "<g id=\"a_node8\"><a xlink:href=\"/operation_buffer_report/6\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"425.5,-446 190.5,-446 190.5,-325 425.5,-325 425.5,-446\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"192,-325.5 192,-444.5 271,-444.5 271,-325.5 192,-325.5\"/>\n",
       "<text text-anchor=\"start\" x=\"194\" y=\"-382.5\" font-family=\"Linux libertine\" font-size=\"10.00\">6: ttnn.softmax</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"272,-385.5 272,-444.5 425,-444.5 425,-385.5 272,-385.5\"/>\n",
       "<text text-anchor=\"start\" x=\"331.5\" y=\"-434.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"316.5\" y=\"-423.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"299\" y=\"-412.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"318.5\" y=\"-401.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"274\" y=\"-390.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"272,-325.5 272,-384.5 425,-384.5 425,-325.5 272,-325.5\"/>\n",
       "<text text-anchor=\"start\" x=\"327\" y=\"-374.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"316.5\" y=\"-363.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 6])</text>\n",
       "<text text-anchor=\"start\" x=\"299\" y=\"-352.5\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"318.5\" y=\"-341.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"274\" y=\"-330.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- ttnn.matmul_20&#45;&gt;ttnn.softmax_23 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>ttnn.matmul_20:#0&#45;&gt;ttnn.softmax_23:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M387,-493C387,-469.98 359.44,-471.33 351.26,-455.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.61,-454.49 349,-445.5 347.79,-456.02 354.61,-454.49\"/>\n",
       "<text text-anchor=\"middle\" x=\"392.5\" y=\"-467\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- ttnn.softmax_23&#45;&gt;ttnn.matmul_26 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>ttnn.softmax_23:#0&#45;&gt;ttnn.matmul_26:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M349,-325C349,-241.7 174.98,-354.06 160.02,-287.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.49,-287.1 159,-277.5 156.53,-287.8 163.49,-287.1\"/>\n",
       "<text text-anchor=\"middle\" x=\"359.5\" y=\"-299\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "<!-- ttnn.from_device_29 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>ttnn.from_device_29</title>\n",
       "<g id=\"a_node10\"><a xlink:href=\"/operation_buffer_report/8\" xlink:title=\"&lt;TABLE&gt;\">\n",
       "<polygon fill=\"#dcdcdc\" stroke=\"black\" points=\"314,-110 58,-110 58,0 314,0 314,-110\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"59,-1 59,-109 159,-109 159,-1 59,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"61\" y=\"-52.5\" font-family=\"Linux libertine\" font-size=\"10.00\">8: ttnn.from_device</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"160,-50 160,-109 313,-109 313,-50 160,-50\"/>\n",
       "<text text-anchor=\"start\" x=\"219.5\" y=\"-99\" font-family=\"Linux libertine\" font-size=\"10.00\">Input 0</text>\n",
       "<text text-anchor=\"start\" x=\"204.5\" y=\"-88\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"187\" y=\"-77\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"206.5\" y=\"-66\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "<text text-anchor=\"start\" x=\"162\" y=\"-55\" font-family=\"Linux libertine\" font-size=\"10.00\">Memory: DRAM, INTERLEAVED</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"160,-1 160,-49 313,-49 313,-1 160,-1\"/>\n",
       "<text text-anchor=\"start\" x=\"215\" y=\"-39\" font-family=\"Linux libertine\" font-size=\"10.00\">Output 0</text>\n",
       "<text text-anchor=\"start\" x=\"204.5\" y=\"-28\" font-family=\"Linux libertine\" font-size=\"10.00\">Shape([6, 3])</text>\n",
       "<text text-anchor=\"start\" x=\"187\" y=\"-17\" font-family=\"Linux libertine\" font-size=\"10.00\">DataType.BFLOAT16</text>\n",
       "<text text-anchor=\"start\" x=\"206.5\" y=\"-6\" font-family=\"Linux libertine\" font-size=\"10.00\">Layout: TILE</text>\n",
       "</a>\n",
       "</g>\n",
       "</g>\n",
       "<!-- ttnn.matmul_26&#45;&gt;ttnn.from_device_29 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>ttnn.matmul_26:#0&#45;&gt;ttnn.from_device_29:$0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237,-157C237,-140.03 237,-133.4 237,-120.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.5,-120 237,-110 233.5,-120 240.5,-120\"/>\n",
       "<text text-anchor=\"middle\" x=\"252.5\" y=\"-131\" font-family=\"Times,serif\" font-size=\"10.00\">0 &#45;&gt; 0</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7907bf57a200>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize(context_vecs_ttnn_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the slight difference between the context vectors on `ttnn` compared to `torch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[ 0.44141,  0.58984,  0.57812],\n",
       "             [ 0.43359,  0.64844,  0.56641],\n",
       "             ...,\n",
       "             [ 0.46094,  0.58594,  0.52344],\n",
       "             [ 0.41602,  0.64844,  0.55859]], shape=Shape([6, 3]), dtype=DataType::BFLOAT16, layout=Layout::TILE)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs_ttnn_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
