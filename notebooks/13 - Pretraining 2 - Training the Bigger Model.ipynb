{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8665dd0",
   "metadata": {},
   "source": [
    "# ðŸ“ NOTE\n",
    "\n",
    "Unless you have chosen to train with the `fineweb-100m` dataset, training on a CPU for a GPT-2 355M model over 1 billion tokens is not a good use of time and resources. \n",
    "\n",
    "For this notebook, we'll leverage **GPU training**. It's really the only way to do training within a reasonable amount of time.\n",
    "\n",
    "If you don't have access to a GPU, there are a few good options out there such as:\n",
    "\n",
    "* Lambda Cloud - https://lambda.ai/service/gpu-cloud\n",
    "* Fly IO - https://fly.io/\n",
    "* Google Cloud - https://cloud.google.com/gpu\n",
    "* Akamai Cloud - https://www.linode.com/products/gpu/\n",
    "\n",
    "For this notebook, I used my personal NVIDIA RTX 6000 ADA GPU to train. \n",
    "\n",
    "The amount of VRAM necessary is dependent upon the batch size. If you have an older card, most likely you'll be using anywhere between 1, 2, or 4 for the batch size. Newer cards can leverage `bfloat16` data type and that significantly reduces the memory necessary to train. You can look to have anywhere from a batch size of 8 to 32.\n",
    "\n",
    "My card supports `bfloat16`, and so do most cloud GPUs now. So the code will be written with that assumption. Adjust the `batch_size` variable as needed when appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085ec19",
   "metadata": {},
   "source": [
    "# Cost of Cloud Training\n",
    "\n",
    "Training on the cloud can be costly, but you can reduce the costs by:\n",
    "\n",
    "- Picking instances with only a single GPU (this notebook only requires one)\n",
    "- Using an older GPU (Turing, Ampere, etc.)\n",
    "- Using a smaller cloud provider\n",
    "- Doing as much pre-processing locally (dataset generation) as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf40f64",
   "metadata": {},
   "source": [
    "# Pretraining 2 - GPT-2 355M \n",
    "\n",
    "**Note** - I've included the training script `model_train.py` at the same level of this notebook. It is the script I had actually used to produce the model. \n",
    "\n",
    "So now we're going to train the 355M parameter model. Our goal here is to produce a high quality, foundational model in which we will be able to use for fine-tuning on various other applications and then do inference on CPU, GPU and our Tenstorrent Wormhole. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dcd847",
   "metadata": {},
   "source": [
    "# Defining Variables\n",
    "\n",
    "Define the `dataset_name` and `device` we'll be using. `cuda:1` is used because I have 2 CUDA cards on my server, where `cuda:1` is the one I want to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842fce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'fineweb-100m'\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bc2e61",
   "metadata": {},
   "source": [
    "# Ensuring a Clean Environment\n",
    "\n",
    "Let's make sure that our GPU has enough memory to do our training. Let's force garbage collection and empty cuda cache to free up VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6b3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if device.startswith(\"cuda\"):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424b96a",
   "metadata": {},
   "source": [
    "## Loading the Input and Validation Tokens\n",
    "\n",
    "We created the dataloaders in the previous notebook. Let's load them back up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8b9088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train_loader.\n",
      "Loaded val_loader\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1786, 316)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.preload_dataloaders import load_pickled_dataloader\n",
    "\n",
    "train_loader = load_pickled_dataloader(f\"data/{dataset_name}/train_loader.dl\")\n",
    "print(\"Loaded train_loader.\")\n",
    "\n",
    "val_loader = load_pickled_dataloader(f\"data/{dataset_name}/val_loader.dl\")\n",
    "print(\"Loaded val_loader\")\n",
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06090699",
   "metadata": {},
   "source": [
    "Validate for the maximum token ID. This makes sure that we don't have any token IDs out of range. Being out of range means that our dataloader could be corrupted. I ended up having to do this because I had bad data in the previous notebook, causing re-do of the loader construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fad6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum token ID: 50255\n"
     ]
    }
   ],
   "source": [
    "# To check token ID range in your dataset\n",
    "max_token = float('-inf')\n",
    "for i, (input_batch, _) in enumerate(train_loader):\n",
    "    max_token = max(max_token, input_batch.max().item())\n",
    "\n",
    "print(f\"Maximum token ID: {max_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d217b6",
   "metadata": {},
   "source": [
    "# Load Up Test Model\n",
    "\n",
    "Let's load up a test model using the GPT2 355M configuration and see what we have! It's also a good way to test moving the model to device to see if we're set up for success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff79635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpt2_model import GPTModel\n",
    "from scripts.gpt2_common import GPT_CONFIG_355M\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_355M)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf4babf",
   "metadata": {},
   "source": [
    "## Existing Training and Validation Loss\n",
    "\n",
    "If you're somehow reloading a model, it's useful to check out the current training and validaton loss. Warning, this will perform a forward pass on all your data, so it is still an expensive effort "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0d8bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.99490008354187\n",
      "Validation loss: 10.998496532440186\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from scripts.train import calc_loss_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# use the num_batches parameter to limit the number of batches processed\n",
    "# 20/2 should be enough to tell us something.\n",
    "with torch.no_grad():\n",
    "  train_loss = calc_loss_loader(train_loader, model, device=device, num_batches=20)\n",
    "  val_loss = calc_loss_loader(val_loader, model, device=device, num_batches=2)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "gc.collect()\n",
    "if device.startswith(\"cuda\"):\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5326261",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now it is time to train our 355M model. Running this script n the notebook is actually not a good thing to do. It will take took long. I have included 2 scripts you can run within a `tmux` session so that you can disconnect from your session while still having your training continue. \n",
    "\n",
    "* `model_train.py`\n",
    "* `model_inference.py`.\n",
    "\n",
    "In `train_model_simple`, I have modified the code to take in a `device` argument now to accommodate a GPU. This cause a ripple effect of adding the arguments elswhere. \n",
    "\n",
    "Additionally, I included a new `max_iters` argument to only train the model up to a specific number of steps as an entire epoch would take too long if just needing to test. We will be using the `max_iters` argument to make sure that this cell finishes within a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06facdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modern CUDA device found. Using tensor cores.\n",
      "Ep 1 (Step 000000 of 1786): Train loss 9.850, Val loss 9.863\n",
      "Every effort moves you theWide the,,.  the the the..,Built.. ,, appalling blame to ,, blame the appalling. FountainRe,, and specializationAfee,. . the,Bah,. ady is Cultural the\n",
      "Ep 1 (Step 000100 of 1786): Train loss 6.884, Val loss 6.862\n",
      "Every effort moves you the team of the work of my way you can be in the top of the ability of our, which's other can be very. If a little, and the end of our own from a bit of the best. In the same year, and\n",
      "Max iterations exceeded at 100 steps. Max: 100 steps\n",
      "Took this long to train: 77947.70359992981 ms\n",
      "Train losses\n",
      "\n",
      "[9.85, 6.884375]\n",
      "Val losses\n",
      "\n",
      "[9.8625, 6.8625]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from scripts.perf_timer import PerfTimer\n",
    "from scripts.train import train_model_simple\n",
    "from scripts.gpt2_model import GPTModel\n",
    "from scripts.gpt2_common import GPT_CONFIG_355M\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Configure the device\n",
    "if device.startswith(\"cuda\"):\n",
    "  capability = torch.cuda.get_device_capability()\n",
    "  if capability[0] >= 7:\n",
    "    print(\"Modern CUDA device found. Using tensor cores.\")\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "  else:\n",
    "    print(\"Tensor cores not supported on this CPU. Will still proceed.\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_355M)\n",
    "\n",
    "# Move model to device BEFORE compiling\n",
    "if device.startswith(\"cuda\"):\n",
    "  torch.cuda.empty_cache()\n",
    "  model = model.to(device).to(torch.bfloat16)\n",
    "else:\n",
    "  model = model.to(device)\n",
    "model = torch.compile(model)\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "  model.parameters(),\n",
    "  lr=2e-4,\n",
    "  weight_decay=0.1,\n",
    "  fused=True\n",
    ")\n",
    "\n",
    "# We have lots of data, so we can just train for a single epoch.\n",
    "num_epochs = 1\n",
    "\n",
    "timer = PerfTimer()\n",
    "\n",
    "timer.start()\n",
    "train_losses, val_losses = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=100,\n",
    "    eval_iter=10, # eval less frequently\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    max_iter=100\n",
    ")\n",
    "timer.stop()\n",
    "\n",
    "print(f\"Took this long to train: {timer.elapsed_ms()} ms\")\n",
    "print(\"Train losses\\n\")\n",
    "print(train_losses)\n",
    "print(\"Val losses\\n\")\n",
    "print(val_losses)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "if device.startswith(\"cuda\"):\n",
    "  torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259b160",
   "metadata": {},
   "source": [
    "## Save the model \n",
    "\n",
    "Oh, we should save our precious efforts! Let's not make all that waiting all for nothing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5d64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-355M-100M-test-model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1afadf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"models/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a2841",
   "metadata": {},
   "source": [
    "## Reload the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0439fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19103bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPTModel(\n",
       "    (tok_emb): Embedding(50257, 1024)\n",
       "    (pos_emb): Embedding(1024, 1024)\n",
       "    (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "    (trf_blocks): Sequential(\n",
       "      (0): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (12): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (13): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (14): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (15): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (16): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (17): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (18): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (19): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (20): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (21): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (22): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (23): TransformerBlock(\n",
       "        (att): MultiHeadAttention(\n",
       "          (W_query): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (W_value): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (layer): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU()\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (norm1): LayerNorm()\n",
       "        (norm2): LayerNorm()\n",
       "        (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm()\n",
       "    (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.model_loader import load_model_from_path\n",
    "\n",
    "model = load_model_from_path(\n",
    "    f'models/{model_name}',\n",
    "    device\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da881d7",
   "metadata": {},
   "source": [
    "## Testing by inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d45f4ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens in 194.2429542541504 ms\n",
      "Output text:\n",
      " Every effort moves you were my good for a new to be a great to a little, and a little and your not a very the top and a lot, I got a more to get that the most. I would take the best that it's a great, and\n"
     ]
    }
   ],
   "source": [
    "from scripts.perf_timer import PerfTimer\n",
    "from scripts.generate import generate\n",
    "from scripts.text_helpers import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "perf_timer = PerfTimer()\n",
    "\n",
    "# Create input tokens directly on the correct device to avoid compilation issues\n",
    "input_ids = text_to_token_ids(\"Every effort moves you\", tokenizer).to(device)\n",
    "\n",
    "perf_timer.start()\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=input_ids,\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_355M[\"context_length\"],\n",
    "    temperature=0.8,\n",
    "    top_k=25,\n",
    "    device=device\n",
    ")\n",
    "perf_timer.stop()\n",
    "\n",
    "print(\"Generated tokens in\", perf_timer.elapsed_ms(), \"ms\")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855735f",
   "metadata": {},
   "source": [
    "## Analyzing the Results\n",
    "\n",
    "- Talk about starting training and validation loss\n",
    "- Talk about the end\n",
    "- Chart the data after grabbing the entire arrays (I printed them out)\n",
    "- Mention that 3B dataset for the 355M model for 1 epoch at the learning rate I was going is probably too little... But good to test our hypothesis.\n",
    "- I didn't save the optimizer, so now I am in a situation where a second epoch has to start with an optimizer without any momentum\n",
    "- Still worth doing a second epoch in that state - I will just increase the learning rate by 3x to get to where i need to go faster -- plus, my model is somewhat stable at this point. (momentum means additional delta determined from past gradients off from the direction we're going)\n",
    "\n",
    "## Next steps to get a quality model\n",
    "- Epoch 2 - on 3B dataset, increased learning rate to account for lack of optimizer saving\n",
    "- Build a 10B dataset and train a version of the 355M model. (Chinchilla scaling law -> 7B tokens is optimal in general)\n",
    "\n",
    "Lastly! Save the optimizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700819e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
