{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Weighted Attention Mechanism V2\n",
    "\n",
    "We continue on with implementing the Simple Weighted Attention Mechanism. We'll call this a \"V2\". It uses `torch` linear layers, the beginnings in demonstrating a neural network.\n",
    "\n",
    "To keep things interesting, we'll dive deeper into understanding the existence of the Tenstorrent hardware. For example, what is the point of having a TPU when we have GPUs? How much faster is a Wormhole compared to just using a very powerful CPU? In this case, since I have a Wormhole n150d, the context of the discussions will be around that.\n",
    "\n",
    "It's really hard to appreciate what the Tenstorrent hardware can do if you're only used to experiencing performance in one perspective. For example, if you are used to doing work with GPU acceleration, you may be going \"what's the point?\" after seeing some of the acceleration that the hardware provides. \n",
    "\n",
    "Anyway we'll look closer to all of this and more in this project. \n",
    "\n",
    "Like with all our other implementations, we will start with the `torch` version. Again, like all other notebooks, the code is inspired by Sebastian Raschka's LLM From Scratch code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Like before we will first start by importing the necessary libraries.\n",
    "\n",
    "We also set the manual seed for `torch` to be `789`. Sebastian chooses this as the seed in his projects, so I will just use that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x733eaa138050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention_v2 Class\n",
    "\n",
    "Next, we will implement an improved version of the self attention mechansim from the previous notebook. This time, we will generalize the implementation to a Python class called `SimpleAttention_v2`. \n",
    "\n",
    "Usage is simple, it is a `torch` module that simply does a forward pass of given inputs. For example, it is used like this:\n",
    "\n",
    "```python\n",
    "# x is a tensor to be treated as input\n",
    "result = SelfAttention_v2(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSelfAttention_v2\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_in, d_out):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    attn_scores = queries @ keys.T\n",
    "    attn_weights = torch.softmax(\n",
    "      attn_scores / keys.shape[-1] ** 0.5,\n",
    "      dim=-1\n",
    "    )\n",
    "    context_vec = attn_weights @ values\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this class in CPU by initializing the context we have been using, creating the `SelfAttention_v2` instance, and then evaluating by performing a \"forward pass\" as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "d = context[0].shape\n",
    "d_in = d[0]\n",
    "d_out = d[0] - 1\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "context_vec = sa_v2(context)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it work with Tenstorrent\n",
    "\n",
    "the tricky part here is to figure out _what_ to port to using the Tenstorrent hardware. \n",
    "\n",
    "With our current skillset (at least mine), I don't know how to do _everything_ from scratch in `ttnn` land, but we can at least accelerate some of the compute by offloading some tensor calculations within the `SelfAttention_v2` class. \n",
    "\n",
    "This means that the forward pass method is a great candidate.\n",
    "\n",
    "```python\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "context_vec = attn_weights @ values\n",
    "```\n",
    "\n",
    "We can send the keys, queries, values tensors to hardware, and then perform a `ttnn.matmul` on them. The goal is to offload as much computation as possible to hardware.\n",
    "\n",
    "At the same time, we have to minimize the amount of data transfers between CPU and device memory as that can destroy any benefits in computing on the Tenstorrent hardware.\n",
    "\n",
    "In order to understand if it helps or hurts, we're going to create a small benchmark with a lot of random matrices and just forward pass repeatedly to see the difference in CPU vs Tenstorrent hardware for this type of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out with configuring the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 13:13:28.073 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "New chip! We now have 1 chips\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n",
      "\u001b[32m2025-04-27 13:13:28.146\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-04-27 13:13:28.162\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-04-27 13:13:28.164\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-04-27 13:13:28.165\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-04-27 13:13:28.165\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-04-27 13:13:28.165\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-04-27 13:13:28.285\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    }
   ],
   "source": [
    "import ttnn\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that, let's line by line translate the forward pass of the self attention mechanism to `ttnn`.\n",
    "\n",
    "First line:\n",
    "\n",
    "```python\n",
    "attn_scores = queries @ keys.T\n",
    "```\n",
    "\n",
    "This involves:\n",
    "1. Assigning queries to be the compute of input with `W_query`.\n",
    "2. Transpose the keys (compute of input with `W_key`)\n",
    "3. Create the `ttnn` tensors from queries and keys transposed.\n",
    "4. Compute the matmul of queries and keys transposed.\n",
    "\n",
    "\n",
    "The two results betwen computing CPU vs hardware should be almost the same (they get close due to precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[ 0.28906,  0.07178,  ...,  0.13379, -0.04980],\n",
       "              [ 0.46484,  0.17090,  ...,  0.17676,  0.00867],\n",
       "              ...,\n",
       "              [ 0.21777,  0.08691,  ...,  0.07812,  0.01489],\n",
       "              [ 0.34180,  0.12598,  ...,  0.12988,  0.00720]], shape=Shape([6, 6]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
       "         [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
       "         [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
       "         [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
       "         [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
       "         [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "queries = sa_v2.W_query(context)\n",
    "keys = sa_v2.W_key(context)\n",
    "keys_transposed = keys.T\n",
    "\n",
    "\n",
    "queries_ttnn = ttnn.from_torch(\n",
    "  queries,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "keys_transposed_ttnn = ttnn.from_torch(\n",
    "  keys_transposed,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "attn_scores_ttnn = ttnn.matmul(\n",
    "  queries_ttnn,\n",
    "  keys_transposed_ttnn\n",
    ")\n",
    "\n",
    "# For comparison\n",
    "attn_scores_cpu = queries @ keys.T \n",
    " \n",
    "attn_scores_ttnn, attn_scores_cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it gets pretty tricky, we have to deal with the softmax. Notice that we can't do a scalar divide against a tensor in ttnn.\n",
    "\n",
    "```python\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "```\n",
    "\n",
    "Here is what needs to happen\n",
    "\n",
    "1. We will need to scale ahead of time, a torch tensor of the attention scores and the scale value. This means that we have to bring the attn_scores_ttnn back to CPU memory to do that. \n",
    "2. Send back the result to hardware.\n",
    "3. Perform the softmax on the last dimension.\n",
    "\n",
    "Again, we should get pretty close to the CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[ 0.19629,  0.16211,  ...,  0.17188,  0.15039],\n",
       "              [ 0.20898,  0.16504,  ...,  0.16602,  0.14355],\n",
       "              ...,\n",
       "              [ 0.18652,  0.16602,  ...,  0.16309,  0.15625],\n",
       "              [ 0.19922,  0.16602,  ...,  0.16602,  0.14941]], shape=Shape([6, 6]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " TorchTensor([[0.1920, 0.1647, 0.1652, 0.1550, 0.1721, 0.1511],\n",
       "              [0.2040, 0.1658, 0.1663, 0.1496, 0.1665, 0.1478],\n",
       "              [0.2035, 0.1659, 0.1662, 0.1499, 0.1662, 0.1482],\n",
       "              [0.1867, 0.1667, 0.1668, 0.1571, 0.1661, 0.1565],\n",
       "              [0.1830, 0.1668, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "              [0.1936, 0.1662, 0.1665, 0.1542, 0.1667, 0.1529]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_scaled_torch = ttnn.to_torch(attn_scores_ttnn) / keys.shape[-1] ** 0.5\n",
    "attn_scores_scaled_ttnn = ttnn.from_torch(\n",
    "  attn_scores_scaled_torch,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "attn_weights_ttnn = ttnn.softmax(attn_scores_scaled_ttnn, dim=-1)\n",
    "\n",
    "# CPU version\n",
    "attn_weights = torch.softmax(attn_scores_scaled_torch, dtype=torch.float32, dim=-1)\n",
    "\n",
    "attn_weights_ttnn, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the context vector. This involves another matmul on the attention weights and values.\n",
    "\n",
    "```python\n",
    "context_vec = attn_weights @ values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[-0.07373,  0.07080],\n",
       "              [-0.07373,  0.07080],\n",
       "              ...,\n",
       "              [-0.07617,  0.06689],\n",
       "              [-0.07471,  0.06982]], shape=Shape([6, 2]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " TorchTensor([[-0.0739,  0.0712],\n",
       "              [-0.0748,  0.0703],\n",
       "              [-0.0749,  0.0702],\n",
       "              [-0.0760,  0.0684],\n",
       "              [-0.0763,  0.0679],\n",
       "              [-0.0754,  0.0693]], grad_fn=<AliasBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = sa_v2.W_value(context)\n",
    "\n",
    "values_ttnn = ttnn.from_torch(\n",
    "  values,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "context_vec_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "context_vec_cpu = attn_weights @ values\n",
    "\n",
    "context_vec_ttnn, context_vec_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a lot more verbose. Let's put it together in a revised SelfAttentionV2 class. We will just need to include an additional device parameter so we can internally move things around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(789)\n",
    "\n",
    "\n",
    "class SelfAttention_ttnn_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self._device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    queries_ttnn = ttnn.from_torch(\n",
    "      queries,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    keys_transposed_ttnn = ttnn.from_torch(\n",
    "      keys.T,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    values_ttnn = ttnn.from_torch(\n",
    "      values,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn,\n",
    "      keys_transposed_ttnn\n",
    "    )\n",
    "\n",
    "    attn_scores_scaled_torch = ttnn.to_torch(attn_scores_ttnn) / (keys.shape[-1] ** 0.5)\n",
    "    attn_scores_scaled_ttnn = ttnn.from_torch(\n",
    "      attn_scores_scaled_torch,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "    attn_weights_ttnn = ttnn.softmax(attn_scores_scaled_ttnn, dim=-1)\n",
    "\n",
    "    context_vec_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "    context_vec = ttnn.to_torch(context_vec_ttnn)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "TorchTensor([[-0.0737,  0.0708],\n",
      "             [-0.0737,  0.0708],\n",
      "             [-0.0737,  0.0708],\n",
      "             [-0.0757,  0.0684],\n",
      "             [-0.0762,  0.0669],\n",
      "             [-0.0747,  0.0698]], dtype=torch.bfloat16)\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "d = context[0].shape\n",
    "d_in = d[0]\n",
    "d_out = d[0] - 1\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(d_in, d_out, device)\n",
    "context_vec = sa_ttnn_v2(context)\n",
    "\n",
    "print(context_vec)\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Time: 0.0992 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.process_time_ns()\n",
    "\n",
    "print(\"hello\")\n",
    "\n",
    "end = time.process_time_ns()\n",
    "\n",
    "print(f\"Time: {(end - start) / 1000000:.4f} milliseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class PerfTimer:\n",
    "  def __init__(self):\n",
    "    self.start_time = 0\n",
    "    self.end_time = 0\n",
    "    \n",
    "  def start(self):\n",
    "    self.start_time = time.time()\n",
    "    \n",
    "  def stop(self):\n",
    "    self.end_time = time.time()\n",
    "\n",
    "  def reset(self):\n",
    "    self.start_time = 0\n",
    "    self.end_time = 0\n",
    "    \n",
    "  def elapsed_ms(self):\n",
    "    return (self.end_time - self.start_time) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1024, 2048]), 81634.54627990723)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "t = PerfTimer()\n",
    "\n",
    "t.start()\n",
    "torch_tensors = torch.stack([torch.randn(1024, 2048) for _ in range(0, 10000)])\n",
    "t.stop()\n",
    "\n",
    "torch_tensors.shape, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0203, -0.0032,  0.0253,  ..., -0.0179, -0.0345,  0.0475],\n",
       "         [-0.0090,  0.0062,  0.0335,  ..., -0.0040, -0.0133,  0.0238],\n",
       "         [-0.0138,  0.0054,  0.0356,  ..., -0.0162, -0.0368,  0.0337],\n",
       "         ...,\n",
       "         [-0.0289, -0.0172,  0.0434,  ..., -0.0038, -0.0330,  0.0301],\n",
       "         [-0.0309, -0.0027,  0.0365,  ..., -0.0138, -0.0279,  0.0271],\n",
       "         [-0.0204,  0.0005,  0.0384,  ..., -0.0225, -0.0309,  0.0429]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " 116655.43222427368)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "t.start()\n",
    "sa_v2 = SelfAttention_v2(2048, 2048)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_v2(tensor)\n",
    "t.stop()\n",
    "\n",
    "result, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[    -0.0258,      0.0009,     -0.0001,  ...,     -0.0284,\n",
       "                    0.0184,     -0.0339],\n",
       "              [    -0.0352,      0.0064,      0.0037,  ...,     -0.0195,\n",
       "                    0.0128,     -0.0422],\n",
       "              [    -0.0270,      0.0104,     -0.0033,  ...,     -0.0183,\n",
       "                    0.0161,     -0.0437],\n",
       "              ...,\n",
       "              [    -0.0289,      0.0104,      0.0033,  ...,     -0.0242,\n",
       "                    0.0034,     -0.0549],\n",
       "              [    -0.0153,     -0.0067,     -0.0028,  ...,     -0.0292,\n",
       "                    0.0149,     -0.0310],\n",
       "              [    -0.0332,      0.0133,      0.0093,  ...,     -0.0297,\n",
       "                    0.0131,     -0.0275]], dtype=torch.bfloat16),\n",
       " 4782.804250717163)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "t.start()\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(2048, 2048, device)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_ttnn_v2(tensor)\n",
    "\n",
    "t.stop()\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "result, t.elapsed_ms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(789)\n",
    "\n",
    "class SelfAttention_ttnn_opt_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out , bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self._device = device\n",
    "    \n",
    "    # Extract weight matrices from PyTorch layers and convert to TTNN once\n",
    "    self.W_query_ttnn = ttnn.from_torch(\n",
    "      self.W_query.weight, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=self._device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    \n",
    "    self.W_key_ttnn = ttnn.from_torch(\n",
    "      self.W_key.weight, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=self._device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "    \n",
    "    self.W_value_ttnn = ttnn.from_torch(\n",
    "      self.W_value.weight, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=self._device,\n",
    "      memory_config=ttnn.L1_MEMORY_CONFIG\n",
    "    )\n",
    "\n",
    "    self._scaler = 1 / (d_out ** 0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_ttnn = ttnn.from_torch(\n",
    "      x, \n",
    "      dtype=ttnn.bfloat16, \n",
    "      layout=ttnn.TILE_LAYOUT, \n",
    "      device=device,\n",
    "    )\n",
    "    queries_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_query_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=32, x=32)\n",
    "    )\n",
    "    values_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_value_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=32, x=32)\n",
    "    )\n",
    "    keys_ttnn = ttnn.linear(\n",
    "      x_ttnn,\n",
    "      self.W_key_ttnn,\n",
    "      transpose_b=True,\n",
    "      core_grid=ttnn.CoreGrid(y=32, x=32)\n",
    "    )\n",
    "\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn, \n",
    "      ttnn.permute(keys_ttnn, (1, 0)),\n",
    "      core_grid=ttnn.CoreGrid(y=32, x=32)\n",
    "    )\n",
    "\n",
    "    attn_weights_ttnn = ttnn.softmax(\n",
    "      attn_scores_ttnn * self._scaler,\n",
    "      dim=-1\n",
    "    )\n",
    "\n",
    "    context_vec_ttnn = ttnn.matmul(\n",
    "      attn_weights_ttnn,\n",
    "      values_ttnn,\n",
    "      core_grid=ttnn.CoreGrid(y=32, x=32)\n",
    "    )\n",
    "\n",
    "    context_vec = ttnn.to_torch(context_vec_ttnn)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Enabling program cache on device 0\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_transpose_wh_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_transpose_wh_interleaved_start_id, transpose_wh\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[-0.0183, -0.0034,  0.0266,  ..., -0.0162, -0.0320,  0.0466],\n",
       "              [-0.0081,  0.0055,  0.0337,  ..., -0.0035, -0.0113,  0.0249],\n",
       "              [-0.0121,  0.0045,  0.0354,  ..., -0.0140, -0.0332,  0.0342],\n",
       "              ...,\n",
       "              [-0.0271, -0.0160,  0.0430,  ..., -0.0034, -0.0315,  0.0309],\n",
       "              [-0.0278, -0.0022,  0.0369,  ..., -0.0125, -0.0265,  0.0276],\n",
       "              [-0.0188,  0.0009,  0.0381,  ..., -0.0210, -0.0293,  0.0420]],\n",
       "             dtype=torch.bfloat16),\n",
       " 110242.26951599121)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "ttnn.enable_program_cache(device)\n",
    "\n",
    "batch = []\n",
    "t.start()\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_opt_v2(2048, 2048, device)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_ttnn_v2(tensor)\n",
    "  \"\"\"\n",
    "  batch.append(result)\n",
    "\n",
    "  if len(batch) == 1000:\n",
    "    for i in range(0, len(batch)):\n",
    "      result = ttnn.to_torch(batch.pop(0))\n",
    "\n",
    "    batch = []\n",
    "  \"\"\"\n",
    "\n",
    "t.stop()\n",
    "\n",
    "\"\"\"\n",
    "for i in range(0, len(batch)):\n",
    "  result = ttnn.to_torch(batch.pop(0))\n",
    "\"\"\"\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "result, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance (ms)**\n",
    "\n",
    "CPU is a Xeon w5-3535x 20c/40t\n",
    "Tensix is on Wormhole n150d\n",
    "\n",
    "\n",
    "|        | CPU      | Tensix    | Tensix Fast |\n",
    "|--------|----------|-----------|-------------|\n",
    "| 1      | 67.33    | 105.28    | 98.39       |\n",
    "| 100    | 1258.74  | 4676.81   | 1183.98     |\n",
    "| 1000   | 11671.33 | 46124.67  | 10935.41    |\n",
    "| 10000  | 115895.56| 467358.59 | 109907.44   |\n",
    "\n",
    "\n",
    "**Power Consumption (Watts)**\n",
    "\n",
    "|         | CPU       | Tensix     | Creating Tensors |\n",
    "|---------|-----------|------------|------------------|\n",
    "| Baseline| 446       | 425        | 444              |\n",
    "| Active  | 740       | 535        | 537              |\n",
    "|         | 116655.43 | 110242.27  | 81634.55         |\n",
    "\n",
    "Wow the CPU uses a lot more power, but we see a point here.. the value of the Wormhole is not that it is that much faster, but that for the performance, we gain incredible power efficiency. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
