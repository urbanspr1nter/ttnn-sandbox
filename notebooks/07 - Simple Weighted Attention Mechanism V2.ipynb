{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Weighted Attention Mechanism V2\n",
    "\n",
    "We continue on with implementing the Simple Weighted Attention Mechanism. We'll call this a \"V2\". It uses `torch` linear layers, the beginnings in demonstrating a neural network.\n",
    "\n",
    "Like with all our other implementations, we will start with the `torch` version. Again, like all other notebooks, the code is inspired by Sebastian Raschka's LLM From Scratch code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Like before we will first start by importing the necessary libraries.\n",
    "\n",
    "We also set the manual seed for `torch` to be 789.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x788b47f30050>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelfAttention_v2 Class\n",
    "\n",
    "Next, we will implement an improved version of the self attention mechansim from the previous notebook. This time, we will generalize th eimplementation to a Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    attn_scores = queries @ keys.T\n",
    "    attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "    context_vec = attn_weights @ values\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this class in CPU by initializing the context we have been using, creating the `SelfAttention_v2` instance, and then evaluating by performing a \"forward pass\" as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0739,  0.0713],\n",
       "        [-0.0748,  0.0703],\n",
       "        [-0.0749,  0.0702],\n",
       "        [-0.0760,  0.0685],\n",
       "        [-0.0763,  0.0679],\n",
       "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "d = context[0].shape\n",
    "d_in = d[0]\n",
    "d_out = d[0] - 1\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "context_vec = sa_v2(context)\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making it work with Tenstorrent\n",
    "\n",
    "the tricky part here is to figure out _what_ to port to using the Tenstorrent hardware. \n",
    "\n",
    "With our current skillset (at least mine), I don't know how to do _everything_ from scratch in `ttnn` land, but we can at least accelerate some of the compute by offloading some tensor calculations within the `SelfAttention_v2` class. \n",
    "\n",
    "This means that the forward pass method is a great candidate.\n",
    "\n",
    "```python\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "context_vec = attn_weights @ values\n",
    "```\n",
    "\n",
    "We can send the keys, queries, values tensors to hardware, and then perform a `ttnn.matmul` on them. The goal is to offload as much computation as possible to hardware.\n",
    "\n",
    "At the same time, we have to minimize the amount of data transfers between CPU and device memory as that can destroy any benefits in computing on the Tenstorrent hardware.\n",
    "\n",
    "In order to understand if it helps or hurts, we're going to create a small benchmark with a lot of random matrices and just forward pass repeatedly to see the difference in CPU vs Tenstorrent hardware for this type of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out with configuring the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 21:07:57.210 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New chip! We now have 1 chips\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-26 21:07:57.274\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-04-26 21:07:57.284\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-04-26 21:07:57.286\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-04-26 21:07:57.287\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-04-26 21:07:57.287\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-04-26 21:07:57.287\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-04-26 21:07:57.312\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n"
     ]
    }
   ],
   "source": [
    "import ttnn\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have that, let's line by line translate the forward pass of the self attention mechanism to `ttnn`.\n",
    "\n",
    "First line:\n",
    "\n",
    "```python\n",
    "attn_scores = queries @ keys.T\n",
    "```\n",
    "\n",
    "This involves:\n",
    "1. Assigning queries to be the compute of input with `W_query`.\n",
    "2. Transpose the keys (compute of input with `W_key`)\n",
    "3. Create the `ttnn` tensors from queries and keys transposed.\n",
    "4. Compute the matmul of queries and keys transposed.\n",
    "\n",
    "\n",
    "The two results betwen computing CPU vs hardware should be almost the same (they get close due to precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[ 0.28906,  0.07178,  ...,  0.13379, -0.04980],\n",
       "              [ 0.46484,  0.17090,  ...,  0.17676,  0.00867],\n",
       "              ...,\n",
       "              [ 0.21777,  0.08691,  ...,  0.07812,  0.01489],\n",
       "              [ 0.34180,  0.12598,  ...,  0.12988,  0.00720]], shape=Shape([6, 6]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
       "         [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
       "         [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
       "         [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
       "         [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
       "         [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "queries = sa_v2.W_query(context)\n",
    "keys = sa_v2.W_key(context)\n",
    "keys_transposed = keys.T\n",
    "\n",
    "\n",
    "queries_ttnn = ttnn.from_torch(\n",
    "  queries,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "keys_transposed_ttnn = ttnn.from_torch(\n",
    "  keys_transposed,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "attn_scores_ttnn = ttnn.matmul(\n",
    "  queries_ttnn,\n",
    "  keys_transposed_ttnn\n",
    ")\n",
    "\n",
    "# For comparison\n",
    "attn_scores_cpu = queries @ keys.T \n",
    " \n",
    "attn_scores_ttnn, attn_scores_cpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it gets pretty tricky, we have to deal with the softmax. Notice that we can't do a scalar divide against a tensor in ttnn.\n",
    "\n",
    "```python\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "```\n",
    "\n",
    "Here is what needs to happen\n",
    "\n",
    "1. We will need to scale ahead of time, a torch tensor of the attention scores and the scale value. This means that we have to bring the attn_scores_ttnn back to CPU memory to do that. \n",
    "2. Send back the result to hardware.\n",
    "3. Perform the softmax on the last dimension.\n",
    "\n",
    "Again, we should get pretty close to the CPU version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[ 0.19629,  0.16211,  ...,  0.17188,  0.15039],\n",
       "              [ 0.20898,  0.16504,  ...,  0.16602,  0.14355],\n",
       "              ...,\n",
       "              [ 0.18652,  0.16602,  ...,  0.16309,  0.15625],\n",
       "              [ 0.19922,  0.16602,  ...,  0.16602,  0.14941]], shape=Shape([6, 6]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " TorchTensor([[0.1920, 0.1647, 0.1652, 0.1550, 0.1721, 0.1511],\n",
       "              [0.2040, 0.1658, 0.1663, 0.1496, 0.1665, 0.1478],\n",
       "              [0.2035, 0.1659, 0.1662, 0.1499, 0.1662, 0.1482],\n",
       "              [0.1867, 0.1667, 0.1668, 0.1571, 0.1661, 0.1565],\n",
       "              [0.1830, 0.1668, 0.1670, 0.1588, 0.1658, 0.1585],\n",
       "              [0.1936, 0.1662, 0.1665, 0.1542, 0.1667, 0.1529]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_scaled_torch = ttnn.to_torch(attn_scores_ttnn) / keys.shape[-1] ** 0.5\n",
    "attn_scores_scaled_ttnn = ttnn.from_torch(\n",
    "  attn_scores_scaled_torch,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "attn_weights_ttnn = ttnn.softmax(attn_scores_scaled_ttnn, dim=-1)\n",
    "\n",
    "# CPU version\n",
    "attn_weights = torch.softmax(attn_scores_scaled_torch, dtype=torch.float32, dim=-1)\n",
    "\n",
    "attn_weights_ttnn, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the context vector. This involves another matmul on the attention weights and values.\n",
    "\n",
    "```python\n",
    "context_vec = attn_weights @ values\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[-0.07373,  0.07080],\n",
       "              [-0.07373,  0.07080],\n",
       "              ...,\n",
       "              [-0.07617,  0.06689],\n",
       "              [-0.07471,  0.06982]], shape=Shape([6, 2]), dtype=DataType::BFLOAT16, layout=Layout::TILE),\n",
       " TorchTensor([[-0.0739,  0.0712],\n",
       "              [-0.0748,  0.0703],\n",
       "              [-0.0749,  0.0702],\n",
       "              [-0.0760,  0.0684],\n",
       "              [-0.0763,  0.0679],\n",
       "              [-0.0754,  0.0693]], grad_fn=<AliasBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = sa_v2.W_value(context)\n",
    "\n",
    "values_ttnn = ttnn.from_torch(\n",
    "  values,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "context_vec_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "context_vec_cpu = attn_weights @ values\n",
    "\n",
    "context_vec_ttnn, context_vec_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a lot more verbose. Let's put it together in a revised SelfAttentionV2 class. We will just need to include an additional device parameter so we can internally move things around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(789)\n",
    "\n",
    "\n",
    "class SelfAttention_ttnn_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self._device = device\n",
    "\n",
    "  def forward(self, x):\n",
    "    keys = self.W_key(x)\n",
    "    queries = self.W_query(x)\n",
    "    values = self.W_value(x)\n",
    "\n",
    "    queries_ttnn = ttnn.from_torch(\n",
    "      queries,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    keys_transposed_ttnn = ttnn.from_torch(\n",
    "      keys.T,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    values_ttnn = ttnn.from_torch(\n",
    "      values,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "      queries_ttnn,\n",
    "      keys_transposed_ttnn\n",
    "    )\n",
    "\n",
    "    attn_scores_scaled_torch = ttnn.to_torch(attn_scores_ttnn) / (keys.shape[-1] ** 0.5)\n",
    "    attn_scores_scaled_ttnn = ttnn.from_torch(\n",
    "      attn_scores_scaled_torch,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "    attn_weights_ttnn = ttnn.softmax(attn_scores_scaled_ttnn, dim=-1)\n",
    "\n",
    "    context_vec_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "    context_vec = ttnn.to_torch(context_vec_ttnn)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "TorchTensor([[-0.0737,  0.0708],\n",
      "             [-0.0737,  0.0708],\n",
      "             [-0.0737,  0.0708],\n",
      "             [-0.0757,  0.0684],\n",
      "             [-0.0762,  0.0669],\n",
      "             [-0.0747,  0.0698]], dtype=torch.bfloat16)\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "context = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "d = context[0].shape\n",
    "d_in = d[0]\n",
    "d_out = d[0] - 1\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(d_in, d_out, device)\n",
    "context_vec = sa_ttnn_v2(context)\n",
    "\n",
    "print(context_vec)\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Time: 0.1215 milliseconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.process_time_ns()\n",
    "\n",
    "print(\"hello\")\n",
    "\n",
    "end = time.process_time_ns()\n",
    "\n",
    "print(f\"Time: {(end - start) / 1000000:.4f} milliseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class PerfTimer:\n",
    "  def __init__(self):\n",
    "    self.start_time = 0\n",
    "    self.end_time = 0\n",
    "    \n",
    "  def start(self):\n",
    "    self.start_time = time.time()\n",
    "    \n",
    "  def stop(self):\n",
    "    self.end_time = time.time()\n",
    "\n",
    "  def reset(self):\n",
    "    self.start_time = 0\n",
    "    self.end_time = 0\n",
    "    \n",
    "  def elapsed_ms(self):\n",
    "    return (self.end_time - self.start_time) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7782.544136047363"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "t = PerfTimer()\n",
    "\n",
    "t.start()\n",
    "torch_tensors = [torch.randn(1024, 2048) for _ in range(0, 1000)]\n",
    "torch_tensors[1:2]\n",
    "t.stop()\n",
    "\n",
    "t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0142, -0.0142, -0.0185,  ...,  0.0308, -0.0289,  0.0347],\n",
       "         [ 0.0125, -0.0044, -0.0308,  ...,  0.0253, -0.0311,  0.0227],\n",
       "         [ 0.0020, -0.0132, -0.0349,  ...,  0.0241, -0.0279,  0.0237],\n",
       "         ...,\n",
       "         [ 0.0190, -0.0007, -0.0203,  ...,  0.0238, -0.0351,  0.0171],\n",
       "         [ 0.0146,  0.0041, -0.0246,  ...,  0.0209, -0.0370,  0.0034],\n",
       "         [ 0.0114, -0.0088, -0.0332,  ...,  0.0046, -0.0331,  0.0101]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " 11865.072011947632)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "t.start()\n",
    "sa_v2 = SelfAttention_v2(2048, 2048)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_v2(tensor)\n",
    "t.stop()\n",
    "\n",
    "result, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[ 0.0143, -0.0148, -0.0179,  ...,  0.0312, -0.0295,  0.0349],\n",
       "              [ 0.0137, -0.0045, -0.0320,  ...,  0.0259, -0.0315,  0.0243],\n",
       "              [ 0.0019, -0.0135, -0.0349,  ...,  0.0245, -0.0273,  0.0239],\n",
       "              ...,\n",
       "              [ 0.0194, -0.0003, -0.0203,  ...,  0.0242, -0.0356,  0.0179],\n",
       "              [ 0.0150,  0.0042, -0.0242,  ...,  0.0210, -0.0369,  0.0032],\n",
       "              [ 0.0115, -0.0100, -0.0334,  ...,  0.0045, -0.0330,  0.0107]],\n",
       "             dtype=torch.bfloat16),\n",
       " 49823.36091995239)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "t.start()\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(2048, 2048, device)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_ttnn_v2(tensor)\n",
    "\n",
    "t.stop()\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "result, t.elapsed_ms()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(789)\n",
    "\n",
    "class SelfAttention_ttnn_v2(nn.Module):\n",
    "  def __init__(self, d_in, d_out, device):\n",
    "    super().__init__()\n",
    "\n",
    "    self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "    self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    self._device = device\n",
    "    \n",
    "    # Extract weight matrices from PyTorch layers and convert to TTNN once\n",
    "    self.W_query_ttnn = ttnn.from_torch(\n",
    "        self.W_query.weight, \n",
    "        dtype=ttnn.bfloat16, \n",
    "        layout=ttnn.TILE_LAYOUT, \n",
    "        device=self._device\n",
    "    )\n",
    "    \n",
    "    self.W_key_ttnn = ttnn.from_torch(\n",
    "        self.W_key.weight, \n",
    "        dtype=ttnn.bfloat16, \n",
    "        layout=ttnn.TILE_LAYOUT, \n",
    "        device=self._device\n",
    "    )\n",
    "    \n",
    "    self.W_value_ttnn = ttnn.from_torch(\n",
    "        self.W_value.weight, \n",
    "        dtype=ttnn.bfloat16, \n",
    "        layout=ttnn.TILE_LAYOUT, \n",
    "        device=self._device\n",
    "    )\n",
    "\n",
    "    self._scaler = 1 / (d_out ** 0.5)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x_t = ttnn.from_torch(\n",
    "      x.T ,\n",
    "      dtype=ttnn.bfloat16,\n",
    "      layout=ttnn.TILE_LAYOUT,\n",
    "      device=self._device\n",
    "    )\n",
    "\n",
    "    keys_ttnn = ttnn.matmul(self.W_key_ttnn, x_t)\n",
    "    keys_ttnn = ttnn.permute(keys_ttnn, (1, 0))\n",
    "\n",
    "    queries_ttnn = ttnn.matmul(self.W_query_ttnn, x_t)\n",
    "    queries_ttnn = ttnn.permute(queries_ttnn, (1, 0))\n",
    "\n",
    "    values_ttnn = ttnn.matmul(self.W_value_ttnn, x_t)\n",
    "    values_ttnn = ttnn.permute(values_ttnn, (1, 0))\n",
    "\n",
    "    keys_transposed_ttnn = ttnn.permute(keys_ttnn, (1, 0))\n",
    "    attn_scores_ttnn = ttnn.matmul(\n",
    "        queries_ttnn, \n",
    "        keys_transposed_ttnn\n",
    "    )\n",
    "    \n",
    "    scaler_ttnn = ttnn.full(attn_scores_ttnn.shape, self._scaler, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=self._device)\n",
    "    attn_weights_ttnn = ttnn.softmax(attn_scores_ttnn * scaler_ttnn, dim=-1)\n",
    "\n",
    "    context_vec_ttnn = ttnn.matmul(attn_weights_ttnn, values_ttnn)\n",
    "\n",
    "    context_vec = ttnn.to_torch(context_vec_ttnn)\n",
    "\n",
    "    return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TorchTensor([[ 0.0003, -0.0002, -0.0006,  ...,  0.0005, -0.0006,  0.0004],\n",
       "              [ 0.0003, -0.0002, -0.0006,  ...,  0.0005, -0.0006,  0.0004],\n",
       "              [ 0.0003, -0.0002, -0.0006,  ...,  0.0005, -0.0006,  0.0004],\n",
       "              ...,\n",
       "              [ 0.0003, -0.0002, -0.0006,  ...,  0.0005, -0.0006,  0.0004],\n",
       "              [ 0.0003, -0.0002, -0.0006,  ...,  0.0005, -0.0006,  0.0004],\n",
       "              [ 0.0003, -0.0002, -0.0006,  ...,  0.0005, -0.0006,  0.0004]],\n",
       "             dtype=torch.bfloat16),\n",
       " 19300.540685653687)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "\n",
    "t.reset()\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "t.start()\n",
    "sa_ttnn_v2 = SelfAttention_ttnn_v2(2048, 2048, device)\n",
    "for tensor in torch_tensors:\n",
    "  result = sa_ttnn_v2(tensor)\n",
    "\n",
    "t.stop()\n",
    "\n",
    "ttnn.close_device(device)\n",
    "\n",
    "result, t.elapsed_ms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
