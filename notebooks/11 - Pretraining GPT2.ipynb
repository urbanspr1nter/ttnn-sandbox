{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eda073",
   "metadata": {},
   "source": [
    "# Pretraining GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "931d1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.0,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0647de",
   "metadata": {},
   "source": [
    "## Torch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cf5b6",
   "metadata": {},
   "source": [
    "Import the libraries, and now we have separated the `GPTModel` implementation to a new file to make the notebook smaller, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b72c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x77375cfdb610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4e337",
   "metadata": {},
   "source": [
    "We will just instantiate the model. Since our config already has 0 drop out, we don't need to turn on `eval` mode for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345b27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da393ec9",
   "metadata": {},
   "source": [
    "Define some helpers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f9b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.generate import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639330fd",
   "metadata": {},
   "source": [
    "Let's test the model at the moment. Again we expect it to generate some garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70abd679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "  model=model,\n",
    "  idx=text_to_token_ids(start_context, tokenizer),\n",
    "  max_new_tokens=10,\n",
    "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae6ae2",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75dbead",
   "metadata": {},
   "source": [
    "we have 2 batches here, so we add the corresponding \"next\" token into the targets while maintaining a consistent context length. this gives a \"sliding window\" effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4aa20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[16833,  3626,  6100],\n",
       "         [   40,  1107,   588]]),\n",
       " tensor([[ 3626,  6100,   345],\n",
       "         [ 1107,   588, 11311]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346c2157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e811430",
   "metadata": {},
   "source": [
    "Probas contains for each token, there are vocab_size probabilities in the last dimension. We want to select the token ID (the index) of the highest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32814b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.8849e-05, 1.5172e-05, 1.1687e-05,  ..., 2.2409e-05,\n",
       "          6.9776e-06, 1.8776e-05],\n",
       "         [9.1569e-06, 1.0062e-05, 7.8786e-06,  ..., 2.9090e-05,\n",
       "          6.0103e-06, 1.3571e-05],\n",
       "         [2.9877e-05, 8.8507e-06, 1.5741e-05,  ..., 3.5456e-05,\n",
       "          1.4094e-05, 1.3526e-05]],\n",
       "\n",
       "        [[1.2561e-05, 2.0538e-05, 1.4332e-05,  ..., 1.0389e-05,\n",
       "          3.4784e-05, 1.4239e-05],\n",
       "         [7.2731e-06, 1.7864e-05, 1.0565e-05,  ..., 2.1206e-05,\n",
       "          1.1390e-05, 1.5559e-05],\n",
       "         [2.9496e-05, 3.3605e-05, 4.1029e-05,  ..., 6.5249e-06,\n",
       "          5.8203e-05, 1.3698e-05]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f8d56",
   "metadata": {},
   "source": [
    "For each word, we find an individual token to succeed the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a4a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a4869",
   "metadata": {},
   "source": [
    "But too bad! we're nowhere close to our target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4708dad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n",
      "Targets batch 2:  really like chocolate\n",
      "Outputs batch 2:  pressuring empoweredfaith\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n",
    "print(f\"Outputs batch 2: {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0dfa2",
   "metadata": {},
   "source": [
    "So what did this probabilities look like for the _real_ indices for targets? You'll find them to be small. The goal now is to increase these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8df0a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289da65",
   "metadata": {},
   "source": [
    "Take the log for individual probabilities and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bc3cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833da750",
   "metadata": {},
   "source": [
    "Take the average and negate to make it positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f9b545f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7518b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b2d99",
   "metadata": {},
   "source": [
    "The above is basically what we call \"cross entropy loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a6310",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Lets just use torch cross_entropy to do everything we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c07811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abec1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea174aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ca81b",
   "metadata": {},
   "source": [
    "We can calculate the perplexity too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "480caa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99233a6",
   "metadata": {},
   "source": [
    "## Data Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fa747d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\") as f:\n",
    "  text_data = f.read()\n",
    "\n",
    "text_data[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ebb59b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ce841e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.prepare_data import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93645756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74a59bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbca7766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee348934",
   "metadata": {},
   "source": [
    "Given the input and targets, we forward pass through the model for the input batch. Then given the logits, we flatten on the dimensions and calculate the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f211f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model):\n",
    "    input_batch, target_batch = input_batch, target_batch\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4f1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_loader(data_loader, model, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae4e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "train_loss = calc_loss_loader(train_loader, model)\n",
    "val_loss = calc_loss_loader(val_loader, model)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc886e",
   "metadata": {},
   "source": [
    "## Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c31d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, eval_iter):\n",
    "    train_loss = calc_loss_loader(train_loader, model, num_batches=eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, num_batches=eval_iter)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, start_context):\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20673ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.794, Val loss 9.909\n",
      "Ep 1 (Step 000005): Train loss 8.038, Val loss 8.324\n",
      "Every effort moves you,,,,,,,,,,,,,,.                                   \n",
      "Ep 2 (Step 000010): Train loss 6.598, Val loss 7.041\n",
      "Ep 2 (Step 000015): Train loss 5.996, Val loss 6.575\n",
      "Every effort moves you, and, and, and, and, and, and, and. \", and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.558, Val loss 6.444\n",
      "Ep 3 (Step 000025): Train loss 5.956, Val loss 7.675\n",
      "Every effort moves you.                                                 \n",
      "Ep 4 (Step 000030): Train loss 4.243, Val loss 6.283\n",
      "Ep 4 (Step 000035): Train loss 4.304, Val loss 6.210\n",
      "Every effort moves you.               \"I--and's--and it's had been, and I had been the, and I had been the honour of the, and he had been, I had\n",
      "Ep 5 (Step 000040): Train loss 3.443, Val loss 6.196\n",
      "Every effort moves you know it was not a littleI glanced after him, and I had been his eyes: \"--his, in fact--as of the picture.     \"--and it's his pictures, and down, and he had been\n",
      "Ep 6 (Step 000045): Train loss 3.226, Val loss 6.247\n",
      "Ep 6 (Step 000050): Train loss 2.691, Val loss 6.236\n",
      "Every effort moves you know; and in a little Mrs.  \"--I looked. \"I looked--and me.  \"Oh, in the moment--as Jack himself, and he had it, a, and down, and he was his\n",
      "Ep 7 (Step 000055): Train loss 2.538, Val loss 6.263\n",
      "Ep 7 (Step 000060): Train loss 1.742, Val loss 6.182\n",
      "Every effort moves you know.\" \"Yes--I glanced after him, and Mrs.  \"I looked--I looked up, I felt to see a smile behind his pictures--as I had been his painting, the donkey. \"There were days when I\n",
      "Ep 8 (Step 000065): Train loss 1.326, Val loss 6.250\n",
      "Ep 8 (Step 000070): Train loss 1.070, Val loss 6.333\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 0.775, Val loss 6.334\n",
      "Ep 9 (Step 000080): Train loss 0.545, Val loss 6.370\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--oh, and in his\n",
      "Ep 10 (Step 000085): Train loss 0.370, Val loss 6.481\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Took this long to train: 133272.39227294922 ms\n"
     ]
    }
   ],
   "source": [
    "from scripts.perf_timer import PerfTimer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "timer = PerfTimer()\n",
    "\n",
    "timer.start()\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "timer.stop()\n",
    "\n",
    "print(f\"Took this long to train: {timer.elapsed_ms()} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e60346ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren Mortgage TT remember gard ACTIONSussedOND Land Engeleddedemate breaths proxies GalaxyForm\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8951d6a",
   "metadata": {},
   "source": [
    "## TTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92299a29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94daf579",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.0,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4a5ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 04:48:20.124 | DEBUG    | ttnn.library_tweaks:prepare_dir_as_metal_home:54 - Existing installation of 0.57.0rc60+any detected\n",
      "2025-05-11 04:48:20.148 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    }
   ],
   "source": [
    "import ttnn\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch import nn\n",
    "from scripts.text_helpers import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "device = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebc5ab",
   "metadata": {},
   "source": [
    "## Initialize the GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1f1650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n",
      "\u001b[32m2025-05-11 04:48:22.049\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-05-11 04:48:22.059\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-11 04:48:22.061\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-05-11 04:48:22.061\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-11 04:48:22.061\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-05-11 04:48:22.061\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-05-11 04:48:22.082\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New chip! We now have 1 chips\n",
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    }
   ],
   "source": [
    "if device:\n",
    "  ttnn.close_device(device)\n",
    "\n",
    "from scripts.gpt2_model_ttnn import GPTModel_ttnn\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "model_ttnn = GPTModel_ttnn(GPT_CONFIG_124M, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0247d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_pad_dims_split_rows_multicore\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_pad_dims_split_rows_multicore, tilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: reader_dropout_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: writer_dropout_interleaved_start_id, reader_dropout_interleaved_start_id, dropout_kernel\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,3. First unused index: 1. Kernels: reader_unary_reduce_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,3. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_reduce_interleaved_start_id, reduce_w\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: reader_unary_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_interleaved_start_id, eltwise_sfpu\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_transpose_wh_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_transpose_wh_interleaved_start_id, transpose_wh\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_multicore, reader_unary_interleaved_start_id, untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_writer_bmm_tile_layout_in1, reader_bmm_tile_layout_in0, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_multicore, reader_unary_interleaved_start_id, pack_untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n",
      "Output text:\n",
      " Every effort moves you collagen HTTPSioliries Simone represented Compos Harrison circumstanceemp\n"
     ]
    }
   ],
   "source": [
    "from scripts.generate_ttnn import generate_text_simple_ttnn\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "inputs_ttnn = ttnn.from_torch(\n",
    "  text_to_token_ids(start_context, tokenizer),\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  dtype=ttnn.uint32,\n",
    "  device=device\n",
    ")\n",
    "token_ids = generate_text_simple_ttnn(\n",
    "  model=model_ttnn,\n",
    "  idx_ttnn=inputs_ttnn,\n",
    "  max_new_tokens=10,\n",
    "  context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "  device=device\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5311586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.prepare_data import create_dataloader_v1\n",
    "\n",
    "with open(\"data/the-verdict.txt\", \"r\") as f:\n",
    "  text_data = f.read()\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149cc90",
   "metadata": {},
   "source": [
    "## Cross Entropy for TTNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b48ae432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_ttnn(x_ttnn):\n",
    "  return ttnn.div(\n",
    "    ttnn.exp(x_ttnn),\n",
    "    ttnn.sum(ttnn.exp(x_ttnn), dim=-1)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b0b76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch_ttnn(input_batch_ttnn, target_batch, model_ttnn, device):\n",
    "    input_batch_ttnn, target_batch = input_batch_ttnn, target_batch\n",
    "    logits_ttnn = model_ttnn(input_batch_ttnn)\n",
    "    logits = ttnn.to_torch(logits_ttnn, device=device)\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef371f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchTensor(10.3125, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# every effort moves\n",
    "inputs = torch.tensor([[16833, 3626, 6100]]) \n",
    "\n",
    "# effort moves you\n",
    "targets = torch.tensor([[3626, 6100, 345]]) \n",
    "\n",
    "inputs_ttnn = ttnn.from_torch(\n",
    "  inputs,\n",
    "  dtype=ttnn.bfloat16,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "\n",
    "loss_torch = calc_loss_batch_ttnn(inputs_ttnn, targets, model_ttnn, device)\n",
    "loss_torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d99fd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader_ttnn(data_loader, model_ttnn, num_batches=None, device=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        input_batch_ttnn = ttnn.from_torch(input_batch, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch_ttnn(input_batch_ttnn, target_batch, model_ttnn, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d646d41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_stick_layout_split_rows_interleaved\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_stick_layout_split_rows_interleaved, tilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_interleaved, reader_unary_interleaved_start_id, untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_interleaved, reader_unary_interleaved_start_id, pack_untilize\n",
      "Training loss: 10.979166666666666\n",
      "Validation loss: 10.9375\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "train_loss = calc_loss_loader_ttnn(train_loader, model_ttnn, device=device)\n",
    "val_loss = calc_loss_loader_ttnn(val_loader, model_ttnn, device=device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_ttnn(model_ttnn, train_loader, val_loader, eval_iter, device):\n",
    "    train_loss = calc_loss_loader_ttnn(train_loader, model_ttnn, num_batches=eval_iter, device=device)\n",
    "    val_loss = calc_loss_loader_ttnn(val_loader, model_ttnn, num_batches=eval_iter, device=device)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample_ttnn(model_ttnn, tokenizer, start_context, device):\n",
    "    context_size = model_ttnn.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple_ttnn(\n",
    "      model=model_ttnn,\n",
    "      idx_ttnn=encoded,\n",
    "      max_new_tokens=50,\n",
    "      context_size=context_size,\n",
    "      device=device\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208da018",
   "metadata": {},
   "source": [
    "The tricky part..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0acf6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model_ttnn, train_loader, val_loader, optimizer, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer, device):\n",
    "  # Initialize lists to track losses and tokens seen\n",
    "  train_losses, val_losses = [], []\n",
    "  global_step = -1\n",
    "\n",
    "  # Main training loop\n",
    "  for epoch in range(num_epochs):\n",
    "      \n",
    "    for input_batch, target_batch in train_loader:\n",
    "      # THIS IS IN TORCH!\n",
    "      optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "\n",
    "      # Transfer the input batch to be in ttnn to perform the forward pass.\n",
    "      # Loss will be calculated on the CPU afterwards.\n",
    "      input_batch_ttnn = ttnn.from_torch(input_batch, dtype=ttnn.bfloat16, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "      loss = calc_loss_batch_ttnn(input_batch_ttnn, target_batch, model_ttnn, device=device)\n",
    "\n",
    "      # ðŸš§ NOTE: All the weights, layers etc need to be updated\n",
    "      loss.backward() # Calculate loss gradients\n",
    "      optimizer.step() # Update model weights using loss gradients\n",
    "\n",
    "      # ðŸš§ AT THIS POINT WE NEED TO UPDATE ALL THE INTERNAL TTNN WEIGHTS\n",
    "      # Yeah, i know it sounds like double work.\n",
    "      # Ex: model_ttnn.updateStuff()\n",
    "      \n",
    "      global_step += 1\n",
    "\n",
    "      # Optional evaluation step\n",
    "      if global_step % eval_freq == 0:\n",
    "        train_loss, val_loss = evaluate_model(\n",
    "            model, train_loader, val_loader, eval_iter)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "    # âœ… Print a sample text after each epoch\n",
    "    generate_and_print_sample_ttnn(model_ttnn, tokenizer, start_context, device)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46c2123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25024a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
