{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5eda073",
   "metadata": {},
   "source": [
    "# Pretraining GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808561cb",
   "metadata": {},
   "source": [
    "We shorten the context length because our data set doesn't have enough tokens for the original 1024 context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.0,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0647de",
   "metadata": {},
   "source": [
    "## Torch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cf5b6",
   "metadata": {},
   "source": [
    "Import the libraries, and now we have separated the `GPTModel` implementation to a new file to make the notebook smaller, of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4e337",
   "metadata": {},
   "source": [
    "We will just instantiate the model. Since our config already has 0 drop out, we don't need to turn on `eval` mode for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b27c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da393ec9",
   "metadata": {},
   "source": [
    "Define some helpers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f9b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.generate import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639330fd",
   "metadata": {},
   "source": [
    "Let's test the model at the moment. Again we expect it to generate some garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abd679",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "  model=model,\n",
    "  idx=text_to_token_ids(start_context, tokenizer),\n",
    "  max_new_tokens=10,\n",
    "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae6ae2",
   "metadata": {},
   "source": [
    "## Loss Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75dbead",
   "metadata": {},
   "source": [
    "we have 2 batches here, so we add the corresponding \"next\" token into the targets while maintaining a consistent context length. this gives a \"sliding window\" effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4aa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]\n",
    "\n",
    "inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e811430",
   "metadata": {},
   "source": [
    "Probas contains for each token, there are vocab_size probabilities in the last dimension. We want to select the token ID (the index) of the highest number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32814b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f8d56",
   "metadata": {},
   "source": [
    "For each word, we find an individual token to succeed the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a4869",
   "metadata": {},
   "source": [
    "But too bad! we're nowhere close to our target!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4708dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "print(f\"Targets batch 2: {token_ids_to_text(targets[1], tokenizer)}\")\n",
    "print(f\"Outputs batch 2: {token_ids_to_text(token_ids[1].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0dfa2",
   "metadata": {},
   "source": [
    "So what did this probabilities look like for the _real_ indices for targets? You'll find them to be small. The goal now is to increase these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7289da65",
   "metadata": {},
   "source": [
    "Take the log for individual probabilities and then concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc3cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833da750",
   "metadata": {},
   "source": [
    "Take the average and negate to make it positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7518b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b2d99",
   "metadata": {},
   "source": [
    "The above is basically what we call \"cross entropy loss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a6310",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "Lets just use torch cross_entropy to do everything we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abec1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea174aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ca81b",
   "metadata": {},
   "source": [
    "We can calculate the perplexity too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480caa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99233a6",
   "metadata": {},
   "source": [
    "## Data Stuff\n",
    "\n",
    "Let's prepare the data for training. We had downloaded `the-verdict.txt` a while back, and now it is time to use it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fa747d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 characters: I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n",
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\") as f:\n",
    "  text_data = f.read()\n",
    "\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"First 100 characters:\", text_data[:100])\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398b737",
   "metadata": {},
   "source": [
    "Now let's split the data into training and validation sets. We'll use 90% of the data for our training data, and the rest of the 10% as the validation data set.\n",
    "\n",
    "To create these data loaders, let's tap into our `create_dataloader_v1` function we wrote a few notebooks back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce841e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.prepare_data import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392a561",
   "metadata": {},
   "source": [
    "Our data set is really small, so we shouldn't have too many batches. Turns out, with `the-verdict.txt`, we only have 1 validation batch for our context length of 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74a59bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e37f09",
   "metadata": {},
   "source": [
    "Let's verify how many tokens are in the training and validation batches individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbca7766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdc302",
   "metadata": {},
   "source": [
    "## Training Loss Calculation\n",
    "\n",
    "The main meat of the training loop involves performing calculations on the loss. We will calculate the cross-entropy loss between an input batch and target batch. \n",
    "\n",
    "What is returned is a tensor representing for given token, we give indication of the loss for each other token within the given vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f211f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model):\n",
    "    input_batch, target_batch = input_batch, target_batch\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc0e4da",
   "metadata": {},
   "source": [
    "`calc_loss_loader` will wrap around the `calc_loss_batch` and process for every input batch and its corresponding target batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd4f1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_loss_loader(data_loader, model, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da140f10",
   "metadata": {},
   "source": [
    "What's the current training loss and validation loss for our model? Well, since we haven't trained it yet, I wouldn't be surprised if it is really high!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7ae4e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.28953828579849666\n",
      "Validation loss: 6.4668426513671875\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "train_loss = calc_loss_loader(train_loader, model)\n",
    "val_loss = calc_loss_loader(val_loader, model)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc886e",
   "metadata": {},
   "source": [
    "## Training Our Model\n",
    "\n",
    "The time has come to train our model! Here, we will use pure CPU inferencing, and I've simplified the original code a bit to make things easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses = [], []\n",
    "    global_step = -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses \n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, eval_iter):\n",
    "    train_loss = calc_loss_loader(train_loader, model, num_batches=eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, num_batches=eval_iter)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, start_context):\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa275dda",
   "metadata": {},
   "source": [
    "Now, let's execute the training loop. It's a good opportunity to also time the entire training. On my i7 13700, it takes a little over 2 minutes for this training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20673ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.perf_timer import PerfTimer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "timer = PerfTimer()\n",
    "\n",
    "timer.start()\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "timer.stop()\n",
    "\n",
    "print(f\"Took this long to train: {timer.elapsed_ms()} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from scripts.generate import generate_text_simple\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=50,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf9e9a",
   "metadata": {},
   "source": [
    "## Saving the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6713c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/gpt2-verdict-model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874af154",
   "metadata": {},
   "source": [
    "## Reload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(\n",
    "  torch.load(\"models/gpt2-verdict-model.pth\", weights_only=True)\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff565267",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8951d6a",
   "metadata": {},
   "source": [
    "## TTNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92299a29",
   "metadata": {},
   "source": [
    "We are not going to train the model using TTNN, instead, we will use it to perform inference in an already trained model. In this case, we diligently trained the GP2 model with our data set using CPU and had saved it to disk. We can reload those weights and reapply them to the GPTModel_ttnn class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4a5ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 19:16:25.110 | DEBUG    | ttnn.library_tweaks:prepare_dir_as_metal_home:54 - Existing installation of 0.57.0rc60+any detected\n",
      "2025-05-12 19:16:25.133 | DEBUG    | ttnn:<module>:83 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/avgdev/.cache/ttnn,model_cache_path=/home/avgdev/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_should_raise_exception=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Device | INFO     | Opening user mode device driver\n",
      "\u001b[32m2025-05-12 19:16:25.714\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\n",
      "\u001b[32m2025-05-12 19:16:25.722\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-12 19:16:25.724\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Harvesting mask for chip 0 is 0x200 (physical layout: 0x1, logical: 0x200, simulated harvesting mask: 0x0).\n",
      "\u001b[32m2025-05-12 19:16:25.724\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Opened PCI device 0; KMD version: 1.33.0, IOMMU: disabled\n",
      "\u001b[32m2025-05-12 19:16:25.725\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected PCI devices: [0]\n",
      "\u001b[32m2025-05-12 19:16:25.725\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Using local chip ids: {0} and remote chip ids {}\n",
      "\u001b[32m2025-05-12 19:16:25.733\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Software version 6.0.0, Ethernet FW version 6.14.0 (Device 0)\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Enabling program cache on device 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New chip! We now have 1 chips\n",
      "Chip initialization complete (found )\n",
      "Chip initializing complete...\n",
      " ARC\n",
      "\n",
      " [4/4] DRAM\n",
      "\n",
      " [16/16] ETH\n",
      "\n",
      " CPU\n",
      "\n",
      "Chip detection complete (found )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ttnn\n",
    "import tiktoken\n",
    "import torch\n",
    "from torch import nn\n",
    "from scripts.text_helpers import text_to_token_ids, token_ids_to_text\n",
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.0,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "device = None\n",
    "\n",
    "if device:\n",
    "  ttnn.close_device(device)\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "device.enable_program_cache()\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(\n",
    "  torch.load(\"models/gpt2-verdict-model.pth\", weights_only=True)\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d55b4",
   "metadata": {},
   "source": [
    "## Open the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633217a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n",
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n",
      "                  Metal | INFO     | Enabling program cache on device 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if device:\n",
    "  ttnn.close_device(device)\n",
    "\n",
    "from scripts.gpt2_model import GPTModel\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)\n",
    "\n",
    "device.enable_program_cache()\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(\n",
    "  torch.load(\"models/gpt2-verdict-model.pth\", weights_only=True)\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebc5ab",
   "metadata": {},
   "source": [
    "## Initialize the GPT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db15bd6",
   "metadata": {},
   "source": [
    "We will reload the model to make sure that we have the latest changes from disk. \n",
    "\n",
    "Since we already have a model instance, we'll just need to copy over all the weights to the `ttnn` version of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1f1650",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.gpt2_model_ttnn import GPTModel_ttnn, TransformerBlock_ttnn\n",
    "\n",
    "model_ttnn = GPTModel_ttnn(GPT_CONFIG_124M, device)\n",
    "\n",
    "model_ttnn.pos_emb.weight = torch.nn.Parameter(model.pos_emb.weight) \n",
    "model_ttnn.tok_emb.weight = torch.nn.Parameter(model.tok_emb.weight)\n",
    "\n",
    "for i, block in enumerate(model.trf_blocks):\n",
    "  t = model_ttnn.trf_blocks_ttnn[i]\n",
    "\n",
    "  t.att.W_key.weight = torch.nn.Parameter(block.att.W_key.weight)\n",
    "  t.att.W_key.bias = torch.nn.Parameter(block.att.W_key.bias)\n",
    "\n",
    "  t.att.W_query.weight = torch.nn.Parameter(block.att.W_query.weight)\n",
    "  t.att.W_query.bias = torch.nn.Parameter(block.att.W_query.bias)\n",
    "  \n",
    "  t.att.W_value.weight = torch.nn.Parameter(block.att.W_value.weight)\n",
    "  t.att.W_value.bias = torch.nn.Parameter(block.att.W_value.bias)\n",
    "  \n",
    "  t.att.out_proj.weight = torch.nn.Parameter(block.att.out_proj.weight)\n",
    "  t.att.out_proj.bias = torch.nn.Parameter(block.att.out_proj.bias)\n",
    "\n",
    "  t.ff.lin_1.weight = torch.nn.Parameter(block.ff.layer[0].weight)\n",
    "  t.ff.lin_1.bias = torch.nn.Parameter(block.ff.layer[0].bias)\n",
    "  t.ff.lin_2.weight = torch.nn.Parameter(block.ff.layer[2].weight)\n",
    "  t.ff.lin_2.bias = torch.nn.Parameter(block.ff.layer[2].bias)\n",
    "\n",
    "  t.norm1.scale = torch.nn.Parameter(block.norm1.scale)\n",
    "  t.norm1.shift = torch.nn.Parameter(block.norm1.shift)\n",
    "\n",
    "  t.norm2.scale = torch.nn.Parameter(block.norm2.scale)\n",
    "  t.norm2.shift = torch.nn.Parameter(block.norm2.shift)\n",
    "\n",
    "model_ttnn.final_norm.shift = torch.nn.Parameter(model.final_norm.shift)\n",
    "model_ttnn.final_norm.scale = torch.nn.Parameter(model.final_norm.scale)\n",
    "model_ttnn.out_head.weight = torch.nn.Parameter(model.out_head.weight)\n",
    "\n",
    "model_ttnn.update_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274e049",
   "metadata": {},
   "source": [
    "## Out Head, Final Norm, and Transformer Blocks Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680dcf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "Shape([50257, 768])\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "Overall Status:  PASS\n",
      "\n",
      "\n",
      "Overall Status:  PASS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_diff': 0.0,\n",
       " 'mean_diff': 0.0,\n",
       " 'correlation': 1.0078125,\n",
       " 'max_diff_status': True,\n",
       " 'mean_diff_status': True,\n",
       " 'correlation_status': True,\n",
       " 'overall_status': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scripts.compare_tensors import compare_tensors\n",
    "\n",
    "# position and token embedding comparisons\n",
    "compare_tensors(\n",
    "  ttnn.to_torch(\n",
    "    ttnn.reshape(\n",
    "      (model_ttnn.tok_emb_ttnn),\n",
    "      (1, model_ttnn.tok_emb_ttnn.shape[0], model_ttnn.tok_emb_ttnn.shape[1])\n",
    "    )\n",
    "  ),\n",
    "  model.tok_emb.weight.unsqueeze(0),\n",
    "  suppress_details=True\n",
    ")\n",
    "\n",
    "compare_tensors(\n",
    "  ttnn.to_torch(\n",
    "    ttnn.reshape(\n",
    "      (model_ttnn.pos_emb_ttnn),\n",
    "      (1, model_ttnn.pos_emb_ttnn.shape[0], model_ttnn.pos_emb_ttnn.shape[1])\n",
    "    )\n",
    "  ),\n",
    "  model.pos_emb.weight.unsqueeze(0),\n",
    "  suppress_details=True\n",
    ")\n",
    "# outhead comparison\n",
    "out_head_shape = model_ttnn.out_head_ttnn.shape\n",
    "print(out_head_shape)\n",
    "\n",
    "compare_tensors(\n",
    "  ttnn.to_torch(\n",
    "    ttnn.reshape(\n",
    "      model_ttnn.out_head_ttnn,\n",
    "      (1, out_head_shape[0], out_head_shape[1])\n",
    "    )\n",
    "  ),\n",
    "  model.out_head.weight.unsqueeze(0),\n",
    "  suppress_details=True\n",
    ")\n",
    "\n",
    "for i, block in enumerate(model.trf_blocks):\n",
    "  t = model_ttnn.trf_blocks_ttnn[i]\n",
    "\n",
    "  compare_tensors(\n",
    "    ttnn.to_torch(\n",
    "      ttnn.reshape(\n",
    "        t.att.W_key_ttnn,\n",
    "        (1, t.att.W_key_ttnn.shape[0], t.att.W_key_ttnn.shape[1]))\n",
    "    ),\n",
    "    model.trf_blocks[i].att.W_key.weight.unsqueeze(0),\n",
    "    suppress_details=True\n",
    "  )\n",
    "  compare_tensors(\n",
    "    ttnn.to_torch(\n",
    "      ttnn.reshape(\n",
    "        t.att.W_query_ttnn,\n",
    "        (1, t.att.W_query_ttnn.shape[0], t.att.W_query_ttnn.shape[1]))\n",
    "    ),\n",
    "    model.trf_blocks[i].att.W_query.weight.unsqueeze(0),\n",
    "    suppress_details=True\n",
    "  )\n",
    "  compare_tensors(\n",
    "    ttnn.to_torch(\n",
    "      ttnn.reshape(\n",
    "        t.att.W_value_ttnn,\n",
    "        (1, t.att.W_value_ttnn.shape[0], t.att.W_value_ttnn.shape[1]))\n",
    "    ),\n",
    "    model.trf_blocks[i].att.W_value.weight.unsqueeze(0),\n",
    "    suppress_details=True\n",
    "  )\n",
    "  compare_tensors(\n",
    "    ttnn.to_torch(\n",
    "      ttnn.reshape(\n",
    "        t.att.out_proj_ttnn,\n",
    "        (1, t.att.out_proj_ttnn.shape[0], t.att.out_proj_ttnn.shape[1]))\n",
    "    ),\n",
    "    model.trf_blocks[i].att.out_proj.weight.unsqueeze(0),\n",
    "    suppress_details=True\n",
    "  )\n",
    "  compare_tensors(\n",
    "    ttnn.to_torch(\n",
    "      ttnn.reshape(\n",
    "        t.att.out_proj_bias_ttnn,\n",
    "        (1, 1, t.att.out_proj_bias_ttnn.shape[0]))\n",
    "    ),\n",
    "    model.trf_blocks[i].att.out_proj.bias.unsqueeze(0).unsqueeze(0),\n",
    "    suppress_details=True\n",
    "  )\n",
    "\n",
    "# Final Norm comparison\n",
    "final_norm_shift_shape = model_ttnn.final_norm.shift_ttnn.shape\n",
    "final_norm_scale_shape = model_ttnn.final_norm.scale_ttnn.shape\n",
    "\n",
    "compare_tensors(\n",
    "  ttnn.to_torch(\n",
    "    ttnn.reshape(\n",
    "      model_ttnn.final_norm.shift_ttnn,\n",
    "      (1, 1, final_norm_shift_shape[0])\n",
    "    )\n",
    "  ),\n",
    "  model.final_norm.shift.unsqueeze(0).unsqueeze(0),\n",
    "  suppress_details=True\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "compare_tensors(\n",
    "  ttnn.to_torch(\n",
    "    ttnn.reshape(\n",
    "      model_ttnn.final_norm.scale_ttnn,\n",
    "      (1, 1, final_norm_scale_shape[0])\n",
    "    )\n",
    "  ),\n",
    "  model.final_norm.scale.unsqueeze(0).unsqueeze(0),\n",
    "  suppress_details=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6b802",
   "metadata": {},
   "source": [
    "## Text Generation with Loaded Weights on Tenstorrent Hardware\n",
    "\n",
    "**! A little note about this section. !**\n",
    "\n",
    "Unfortunately as of May 2025, `ttnn` seems to have a bit of trouble calculating the `argmax` of a tensor. It also has trouble with concatenating two tensors together too. \n",
    "\n",
    "Because of that, I have to move the tensors from the device memory _into_ host memory. This sucks because then we just nullify all the work we did in \"accelerating\" the computation for inference.\n",
    "\n",
    "The transfer between the Wormhole and the Core i7 13700 is just too slow. Repeating this for multiple iterations of token generation, it just results in a horrible experience.\n",
    "\n",
    "For the sake of education, I will be assuming that we do get acceleration from all this in the world where `argmax` and `concat` are fixed in `ttnn`. \n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247d270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_pad_dims_split_rows_multicore\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_pad_dims_split_rows_multicore, tilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,3. First unused index: 1. Kernels: reader_unary_reduce_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,3. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_reduce_interleaved_start_id, reduce_w\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: reader_unary_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_interleaved_start_id, eltwise_sfpu\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_transpose_wh_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_transpose_wh_interleaved_start_id, transpose_wh\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_sender_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_multicore, reader_unary_interleaved_start_id, untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_writer_bmm_tile_layout_in1, reader_bmm_tile_layout_in0, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_multicore, reader_unary_interleaved_start_id, pack_untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 2,5,6,7,11. First unused index: 1. Kernels: writer_unary_interleaved_start_id_blocked_sm, reader_unary_interleaved_sm, softmax\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_interleaved_wh_multicore\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_wh_multicore, reader_unary_interleaved_wh_multicore, untilize_wh\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_pad_multicore_both_dims\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id_wh, reader_unary_pad_multicore_both_dims, tilize_wh\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_stick_layout_split_rows_interleaved\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_interleaved_start_id, reader_unary_stick_layout_split_rows_interleaved, tilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_interleaved, reader_unary_interleaved_start_id, untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_interleaved, reader_unary_interleaved_start_id, pack_untilize\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_receiver, bmm_large_block_zm_fused_bias_activation\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 4,5. First unused index: 2. Kernels: reader_bmm_tile_layout_in1_receiver_writer_padding, reader_bmm_tile_layout_in0_sender_padding, bmm_large_block_zm_fused_bias_activation\n",
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. But no--for it was not till after that event that the _rose Dubarry\n"
     ]
    }
   ],
   "source": [
    "from scripts.generate_ttnn import generate_text_simple_ttnn\n",
    "\n",
    "torch.manual_seed(123)\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "inputs_ttnn = ttnn.from_torch(\n",
    "  text_to_token_ids(start_context, tokenizer),\n",
    "  dtype=ttnn.uint32,\n",
    "  layout=ttnn.TILE_LAYOUT,\n",
    "  device=device\n",
    ")\n",
    "token_ids_ttnn = generate_text_simple_ttnn(\n",
    "  model=model_ttnn,\n",
    "  idx_ttnn=inputs_ttnn,\n",
    "  max_new_tokens=100,\n",
    "  context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "  device=device\n",
    ")\n",
    "\n",
    "token_ids = ttnn.to_torch(token_ids_ttnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827af49",
   "metadata": {},
   "source": [
    "Now, the output text we get from doing all inference on the hardware should be the same as the CPU's generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cfd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I couldn't look at that thing--couldn't face it. But I forced myself to put it here; and now it's cured me--cured me. But no--for it was not till after that event that the _rose Dubarry\n"
     ]
    }
   ],
   "source": [
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27c38f",
   "metadata": {},
   "source": [
    "Finally, close the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fdb5b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "`ttnn` is an operator library. It's meant for raw compute of tensors. Therefore, we should leverage the library on Tenstorrent hardware for _inference_. \n",
    "\n",
    "There are ways we can train on the hardware, but I'm not skilled enough yet.\n",
    "\n",
    "For the rest of the notebook series, we'll continue to use the method in training through CPU, and inferencing through Tenstorrent hardware. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
