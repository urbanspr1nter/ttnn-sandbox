{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511ed556-c30c-4ce3-8163-84475bd3cdca",
   "metadata": {},
   "source": [
    "# tt-NN Embedding Layer Example\n",
    "\n",
    "This notebook shows how you can create an embedding layer out of `ttnn` tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d0b07-b252-4674-bce8-e6e5c766d31f",
   "metadata": {},
   "source": [
    "Let's assume we are developing a GPT-2 LLM model. We will need to specify a `vocab_size` and `output_dim`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c39a0d34-57ef-478e-8985-eadaeab2d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f938c80-1221-456b-b8d3-84c5ea6e75c8",
   "metadata": {},
   "source": [
    "Next, let's import some dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63223398-3da0-43e3-9ff4-ef0af4313377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "from scripts.prepare_data import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cc017-d537-4d37-b715-6f0f381b7608",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's build a simple dataset by first acquiring some text. We will use the \"the-verdict.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c82783-8481-4bae-a68e-a431a2634575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap g\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"data/the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"data/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    \n",
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(raw_text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7fe0df-6951-4318-ae8b-3689b88046eb",
   "metadata": {},
   "source": [
    "Next, let's create a dataloader so that we can obtain some batches. We'll assume a context length of 4, and batch size of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0999244a-927f-4cb2-bb3b-06d24379e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 4\n",
    "batch_size = 8\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=batch_size, max_length=context_length,\n",
    "    stride=context_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad08603-f2a8-45e1-9268-69541c92e888",
   "metadata": {},
   "source": [
    "## Torch Example\n",
    "\n",
    "First, in `torch`, we can typically create input embeddings by creating a token embedding layer, and positional embedding layer concatenated together. The token embedding layer receives the input batch, and the positional embedding can be initialized to increasing numbers. It is pretty simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f6f765f-d5e0-4040-89b1-57e8fbeb85e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2732,  2.1288,  0.8971,  ..., -1.2919, -0.0776,  2.9252],\n",
      "         [-1.3840,  3.2146, -2.3263,  ...,  2.0142, -1.3516,  0.9443],\n",
      "         [ 2.5248,  1.8677, -1.3374,  ...,  0.3811,  2.8022,  0.7201],\n",
      "         [-0.1077, -0.5230,  0.9847,  ..., -0.7862, -0.1137, -1.3520]],\n",
      "\n",
      "        [[-3.3425,  1.7041,  1.2159,  ..., -0.4244, -1.1343,  0.3792],\n",
      "         [-2.1216,  0.7098, -0.6508,  ...,  0.8105, -2.8036,  0.3345],\n",
      "         [ 1.7248,  1.1010, -0.3695,  ..., -1.2099,  1.5292,  1.6327],\n",
      "         [ 0.7812, -0.2927,  1.2999,  ..., -1.6942,  0.6392, -0.6615]]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "\n",
    "positional_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "positional_embeddings = positional_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "input_embeddings = token_embeddings + positional_embeddings\n",
    "\n",
    "print(input_embeddings[0:2])\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c61576-cc58-43fe-8358-d772d59f7ae2",
   "metadata": {},
   "source": [
    "## tt-NN Example\n",
    "\n",
    "Unfortunately life isn't as easy with `ttnn`, but we can get there. Let's create the token embeddings and positional embeddings one-by-one and we can combine them to create the input_embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fb9e4-a1ec-48bf-ba4c-410fb6cba9cd",
   "metadata": {},
   "source": [
    "Various operations require the tensors to be on the device. So let's initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae488bc8-56e1-453e-b94b-382492bf8ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Initializing device 0. Program cache is NOT enabled\n",
      "                  Metal | INFO     | AI CLK for device 0 is:   1000 MHz\n"
     ]
    }
   ],
   "source": [
    "device_id = 0 \n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007ddf27-191a-4ab9-87e7-8054c4d6df1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ttnn.Tensor([[   40,   367,  ...,  2885,  1464],\n",
       "              [ 1807,  3619,  ...,   402,   271],\n",
       "              ...,\n",
       "              [ 1049,  5975,  ...,   284,   502],\n",
       "              [  284,  3285,  ...,   326,    11]], shape=Shape([8, 4]), dtype=DataType::UINT32, layout=Layout::ROW_MAJOR),\n",
       " ttnn.Tensor([[  367,  2885,  ...,  1464,  1807],\n",
       "              [ 3619,   402,  ...,   271, 10899],\n",
       "              ...,\n",
       "              [ 5975,   284,  ...,   502,   284],\n",
       "              [ 3285,   326,  ...,    11,   287]], shape=Shape([8, 4]), dtype=DataType::UINT32, layout=Layout::ROW_MAJOR))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_ttnn = ttnn.from_torch(inputs, dtype=ttnn.uint32)\n",
    "targets_ttnn = ttnn.from_torch(targets, dtype=ttnn.uint32)\n",
    "\n",
    "inputs_ttnn = ttnn.to_device(inputs_ttnn, device)\n",
    "targets_ttnn = ttnn.to_device(targets_ttnn, device)\n",
    "\n",
    "inputs_ttnn, targets_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c19e3b-6a28-469b-ab1d-4945508c99aa",
   "metadata": {},
   "source": [
    "Creating an embedding tensor is more involved. We will need to initialize a weight matrix that has the dimensions of the vocabularly size and output dimensions.\n",
    "\n",
    "The dimensions are (50257, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66a7ec89-bd7d-4ccf-a7c4-ff891891474f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[-1.49219, -0.12695,  ..., -0.68750, -1.35156],\n",
       "             [-2.32812,  0.56641,  ..., -1.00781,  0.49219],\n",
       "             ...,\n",
       "             [ 0.22852, -0.76562,  ...,  0.34766, -0.01544],\n",
       "             [-0.14844,  1.01562,  ..., -1.02344, -0.09717]], shape=Shape([50257, 256]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding_weights_ttnn = ttnn.from_torch(\n",
    "    torch.randn(vocab_size, output_dim),\n",
    "    dtype=ttnn.bfloat16\n",
    ")\n",
    "token_embedding_weights_ttnn = ttnn.to_device(token_embedding_weights_ttnn, device)\n",
    "\n",
    "token_embedding_weights_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb36dec-3051-4ec0-ad48-f938956be700",
   "metadata": {},
   "source": [
    "Now we can create the token_embeddings in one shot with `ttnn.embedding`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7670a363-746d-48d6-9295-cf207ac51c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[[-2.10938,  0.69531,  ..., -0.66406,  0.23926],\n",
       "              [ 1.18750,  1.09375,  ...,  0.46484, -0.04858],\n",
       "              ...,\n",
       "              [-0.54297,  0.03662,  ...,  0.90625, -1.48438],\n",
       "              [ 0.15820,  1.18750,  ..., -0.15430, -0.79688]],\n",
       "\n",
       "             [[-2.28125, -0.88672,  ...,  0.57422, -0.40820],\n",
       "              [-0.38672, -2.31250,  ..., -1.14844,  0.68359],\n",
       "              ...,\n",
       "              [-0.44727, -1.00781,  ...,  0.21191,  0.01501],\n",
       "              [ 1.12500,  2.42188,  ..., -2.37500, -1.69531]],\n",
       "\n",
       "             ...,\n",
       "\n",
       "             [[-0.61328, -1.60156,  ..., -0.27930,  1.07031],\n",
       "              [-0.42773,  0.78906,  ...,  0.52734, -1.40625],\n",
       "              ...,\n",
       "              [ 2.29688, -0.69141,  ...,  1.17969, -0.17969],\n",
       "              [-0.69531, -1.21875,  ...,  0.91406, -0.88672]],\n",
       "\n",
       "             [[ 2.29688, -0.69141,  ...,  1.17969, -0.17969],\n",
       "              [-0.97266, -0.61328,  ...,  0.57422,  0.76953],\n",
       "              ...,\n",
       "              [ 0.07910,  1.10156,  ..., -0.08643,  0.18555],\n",
       "              [-0.31641, -0.17578,  ..., -1.57812,  0.55469]]], shape=Shape([8, 4, 256]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings_ttnn = ttnn.embedding(inputs_ttnn, token_embedding_weights_ttnn)\n",
    "token_embeddings_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c28f14-73c9-4f52-be35-6c1f60920435",
   "metadata": {},
   "source": [
    "We can repeat the same thing with positional embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dac65bd-1514-43f0-9cd4-241fda3727fb",
   "metadata": {},
   "source": [
    "We'll need to generate some positional inputs first. We'll create a simple tensor from 0 to the context_length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c93d15a6-3fd9-4ab3-8073-8548d71a00e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([    0,     1,  ...,     2,     3], shape=Shape([4]), dtype=DataType::UINT32, layout=Layout::ROW_MAJOR)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_inputs_ttnn = ttnn.arange(end=context_length, dtype=ttnn.uint32)\n",
    "positional_inputs_ttnn = ttnn.to_device(positional_inputs_ttnn, device)\n",
    "\n",
    "positional_inputs_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a86fc4-70b9-44d4-82ed-15c6d4522aff",
   "metadata": {},
   "source": [
    "Now we can create positional embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63411ea3-a073-4486-9b09-d2c263cb0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embeddings_weights = ttnn.from_torch(\n",
    "    torch.randn(context_length, output_dim),\n",
    "    dtype=ttnn.bfloat16\n",
    ")\n",
    "positional_embeddings_weights = ttnn.to_device(positional_embeddings_weights, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596aa8e7-ee2c-4a3d-9e31-027d9cbd53c9",
   "metadata": {},
   "source": [
    "Create positional embeddings now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88c213c7-a665-40b5-b802-9f5fe2fea44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[ 0.01794, -1.33594,  ...,  1.28125,  0.78125],\n",
       "             [-0.25195,  0.05713,  ...,  1.07031,  0.60938],\n",
       "             ...,\n",
       "             [ 0.79688, -1.25000,  ...,  0.17578, -1.56250],\n",
       "             [-0.19629,  0.48828,  ..., -0.88672, -1.84375]], shape=Shape([4, 256]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embeddings_ttnn = ttnn.embedding(positional_inputs_ttnn, positional_embeddings_weights)\n",
    "positional_embeddings_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8cd3c-59d3-401f-bac9-1abd105a6804",
   "metadata": {},
   "source": [
    "We're not quite done with the positional_embeddings_ttn yet. We have to now reshape for addition operation coming up. This involves:\n",
    "1. Reshape the positional_embeddings_ttnn tensor to be the same number of dimensions as the token_embeddings_ttn.\n",
    "2. Use repeat_interleave to make an effective addition broadcast across all elements in the tensor when added against the token_embeddings_ttnn\n",
    "\n",
    "It is expected that we turn the (4, 246) shape into a (8, 4, 256) shape tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae17dc5f-2d3c-4b70-9a96-bf5efd5d8728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[[ 0.01794, -1.33594,  ...,  1.28125,  0.78125],\n",
       "              [-0.25195,  0.05713,  ...,  1.07031,  0.60938],\n",
       "              ...,\n",
       "              [ 0.79688, -1.25000,  ...,  0.17578, -1.56250],\n",
       "              [-0.19629,  0.48828,  ..., -0.88672, -1.84375]],\n",
       "\n",
       "             [[ 0.01794, -1.33594,  ...,  1.28125,  0.78125],\n",
       "              [-0.25195,  0.05713,  ...,  1.07031,  0.60938],\n",
       "              ...,\n",
       "              [ 0.79688, -1.25000,  ...,  0.17578, -1.56250],\n",
       "              [-0.19629,  0.48828,  ..., -0.88672, -1.84375]],\n",
       "\n",
       "             ...,\n",
       "\n",
       "             [[ 0.01794, -1.33594,  ...,  1.28125,  0.78125],\n",
       "              [-0.25195,  0.05713,  ...,  1.07031,  0.60938],\n",
       "              ...,\n",
       "              [ 0.79688, -1.25000,  ...,  0.17578, -1.56250],\n",
       "              [-0.19629,  0.48828,  ..., -0.88672, -1.84375]],\n",
       "\n",
       "             [[ 0.01794, -1.33594,  ...,  1.28125,  0.78125],\n",
       "              [-0.25195,  0.05713,  ...,  1.07031,  0.60938],\n",
       "              ...,\n",
       "              [ 0.79688, -1.25000,  ...,  0.17578, -1.56250],\n",
       "              [-0.19629,  0.48828,  ..., -0.88672, -1.84375]]], shape=Shape([8, 4, 256]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_embeddings_ttnn = ttnn.reshape(positional_embeddings_ttnn, (1, context_length, output_dim))\n",
    "positional_embeddings_ttnn = ttnn.repeat_interleave(positional_embeddings_ttnn, repeats=batch_size, dim=0)\n",
    "positional_embeddings_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a85924e-ded5-468b-8d9b-6833f13eb5cd",
   "metadata": {},
   "source": [
    "We can now compute the input_embeddings with token_embeddings_tttn and positional_embeddings_ttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42b6f779-c024-47ad-b0e2-e54b5a73c43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[[-2.09375, -0.64062,  ...,  0.61719,  1.02344],\n",
       "              [ 0.93750,  1.14844,  ...,  1.53906,  0.56250],\n",
       "              ...,\n",
       "              [ 0.25391, -1.21094,  ...,  1.08594, -3.04688],\n",
       "              [-0.03809,  1.67969,  ..., -1.03906, -2.64062]],\n",
       "\n",
       "             [[-1.64062,  0.16406,  ..., -3.67188,  1.45312],\n",
       "              [ 1.88281, -1.03906,  ..., -0.11719, -0.57812],\n",
       "              ...,\n",
       "              [ 0.64062, -0.12109,  ..., -0.50781, -1.46094],\n",
       "              [ 0.51953, -0.50391,  ..., -0.61328,  2.45312]],\n",
       "\n",
       "             ...,\n",
       "\n",
       "             [[ 0.00000,  0.00000,  ..., -2.56250, -0.89844],\n",
       "              [-4.28125,  1.70312,  ..., -2.46875,  1.97656],\n",
       "              ...,\n",
       "              [ 0.00000,  0.00000,  ..., -2.28125, -1.56250],\n",
       "              [ 1.82812, -0.74219,  ...,  0.60938, -4.09375]],\n",
       "\n",
       "             [[ 0.00000, -0.00000,  ..., 14637248544768.00000, 14637248544768.00000],\n",
       "              [-0.00000, -0.00000,  ..., 29274497089536.00000, 14637248544768.00000],\n",
       "              ...,\n",
       "              [-0.00000, -0.00000,  ..., 14637248544768.00000, 14637248544768.00000],\n",
       "              [-0.00000, -0.00000,  ..., 14637248544768.00000, 14637248544768.00000]]], shape=Shape([8, 4, 256]), dtype=DataType::BFLOAT16, layout=Layout::TILE)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings_ttnn = ttnn.add(\n",
    "    ttnn.tilize(token_embeddings_ttnn),\n",
    "    ttnn.tilize(positional_embeddings_ttnn)\n",
    ")\n",
    "input_embeddings_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d4a849-7353-486b-a686-5b05c9f1a345",
   "metadata": {},
   "source": [
    "Thre's a lot of padding inserted, which is why you will see extreme values at the end of the tensors. We can untilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d5a8760-7be8-4b50-8c32-8bd16cb57329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: reader_unary_interleaved_start_id\n",
      "                  Metal | WARNING  | Circular buffer indices are not contiguous starting at 0. This will hurt dispatch performance. Non-contiguous indices: 16. First unused index: 1. Kernels: writer_unary_stick_layout_split_rows_interleaved, reader_unary_interleaved_start_id, pack_untilize\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ttnn.Tensor([[[-2.09375, -0.64062,  ...,  0.61719,  1.02344],\n",
       "              [ 0.93750,  1.14844,  ...,  1.53906,  0.56250],\n",
       "              ...,\n",
       "              [ 0.25391, -1.21094,  ...,  1.08594, -3.04688],\n",
       "              [-0.03809,  1.67969,  ..., -1.03906, -2.64062]],\n",
       "\n",
       "             [[-2.26562, -2.21875,  ...,  1.85938,  0.37305],\n",
       "              [-0.64062, -2.25000,  ..., -0.07812,  1.29688],\n",
       "              ...,\n",
       "              [ 0.34961, -2.26562,  ...,  0.38867, -1.54688],\n",
       "              [ 0.92969,  2.90625,  ..., -3.26562, -3.54688]],\n",
       "\n",
       "             ...,\n",
       "\n",
       "             [[-0.59375, -2.93750,  ...,  1.00000,  1.85156],\n",
       "              [-0.67969,  0.84766,  ...,  1.60156, -0.79688],\n",
       "              ...,\n",
       "              [ 3.09375, -1.94531,  ...,  1.35938, -1.74219],\n",
       "              [-0.89062, -0.73047,  ...,  0.02734, -2.73438]],\n",
       "\n",
       "             [[ 2.31250, -2.03125,  ...,  2.46875,  0.60156],\n",
       "              [-1.22656, -0.55469,  ...,  1.64844,  1.38281],\n",
       "              ...,\n",
       "              [ 0.87500, -0.14844,  ...,  0.08936, -1.37500],\n",
       "              [-0.51172,  0.31250,  ..., -2.46875, -1.28906]]], shape=Shape([8, 4, 256]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings_ttnn = ttnn.untilize(input_embeddings_ttnn)\n",
    "input_embeddings_ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6716f2f9-690e-4ae2-a68a-fe952961a21e",
   "metadata": {},
   "source": [
    "Let's do a sanity check. We're expecting the same (8, 4, 256) shape.\n",
    "\n",
    "This means a batch_size of 8, with 4 tokens in context, for 256 dimensions. The greater the dimensions the more \"detail\" we will have to record the embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86f77ece-59bc-42d2-9354-a1cd66720dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ttnn.Tensor([[[-2.09375, -0.64062,  ...,  0.61719,  1.02344],\n",
      "              [ 0.93750,  1.14844,  ...,  1.53906,  0.56250],\n",
      "              ...,\n",
      "              [ 0.25391, -1.21094,  ...,  1.08594, -3.04688],\n",
      "              [-0.03809,  1.67969,  ..., -1.03906, -2.64062]],\n",
      "\n",
      "             [[-1.64062,  0.16406,  ..., -3.67188,  1.45312],\n",
      "              [ 1.88281, -1.03906,  ..., -0.11719, -0.57812],\n",
      "              ...,\n",
      "              [ 0.64062, -0.12109,  ..., -0.50781, -1.46094],\n",
      "              [ 0.51953, -0.50391,  ..., -0.61328,  2.45312]]], shape=Shape([2, 4, 256]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)\n",
      "Shape([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings_ttnn[0:2])\n",
    "print(input_embeddings_ttnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecd38db-dad3-4d26-8366-676349a20edd",
   "metadata": {},
   "source": [
    "Finally, don't forget to clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "331df0f7-a4ee-4ef4-a312-e68d2b001079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Metal | INFO     | Closing device 0\n",
      "                  Metal | INFO     | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb0dc9-5b81-4097-88a5-49ed75a93ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
